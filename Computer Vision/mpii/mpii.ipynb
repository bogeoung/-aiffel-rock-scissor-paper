{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 행동 스티커 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1' #CPU 사용\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "workdir = os.path.join(os.getenv('HOME'),'aiffel/mpii')\n",
    "os.chdir(workdir)\n",
    "\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "import ray\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### json 파싱\n",
    "앞서 다운 받은 `train.json`과 `validation.json`은 이미지에 담겨 있는 사람들의 pose keypoint 정보들을 가지고 있음. 이는 Pose Estimation을 위한 label로 삼을 수 있음.  \n",
    "이 json파일들이 어떻게 구성되어 있는지 확인하기 위해 샘플로 annotation정보를 1개만 출력함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "json_file_path = os.getenv('HOME')+'/aiffel/mpii/mpii_human_pose_v1_u12_2/train.json'\n",
    "\n",
    "with open(json_file_path) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2) # json beautify\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`joints`** \n",
    "\n",
    "`joints`가 label로 사용할 keypoint의 label임.  \n",
    "이미지 형상과 사람의 포즈에 따라 모든 label이 이미지에 나타나지 않기 떄문에 `joints_vis`를 이용해서 실제로 사용할 수 있는 keypoint인지를 나타냄.  \n",
    "`joints`의 순서는 다음과 같음.  \n",
    "0 - 오른쪽 발목  \n",
    "1 - 오른쪽 무릎  \n",
    "2 - 오른쪽 엉덩이  \n",
    "3 - 왼쪽 엉덩이  \n",
    "4 - 왼쪽 무릎  \n",
    "5 - 왼쪽 발목  \n",
    "6 - 골반  \n",
    "7 - 가슴(흉부)  \n",
    "8 - 목  \n",
    "9 - 머리 위  \n",
    "10 - 오른쪽 손목  \n",
    "11 - 오른쪽 팔꿈치  \n",
    "12 - 오른쪽 어깨  \n",
    "13 - 왼쪽 어깨  \n",
    "14 - 왼쪽 팔꿈치  \n",
    "15 - 왼쪽 손목  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`scale`**  \n",
    "높이 = scale * 200px  \n",
    "scale정보가 coco dataset에는 scale 값 또한 2차원으로 주어져서 bbox scale이 나오지만 mpii는 높이만 나옴   \n",
    "\n",
    "\n",
    "**`center`**  \n",
    "사람의 중심점을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json annotation을 파싱하는 함수 \n",
    "# image의 전체 path를 묶어 dict 타입의 label로 만듬. -> 이 label을 통해 학습 수행\n",
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfrecord 파일 만들기\n",
    "일반적으로 학습 과정에서 gpu의 연산 속도보다 HDD I/O의 속도가 느리기 때문에 병목 현상이 발생하고 효율성이 떨어지는 것을 관찰할 수 있음.  \n",
    "따라서 \"학습 데이터를 어떻게 빠르게 읽는가\"에 대한 고민이 생김.  \n",
    "\n",
    "학습 속도를 향상시키기 위해서 data read(또는 prefetch) 또는 데이터 변환 단계에서 gpu학습과 병렬적으로 수행되도록 prefetch를 적용해야함.  \n",
    "수행방법은 tf.data의 map함수를 이요하고 cache에 저장해두는 방법을 사용함.  \n",
    "\n",
    "tf는 데이터셋을 tfrecord 형태로 표현함으로써 위 변환을 자동화 해줌.  \n",
    "`tfrecord`는 binary record sequence를 저장하기 위한 형식으로, 내부적으로는 protocol buffer를 이용함.  \n",
    "\n",
    "protobuf 는 크로스플랫폼에서 사용할 수 있는 직렬화 데이터 라이브러리라고 생각하면 됨.  데이터셋 크기가 크기 때문에 빠른 학습을 위해서 이 정보를 tfrecord 파일로 변환함.  \n",
    "\n",
    "- annotation 을 total_shards 개수로 나눔(chunkify) (train : 64개, val : 8개)\n",
    "- build_single_tfrecord 함수를 통해 tfrecord 로 저장\n",
    "- 각 chunk 끼리 dependency 가 없기 때문에 병렬처리가 가능, ray를 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        # train_0001_of_0064.tfrecords\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, './tfrecords_mpii/{}_{}_of_{}.tfrecords'.format(\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**annotation을 적절한 개수로 나누는 함수 `chunkify`** \n",
    "- l 은 annotation, n은 shard 개수\n",
    "- shard 개수 단위로 annotation list 를 나누어서 새로운 list를 만듭니다.\n",
    "- numpy array 라고 가정하면 (size, shard, anno_content) 정도의 shape을 가짐. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tfrecord 1개를 저장하는 함수 `build_single_tfrecord`**\n",
    "- TFRecordWriter 를 이용해서 anno_list 를 shard 개수 단위로 작성함.\n",
    "- generate_tfexample 함수를 사용함.\n",
    "- [중요] write 할 때 string 으로 serialize 해야함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.example만드는 `generate_tfexample`**\n",
    "- 우리가 정의한 json 의 python type의 값들을 tfexample 에 사용할 수 있는 값으로 변환함.\n",
    "- image 파일은 byte 로 변환합니다. bitmap 으로 저장하게되면 파일용량이 상당히 커지기 때문에 만약 jpeg 타입이 아닌 경우 jpeg 으로 변환 후 content 로 불러서 저장함. (H,W,C)\n",
    "- 각 label 값을 tf.train.Feature 로 저장합니다. 이 때 데이터 타입에 주의해야 함.\n",
    "- 이미지는 byte 인코딩 된 값을 그대로 넣음.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray\n",
    "Ray는 파이썬을 위한 간단한 분산 어플리케이션 api임.  \n",
    "참고자료 : [https://docs.ray.io/en/latest/](https://docs.ray.io/en/latest/)  \n",
    "\n",
    "위 내용들을 모두 하나의 파일로 정리하면 다음과 같음.  \n",
    "\n",
    "**tfrecords_mpii.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 19:58:03,588\tINFO services.py:1269 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': './images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': './images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n",
      "\u001b[2m\u001b[36m(pid=3352)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3359)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3350)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3351)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3357)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3360)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3354)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3353)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3358)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3355)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3349)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3357)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3357)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3352)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3352)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3359)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3359)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3353)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3353)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3354)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3354)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3355)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3355)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3360)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3360)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3358)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3358)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3351)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3349)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3349)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3351)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3350)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3350)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3357)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3357)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3352)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3352)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3359)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3359)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3353)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3353)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3349)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3349)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3350)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3350)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3354)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3354)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3351)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3351)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3360)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3360)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3355)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3355)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3358)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3358)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3357)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3357)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3352)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3352)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3359)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3359)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3353)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3353)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3349)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3349)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3350)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3350)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0042_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3354)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3354)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3351)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3351)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3360)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3360)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3355)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3355)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3357)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3357)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3358)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3358)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3352)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3352)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3359)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3359)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3353)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3353)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3349)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3349)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3350)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3350)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3354)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3354)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3351)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3351)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3355)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3355)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3360)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3360)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3352)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3352)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3358)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3358)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3357)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3357)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3359)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3359)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3353)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3353)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3349)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3350)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3354)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3355)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3351)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3360)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3359)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3358)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3352)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3357)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3352)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3359)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3351)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3357)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3360)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3353)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3353)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3358)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3355)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3351)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3353)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3359)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3352)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3357)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3358)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=3355)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "Successfully wrote 25204 annotations to TF Records.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "import ray\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "ray.init()\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n - 1):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    results.append(l[start:])\n",
    "    return results\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy(\n",
    "        )  # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def generate_tfexample(anno):\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    # x = [\n",
    "    #     joint[0] / width if joint[0] >= 0 else joint[0]\n",
    "    #     for joint in anno['joints']\n",
    "    # ]\n",
    "    # y = [\n",
    "    #     joint[1] / height if joint[1] >= 0 else joint[0]\n",
    "    #     for joint in anno['joints']\n",
    "    # ]\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    # 0 - invisible, 1 - occluded, 2 - visible\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        # 'image/object/parts/x':\n",
    "        # tf.train.Feature(float_list=tf.train.FloatList(value=x)),\n",
    "        # 'image/object/parts/y':\n",
    "        # tf.train.Feature(float_list=tf.train.FloatList(value=y)),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno_list in chunk:\n",
    "            tf_example = generate_tfexample(anno_list)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)\n",
    "\n",
    "\n",
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        # train_0001_of_0064.tfrecords\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, './tfrecords_mpii/{}_{}_of_{}.tfrecords'.format(\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)\n",
    "\n",
    "\n",
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation\n",
    "\n",
    "\n",
    "def main():\n",
    "    print('Start to parse annotations.')\n",
    "    if not os.path.exists('./tfrecords_mpii'):\n",
    "        os.makedirs('./tfrecords_mpii')\n",
    "\n",
    "    with open(workdir + '/mpii_human_pose_v1_u12_2/train.json') as train_json:\n",
    "        train_annos = json.load(train_json)\n",
    "        train_annotations = [\n",
    "            parse_one_annotation(anno, './images/')\n",
    "            for anno in train_annos\n",
    "        ]\n",
    "        print('First train annotation: ', train_annotations[0])\n",
    "        del (train_annos)\n",
    "\n",
    "    with open(workdir + '/mpii_human_pose_v1_u12_2/validation.json') as val_json:\n",
    "        val_annos = json.load(val_json)\n",
    "        val_annotations = [\n",
    "            parse_one_annotation(anno, './images/') for anno in val_annos\n",
    "        ]\n",
    "        print('First val annotation: ', val_annotations[0])\n",
    "        del (val_annos)\n",
    "\n",
    "    print('Start to build TF Records.')\n",
    "    build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "    build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "    print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "        len(train_annotations) + len(val_annotations)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3360)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "     17      17     391\r\n"
     ]
    }
   ],
   "source": [
    "# 약 200mb 정도의 tfrecords들이 72개 만들어진 것을 확인할 수 있음. \n",
    "%ls | wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data label로 만들기\n",
    "tfrecords 파일을 읽고 전처리를 할 수 있는 dataloader를 만듬.  \n",
    "\n",
    "**`Preprocessor` class**  \n",
    "####  `__call__()` 메소드  \n",
    "`Preprocessor` 클래스 코드의 `__call__()` 메소드 내부에서 진행되는 주요 과정은 다음과 같음.  \n",
    "- tfrecord 파일이기 때문에 병렬로 읽는 것은 tf 가 지원해주고 있음. `self.parse_tfexample()` 에 구현되어 있고 이 함수를 통해 tf.tensor 로 이루어진 dictionary 형태의 features를 얻을 수 있음.  \n",
    "- 즉 image 는 `features['image/encoded']` 형태로 사용할 수 있고 tfrecord 를 저장할 때 jpeg encoding 된 값을 넣었으므로 `tf.io.decode_jpeg()`로 decoding 하여 tensor 형태의 이미지를 얻음.  \n",
    "- `crop_roi()` 메소드를 이용해 해당 이미지를 학습하기 편하도록 몇가지 트릭을 적용함. \n",
    "- `make_heatmaps()` 메소드를 이용해 label을 heatmap 으로 나타냄.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  `parse_tfexample` 메소드  \n",
    "\n",
    "- tfrecord 파일 형식을 우리가 저장한 data type feature 에 맞게 parsing 함.   \n",
    "- tf 가 자동으로 parsing 해주는 점은 아주 편하지만 feature description 을 정확하게 알고 있어야하는 단점이 있음.   \n",
    "- 즉, tfrecord 에서 사용할 key 값들과 data type 을 모르면 tfrecord 파일을 사용하기 굉장히 어려움. (serialize 되어있으므로..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  `crop_roi` 메소드  \n",
    "\n",
    "- 얻은 image 와 label 을 이용해서 적절한 학습형태로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `make_heatmaps` 메소드  \n",
    "\n",
    "- 우리가 알고 있는 것은 joints의 위치, center의 좌표, body height값임. 균일하게 학습하기 위해 body width도 적절히 정하는 것도 중요함.  \n",
    "\n",
    "- 높이 정보와 keypoint 위치를 이용해서 정사각형 박스를 사용하는 것을 기본으로 디자인 함. 이와 관련해서는 여러 방법이 있을 수 있지만, 우리가 임의로 조정한 crop box가 이미지 바깥으로 나가지 않는지 예외처리 하는 것을 더 중요하게 봄.  \n",
    "\n",
    "- (x,y)좌표로 되어있는 keypoint를 heatmap으로 변경시킴. \n",
    "- 16개의 점을 generate_2d_gaussian() 함수를 이용해서 64x64 의 map 에 표현함. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  `generate_2d_guassian` 메소드   \n",
    "\n",
    "- sigma 값이 1 이고 window size 7 인 필터를 이용해서 만듬.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 내용들을 하나의 py 파일로 정리하면 다음과 같음.  \n",
    "\n",
    "**preprocess.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y)\n",
    "\n",
    "        # print (image.shape, heatmaps.shape, type(heatmaps))\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "\n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "\n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "\n",
    "\n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \"\"\"\n",
    "        \"The same technique as Tompson et al. is used for supervision. A MeanSquared Error (MSE) loss is\n",
    "        applied comparing the predicted heatmap to a ground-truth heatmap consisting of a 2D gaussian\n",
    "        (with standard deviation of 1 px) centered on the keypoint location.\"\n",
    "\n",
    "        https://github.com/princeton-vl/pose-hg-train/blob/master/src/util/img.lua#L204\n",
    "        \"\"\"\n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        # this gaussian patch is 7x7, let's get four corners of it first\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "        # if the patch is out of image boundary we simply return nothing according to the source code\n",
    "        # [1]\"In these cases the joint is either truncated or severely occluded, so for\n",
    "        # supervision a ground truth heatmap of all zeros is provided.\"\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        # the center of the gaussian patch should be 1\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        # generate this 7x7 gaussian patch\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        # part of the patch could be out of the boundary, so we need to determine the valid range\n",
    "        # if xmin = -2, it means the 2 left-most columns are invalid, which is max(0, -(-2)) = 2\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        # if xmin = 59, xmax = 66, but our output is 64x64, then we should discard 2 right-most columns\n",
    "        # which is min(64, 66) - 59 = 5, and column 6 and 7 are discarded\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        # also, we need to determine where to put this patch in the whole heatmap\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        # finally, insert this patch into the heatmap\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "\n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * self.heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * self.heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "        num_heatmap = self.heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(self.heatmap_shape[1], self.heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "\n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example_proto):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example_proto,\n",
    "                                          image_feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hourglass 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Residual block module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,  # lift channels first\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hourglass module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \"\"\"\n",
    "    https://github.com/princeton-vl/pose-hg-train/blob/master/src/models/hg.lua#L3\n",
    "    \"\"\"\n",
    "    # Upper branch\n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    # Lower branch\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### intermediate output을 위한 linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked Hourglass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), num_stack=4, num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \"\"\"\n",
    "    https://github.com/princeton-vl/pose-hg-train/blob/master/src/models/hg.lua#L33\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # initial processing of the image\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        # predict 256 channels like a fully connected layer.\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        # predict final channels, which is also the number of predicted heatmap\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        # if it's not the last stack, we need to add predictions back\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackedHourglassNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 엔진 만들기\n",
    "학습 코드 `train.py`를 구현.  \n",
    "지금까지 제작한 `*.py` 모듈들은 여기서 참조(import)되어 사용될 것임. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from datetime import datetime\n",
    "\n",
    "import click\n",
    "import tensorflow as tf\n",
    "\n",
    "from hourglass104 import StackedHourglassNetwork\n",
    "from preprocess import Preprocessor\n",
    "\n",
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `automatic_gpu_usage` 메소드    \n",
    "- gpu memory growth 옵션을 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from datetime import datetime\n",
    "\n",
    "import click\n",
    "import tensorflow as tf\n",
    "\n",
    "from hourglass104 import StackedHourglassNetwork\n",
    "from preprocess import Preprocessor\n",
    "\n",
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def automatic_gpu_usage() :\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer class\n",
    "- loss : MSE (heatmap 을 pixel 단위 MSE 로 계산) → 실제 계산은 약간 달라, compute_loss() 에서 새로 구현함.\n",
    "- strategy : 분산학습용 tf.strategy 임. 사용 가능한 GPU가 1개뿐이라면 사용하지 않음.\n",
    "- optimizer : Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `lr_decay` 메소드  \n",
    "- learning rate : decay step에 따라 1/10씩 작아지도록 설정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `compute_loss` 메소드\n",
    "- loss function 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `train_step` , `val_step` 메소드\n",
    "이론대로라면 self.loss_object 를 사용해서 MSE 로 구현하는 것이 맞지만 사실 동일 weight MSE 는 수렴이 잘 되지 않음.   \n",
    "예측해야하는 positive (joint 들) 의 비율이 negative (배경이라고 할 수 있겠죠?) 에 비해 상당히 적은 비율로 등장하기 때문.  \n",
    "\n",
    "label이 배경이 아닌 경우(heatmap 값이 0보다 큰 경우)에 추가적인 weight를 줃도록 함.  \n",
    "weight가 82인 이유는 heatmap 전체 크기인 64x64에서 gaussian point 등장 비율이 7x7 패치이기 때문에 64/7 = 9.1 -> 9x9로 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate,\n",
    "                 version='0.0.1',\n",
    "                 start_epoch=1,\n",
    "                 tensorboard_dir='./logs'):\n",
    "        self.start_epoch = start_epoch\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        # \"we use rmsprop with a learning rate of 2.5e-4.\"\"\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.tensorboard_dir = tensorboard_dir\n",
    "        self.best_model = None\n",
    "        self.version = version\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.experimental_run_v2(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.experimental_run_v2(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        summary_writer = tf.summary.create_file_writer(self.tensorboard_dir)\n",
    "        summary_writer.set_as_default()\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.epochs + 1):\n",
    "            tf.summary.experimental.set_step(epoch)\n",
    "\n",
    "            self.lr_decay()\n",
    "            tf.summary.scalar('epoch learning rate',\n",
    "                              self.current_learning_rate)\n",
    "\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "            tf.summary.scalar('epoch train loss', train_loss)\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "            tf.summary.scalar('epoch val loss', val_loss)\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = './models/model-v{}-epoch-{}-loss-{:.4f}.h5'.format(\n",
    "            self.version, epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.dataset 만들기  \n",
    "##### `create_dataset` 메소드  \n",
    "tfrecord파일을 `tf.dataset`으로 만듬."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `train` 메소드  \n",
    "train함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def train(epochs, start_epoch, learning_rate, tensorboard_dir, checkpoint,\n",
    "          num_heatmap, batch_size, train_tfrecords, val_tfrecords, version):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(os.path.join('./models')):\n",
    "        os.makedirs(os.path.join('./models/'))\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "        if checkpoint and os.path.exists(checkpoint):\n",
    "            model.load_weights(checkpoint)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate,\n",
    "            start_epoch=start_epoch,\n",
    "            version=version,\n",
    "            tensorboard_dir=tensorboard_dir)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 2.43954825 epoch total loss 2.43954825\n",
      "Trained batch 2 batch loss 2.38080192 epoch total loss 2.41017509\n",
      "Trained batch 3 batch loss 2.45632935 epoch total loss 2.42555976\n",
      "Trained batch 4 batch loss 2.4508121 epoch total loss 2.43187284\n",
      "Trained batch 5 batch loss 2.39805412 epoch total loss 2.42510915\n",
      "Trained batch 6 batch loss 2.2803793 epoch total loss 2.40098739\n",
      "Trained batch 7 batch loss 2.25232601 epoch total loss 2.37975\n",
      "Trained batch 8 batch loss 2.059057 epoch total loss 2.33966351\n",
      "Trained batch 9 batch loss 2.11920094 epoch total loss 2.31516767\n",
      "Trained batch 10 batch loss 2.22806096 epoch total loss 2.30645704\n",
      "Trained batch 11 batch loss 2.17170811 epoch total loss 2.2942071\n",
      "Trained batch 12 batch loss 2.14521074 epoch total loss 2.28179073\n",
      "Trained batch 13 batch loss 2.13886261 epoch total loss 2.2707963\n",
      "Trained batch 14 batch loss 2.12208605 epoch total loss 2.26017404\n",
      "Trained batch 15 batch loss 1.98614454 epoch total loss 2.24190545\n",
      "Trained batch 16 batch loss 1.9977411 epoch total loss 2.22664523\n",
      "Trained batch 17 batch loss 2.08849788 epoch total loss 2.21851897\n",
      "Trained batch 18 batch loss 2.06616306 epoch total loss 2.21005464\n",
      "Trained batch 19 batch loss 1.98631823 epoch total loss 2.1982789\n",
      "Trained batch 20 batch loss 1.89321184 epoch total loss 2.1830256\n",
      "Trained batch 21 batch loss 1.86678791 epoch total loss 2.1679666\n",
      "Trained batch 22 batch loss 1.96808076 epoch total loss 2.15888095\n",
      "Trained batch 23 batch loss 1.99255073 epoch total loss 2.15164924\n",
      "Trained batch 24 batch loss 1.99519658 epoch total loss 2.1451304\n",
      "Trained batch 25 batch loss 1.98424292 epoch total loss 2.13869476\n",
      "Trained batch 26 batch loss 1.7794826 epoch total loss 2.12487888\n",
      "Trained batch 27 batch loss 1.88091898 epoch total loss 2.11584353\n",
      "Trained batch 28 batch loss 1.82982719 epoch total loss 2.10562849\n",
      "Trained batch 29 batch loss 1.85829902 epoch total loss 2.0971\n",
      "Trained batch 30 batch loss 1.87052929 epoch total loss 2.08954763\n",
      "Trained batch 31 batch loss 1.86383629 epoch total loss 2.08226657\n",
      "Trained batch 32 batch loss 1.83511186 epoch total loss 2.074543\n",
      "Trained batch 33 batch loss 1.81457829 epoch total loss 2.06666517\n",
      "Trained batch 34 batch loss 1.84242761 epoch total loss 2.06007\n",
      "Trained batch 35 batch loss 1.87017059 epoch total loss 2.05464435\n",
      "Trained batch 36 batch loss 1.92086959 epoch total loss 2.05092835\n",
      "Trained batch 37 batch loss 1.88960207 epoch total loss 2.04656816\n",
      "Trained batch 38 batch loss 1.88852918 epoch total loss 2.04240918\n",
      "Trained batch 39 batch loss 1.87658668 epoch total loss 2.03815722\n",
      "Trained batch 40 batch loss 1.85552692 epoch total loss 2.03359175\n",
      "Trained batch 41 batch loss 1.83788145 epoch total loss 2.02881837\n",
      "Trained batch 42 batch loss 1.84249377 epoch total loss 2.02438188\n",
      "Trained batch 43 batch loss 1.871804 epoch total loss 2.02083349\n",
      "Trained batch 44 batch loss 1.85672164 epoch total loss 2.01710367\n",
      "Trained batch 45 batch loss 1.83847547 epoch total loss 2.01313424\n",
      "Trained batch 46 batch loss 1.77342904 epoch total loss 2.00792336\n",
      "Trained batch 47 batch loss 1.75082684 epoch total loss 2.00245309\n",
      "Trained batch 48 batch loss 1.77357364 epoch total loss 1.99768484\n",
      "Trained batch 49 batch loss 1.63335407 epoch total loss 1.99024951\n",
      "Trained batch 50 batch loss 1.68613029 epoch total loss 1.98416698\n",
      "Trained batch 51 batch loss 1.68583369 epoch total loss 1.97831738\n",
      "Trained batch 52 batch loss 1.70537353 epoch total loss 1.97306848\n",
      "Trained batch 53 batch loss 1.80951405 epoch total loss 1.96998262\n",
      "Trained batch 54 batch loss 1.71662748 epoch total loss 1.9652909\n",
      "Trained batch 55 batch loss 1.79409862 epoch total loss 1.96217835\n",
      "Trained batch 56 batch loss 1.77924156 epoch total loss 1.95891166\n",
      "Trained batch 57 batch loss 1.74894452 epoch total loss 1.95522809\n",
      "Trained batch 58 batch loss 1.69273376 epoch total loss 1.95070231\n",
      "Trained batch 59 batch loss 1.69194937 epoch total loss 1.9463166\n",
      "Trained batch 60 batch loss 1.60624456 epoch total loss 1.94064879\n",
      "Trained batch 61 batch loss 1.77352047 epoch total loss 1.93790901\n",
      "Trained batch 62 batch loss 1.73848104 epoch total loss 1.93469238\n",
      "Trained batch 63 batch loss 1.75026417 epoch total loss 1.93176496\n",
      "Trained batch 64 batch loss 1.8510648 epoch total loss 1.93050408\n",
      "Trained batch 65 batch loss 1.81328201 epoch total loss 1.92870057\n",
      "Trained batch 66 batch loss 1.90813828 epoch total loss 1.92838907\n",
      "Trained batch 67 batch loss 1.81145597 epoch total loss 1.92664397\n",
      "Trained batch 68 batch loss 1.71790504 epoch total loss 1.92357433\n",
      "Trained batch 69 batch loss 1.71060455 epoch total loss 1.92048776\n",
      "Trained batch 70 batch loss 1.75546372 epoch total loss 1.91813028\n",
      "Trained batch 71 batch loss 1.61484742 epoch total loss 1.91385877\n",
      "Trained batch 72 batch loss 1.6830107 epoch total loss 1.91065264\n",
      "Trained batch 73 batch loss 1.66237915 epoch total loss 1.9072516\n",
      "Trained batch 74 batch loss 1.59755635 epoch total loss 1.90306652\n",
      "Trained batch 75 batch loss 1.73992193 epoch total loss 1.9008913\n",
      "Trained batch 76 batch loss 1.75821137 epoch total loss 1.89901388\n",
      "Trained batch 77 batch loss 1.80581784 epoch total loss 1.89780354\n",
      "Trained batch 78 batch loss 1.76799273 epoch total loss 1.89613926\n",
      "Trained batch 79 batch loss 1.79595137 epoch total loss 1.894871\n",
      "Trained batch 80 batch loss 1.74673748 epoch total loss 1.89301932\n",
      "Trained batch 81 batch loss 1.71788657 epoch total loss 1.8908571\n",
      "Trained batch 82 batch loss 1.74900556 epoch total loss 1.88912725\n",
      "Trained batch 83 batch loss 1.69594586 epoch total loss 1.88679981\n",
      "Trained batch 84 batch loss 1.69163656 epoch total loss 1.88447642\n",
      "Trained batch 85 batch loss 1.71950793 epoch total loss 1.8825357\n",
      "Trained batch 86 batch loss 1.6452409 epoch total loss 1.87977648\n",
      "Trained batch 87 batch loss 1.62747502 epoch total loss 1.87687647\n",
      "Trained batch 88 batch loss 1.63835847 epoch total loss 1.87416601\n",
      "Trained batch 89 batch loss 1.63713455 epoch total loss 1.87150264\n",
      "Trained batch 90 batch loss 1.61868489 epoch total loss 1.86869359\n",
      "Trained batch 91 batch loss 1.71470189 epoch total loss 1.86700141\n",
      "Trained batch 92 batch loss 1.75242603 epoch total loss 1.86575603\n",
      "Trained batch 93 batch loss 1.7734139 epoch total loss 1.86476302\n",
      "Trained batch 94 batch loss 1.72087896 epoch total loss 1.86323237\n",
      "Trained batch 95 batch loss 1.71893215 epoch total loss 1.86171341\n",
      "Trained batch 96 batch loss 1.7377665 epoch total loss 1.86042225\n",
      "Trained batch 97 batch loss 1.73680449 epoch total loss 1.85914791\n",
      "Trained batch 98 batch loss 1.64089656 epoch total loss 1.85692084\n",
      "Trained batch 99 batch loss 1.4946003 epoch total loss 1.85326099\n",
      "Trained batch 100 batch loss 1.54266071 epoch total loss 1.850155\n",
      "Trained batch 101 batch loss 1.61046231 epoch total loss 1.84778178\n",
      "Trained batch 102 batch loss 1.62112975 epoch total loss 1.84555972\n",
      "Trained batch 103 batch loss 1.67105365 epoch total loss 1.84386539\n",
      "Trained batch 104 batch loss 1.73186493 epoch total loss 1.84278846\n",
      "Trained batch 105 batch loss 1.63191116 epoch total loss 1.84078\n",
      "Trained batch 106 batch loss 1.60349905 epoch total loss 1.83854163\n",
      "Trained batch 107 batch loss 1.54919338 epoch total loss 1.83583736\n",
      "Trained batch 108 batch loss 1.69461143 epoch total loss 1.83452976\n",
      "Trained batch 109 batch loss 1.62564576 epoch total loss 1.83261335\n",
      "Trained batch 110 batch loss 1.67634773 epoch total loss 1.83119273\n",
      "Trained batch 111 batch loss 1.67166948 epoch total loss 1.82975566\n",
      "Trained batch 112 batch loss 1.69118798 epoch total loss 1.82851851\n",
      "Trained batch 113 batch loss 1.76201594 epoch total loss 1.82792985\n",
      "Trained batch 114 batch loss 1.77136183 epoch total loss 1.82743371\n",
      "Trained batch 115 batch loss 1.79598987 epoch total loss 1.82716024\n",
      "Trained batch 116 batch loss 1.5238812 epoch total loss 1.82454574\n",
      "Trained batch 117 batch loss 1.57710576 epoch total loss 1.82243085\n",
      "Trained batch 118 batch loss 1.63814282 epoch total loss 1.82086909\n",
      "Trained batch 119 batch loss 1.70671439 epoch total loss 1.81990969\n",
      "Trained batch 120 batch loss 1.72126794 epoch total loss 1.81908774\n",
      "Trained batch 121 batch loss 1.72392941 epoch total loss 1.8183012\n",
      "Trained batch 122 batch loss 1.7239455 epoch total loss 1.81752777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 123 batch loss 1.66911769 epoch total loss 1.81632113\n",
      "Trained batch 124 batch loss 1.73577356 epoch total loss 1.81567156\n",
      "Trained batch 125 batch loss 1.72361565 epoch total loss 1.81493521\n",
      "Trained batch 126 batch loss 1.70499074 epoch total loss 1.8140626\n",
      "Trained batch 127 batch loss 1.75179744 epoch total loss 1.81357229\n",
      "Trained batch 128 batch loss 1.71510959 epoch total loss 1.81280303\n",
      "Trained batch 129 batch loss 1.70718145 epoch total loss 1.8119843\n",
      "Trained batch 130 batch loss 1.74139476 epoch total loss 1.8114413\n",
      "Trained batch 131 batch loss 1.76240945 epoch total loss 1.81106699\n",
      "Trained batch 132 batch loss 1.79894304 epoch total loss 1.81097519\n",
      "Trained batch 133 batch loss 1.64030933 epoch total loss 1.80969191\n",
      "Trained batch 134 batch loss 1.69346511 epoch total loss 1.80882454\n",
      "Trained batch 135 batch loss 1.70813131 epoch total loss 1.80807865\n",
      "Trained batch 136 batch loss 1.7232101 epoch total loss 1.80745459\n",
      "Trained batch 137 batch loss 1.63857353 epoch total loss 1.80622196\n",
      "Trained batch 138 batch loss 1.63515711 epoch total loss 1.80498242\n",
      "Trained batch 139 batch loss 1.56745338 epoch total loss 1.80327356\n",
      "Trained batch 140 batch loss 1.67432427 epoch total loss 1.80235255\n",
      "Trained batch 141 batch loss 1.71433496 epoch total loss 1.80172837\n",
      "Trained batch 142 batch loss 1.67607033 epoch total loss 1.80084348\n",
      "Trained batch 143 batch loss 1.72894871 epoch total loss 1.80034077\n",
      "Trained batch 144 batch loss 1.73906469 epoch total loss 1.79991531\n",
      "Trained batch 145 batch loss 1.70526886 epoch total loss 1.79926252\n",
      "Trained batch 146 batch loss 1.74920118 epoch total loss 1.79891968\n",
      "Trained batch 147 batch loss 1.68453217 epoch total loss 1.7981416\n",
      "Trained batch 148 batch loss 1.61867189 epoch total loss 1.796929\n",
      "Trained batch 149 batch loss 1.55923128 epoch total loss 1.79533374\n",
      "Trained batch 150 batch loss 1.63532281 epoch total loss 1.79426694\n",
      "Trained batch 151 batch loss 1.67543638 epoch total loss 1.79348\n",
      "Trained batch 152 batch loss 1.67418528 epoch total loss 1.79269528\n",
      "Trained batch 153 batch loss 1.61418557 epoch total loss 1.79152858\n",
      "Trained batch 154 batch loss 1.71531487 epoch total loss 1.79103363\n",
      "Trained batch 155 batch loss 1.65549743 epoch total loss 1.79015911\n",
      "Trained batch 156 batch loss 1.76292515 epoch total loss 1.7899847\n",
      "Trained batch 157 batch loss 1.6848799 epoch total loss 1.78931522\n",
      "Trained batch 158 batch loss 1.67489922 epoch total loss 1.78859103\n",
      "Trained batch 159 batch loss 1.72519374 epoch total loss 1.78819227\n",
      "Trained batch 160 batch loss 1.62802196 epoch total loss 1.78719115\n",
      "Trained batch 161 batch loss 1.54294324 epoch total loss 1.7856741\n",
      "Trained batch 162 batch loss 1.54456782 epoch total loss 1.78418577\n",
      "Trained batch 163 batch loss 1.51241827 epoch total loss 1.78251839\n",
      "Trained batch 164 batch loss 1.46262133 epoch total loss 1.78056777\n",
      "Trained batch 165 batch loss 1.43107009 epoch total loss 1.77844965\n",
      "Trained batch 166 batch loss 1.38481855 epoch total loss 1.77607834\n",
      "Trained batch 167 batch loss 1.3623085 epoch total loss 1.7736007\n",
      "Trained batch 168 batch loss 1.44830155 epoch total loss 1.77166438\n",
      "Trained batch 169 batch loss 1.61206961 epoch total loss 1.77072\n",
      "Trained batch 170 batch loss 1.72798491 epoch total loss 1.77046871\n",
      "Trained batch 171 batch loss 1.67494941 epoch total loss 1.7699101\n",
      "Trained batch 172 batch loss 1.64116538 epoch total loss 1.7691617\n",
      "Trained batch 173 batch loss 1.58583784 epoch total loss 1.76810205\n",
      "Trained batch 174 batch loss 1.38247299 epoch total loss 1.76588583\n",
      "Trained batch 175 batch loss 1.34551501 epoch total loss 1.76348376\n",
      "Trained batch 176 batch loss 1.42719316 epoch total loss 1.76157296\n",
      "Trained batch 177 batch loss 1.5837369 epoch total loss 1.76056826\n",
      "Trained batch 178 batch loss 1.70086634 epoch total loss 1.76023281\n",
      "Trained batch 179 batch loss 1.8315897 epoch total loss 1.76063156\n",
      "Trained batch 180 batch loss 1.68024516 epoch total loss 1.76018488\n",
      "Trained batch 181 batch loss 1.51102674 epoch total loss 1.75880826\n",
      "Trained batch 182 batch loss 1.6140995 epoch total loss 1.75801325\n",
      "Trained batch 183 batch loss 1.68180513 epoch total loss 1.75759673\n",
      "Trained batch 184 batch loss 1.692173 epoch total loss 1.75724113\n",
      "Trained batch 185 batch loss 1.73727965 epoch total loss 1.75713325\n",
      "Trained batch 186 batch loss 1.68597746 epoch total loss 1.75675058\n",
      "Trained batch 187 batch loss 1.73360467 epoch total loss 1.75662684\n",
      "Trained batch 188 batch loss 1.69460988 epoch total loss 1.75629699\n",
      "Trained batch 189 batch loss 1.66235089 epoch total loss 1.7558\n",
      "Trained batch 190 batch loss 1.6153084 epoch total loss 1.75506043\n",
      "Trained batch 191 batch loss 1.60357678 epoch total loss 1.75426733\n",
      "Trained batch 192 batch loss 1.69461524 epoch total loss 1.75395668\n",
      "Trained batch 193 batch loss 1.63917184 epoch total loss 1.75336182\n",
      "Trained batch 194 batch loss 1.60955906 epoch total loss 1.75262058\n",
      "Trained batch 195 batch loss 1.52818143 epoch total loss 1.75146949\n",
      "Trained batch 196 batch loss 1.57145619 epoch total loss 1.75055099\n",
      "Trained batch 197 batch loss 1.63201451 epoch total loss 1.74994934\n",
      "Trained batch 198 batch loss 1.486341 epoch total loss 1.74861789\n",
      "Trained batch 199 batch loss 1.38464952 epoch total loss 1.74678886\n",
      "Trained batch 200 batch loss 1.55099702 epoch total loss 1.74580991\n",
      "Trained batch 201 batch loss 1.62454581 epoch total loss 1.74520659\n",
      "Trained batch 202 batch loss 1.50018597 epoch total loss 1.74399364\n",
      "Trained batch 203 batch loss 1.5324856 epoch total loss 1.74295163\n",
      "Trained batch 204 batch loss 1.59827328 epoch total loss 1.74224246\n",
      "Trained batch 205 batch loss 1.56213558 epoch total loss 1.74136376\n",
      "Trained batch 206 batch loss 1.57764661 epoch total loss 1.740569\n",
      "Trained batch 207 batch loss 1.68354142 epoch total loss 1.7402935\n",
      "Trained batch 208 batch loss 1.65660501 epoch total loss 1.73989117\n",
      "Trained batch 209 batch loss 1.68725276 epoch total loss 1.7396394\n",
      "Trained batch 210 batch loss 1.47422469 epoch total loss 1.73837543\n",
      "Trained batch 211 batch loss 1.36061668 epoch total loss 1.73658514\n",
      "Trained batch 212 batch loss 1.27377653 epoch total loss 1.73440206\n",
      "Trained batch 213 batch loss 1.59856391 epoch total loss 1.73376441\n",
      "Trained batch 214 batch loss 1.6439625 epoch total loss 1.73334467\n",
      "Trained batch 215 batch loss 1.75178385 epoch total loss 1.73343039\n",
      "Trained batch 216 batch loss 1.76651311 epoch total loss 1.73358357\n",
      "Trained batch 217 batch loss 1.72597098 epoch total loss 1.73354852\n",
      "Trained batch 218 batch loss 1.70326209 epoch total loss 1.73340952\n",
      "Trained batch 219 batch loss 1.5529362 epoch total loss 1.73258543\n",
      "Trained batch 220 batch loss 1.43314254 epoch total loss 1.7312243\n",
      "Trained batch 221 batch loss 1.49155271 epoch total loss 1.73013985\n",
      "Trained batch 222 batch loss 1.57086158 epoch total loss 1.72942233\n",
      "Trained batch 223 batch loss 1.6399976 epoch total loss 1.72902131\n",
      "Trained batch 224 batch loss 1.6618005 epoch total loss 1.72872126\n",
      "Trained batch 225 batch loss 1.64256263 epoch total loss 1.72833824\n",
      "Trained batch 226 batch loss 1.65654349 epoch total loss 1.72802055\n",
      "Trained batch 227 batch loss 1.65700817 epoch total loss 1.72770774\n",
      "Trained batch 228 batch loss 1.53910244 epoch total loss 1.72688055\n",
      "Trained batch 229 batch loss 1.56039357 epoch total loss 1.72615349\n",
      "Trained batch 230 batch loss 1.54470801 epoch total loss 1.72536457\n",
      "Trained batch 231 batch loss 1.65484357 epoch total loss 1.72505939\n",
      "Trained batch 232 batch loss 1.68650591 epoch total loss 1.72489309\n",
      "Trained batch 233 batch loss 1.62838316 epoch total loss 1.72447896\n",
      "Trained batch 234 batch loss 1.64334106 epoch total loss 1.72413218\n",
      "Trained batch 235 batch loss 1.6934098 epoch total loss 1.72400153\n",
      "Trained batch 236 batch loss 1.6164906 epoch total loss 1.72354591\n",
      "Trained batch 237 batch loss 1.66412842 epoch total loss 1.72329521\n",
      "Trained batch 238 batch loss 1.65716219 epoch total loss 1.72301733\n",
      "Trained batch 239 batch loss 1.69078267 epoch total loss 1.72288251\n",
      "Trained batch 240 batch loss 1.67989945 epoch total loss 1.72270346\n",
      "Trained batch 241 batch loss 1.69209754 epoch total loss 1.7225765\n",
      "Trained batch 242 batch loss 1.64783 epoch total loss 1.72226763\n",
      "Trained batch 243 batch loss 1.77956235 epoch total loss 1.72250342\n",
      "Trained batch 244 batch loss 1.712834 epoch total loss 1.72246373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 245 batch loss 1.64032269 epoch total loss 1.72212851\n",
      "Trained batch 246 batch loss 1.53555787 epoch total loss 1.7213701\n",
      "Trained batch 247 batch loss 1.53688407 epoch total loss 1.72062314\n",
      "Trained batch 248 batch loss 1.42947161 epoch total loss 1.71944916\n",
      "Trained batch 249 batch loss 1.52767086 epoch total loss 1.71867907\n",
      "Trained batch 250 batch loss 1.50514805 epoch total loss 1.71782494\n",
      "Trained batch 251 batch loss 1.42488134 epoch total loss 1.71665788\n",
      "Trained batch 252 batch loss 1.33387816 epoch total loss 1.71513903\n",
      "Trained batch 253 batch loss 1.35057914 epoch total loss 1.71369803\n",
      "Trained batch 254 batch loss 1.43498874 epoch total loss 1.71260083\n",
      "Trained batch 255 batch loss 1.47236943 epoch total loss 1.71165884\n",
      "Trained batch 256 batch loss 1.60798097 epoch total loss 1.71125376\n",
      "Trained batch 257 batch loss 1.683079 epoch total loss 1.71114409\n",
      "Trained batch 258 batch loss 1.70498145 epoch total loss 1.71112025\n",
      "Trained batch 259 batch loss 1.71806836 epoch total loss 1.71114707\n",
      "Trained batch 260 batch loss 1.69151914 epoch total loss 1.71107161\n",
      "Trained batch 261 batch loss 1.50176716 epoch total loss 1.71026969\n",
      "Trained batch 262 batch loss 1.53113568 epoch total loss 1.70958602\n",
      "Trained batch 263 batch loss 1.60295391 epoch total loss 1.70918059\n",
      "Trained batch 264 batch loss 1.64500058 epoch total loss 1.70893741\n",
      "Trained batch 265 batch loss 1.57827806 epoch total loss 1.70844436\n",
      "Trained batch 266 batch loss 1.63637 epoch total loss 1.70817351\n",
      "Trained batch 267 batch loss 1.62568772 epoch total loss 1.70786464\n",
      "Trained batch 268 batch loss 1.65979922 epoch total loss 1.70768523\n",
      "Trained batch 269 batch loss 1.72361887 epoch total loss 1.70774448\n",
      "Trained batch 270 batch loss 1.6871103 epoch total loss 1.70766807\n",
      "Trained batch 271 batch loss 1.61861968 epoch total loss 1.70733953\n",
      "Trained batch 272 batch loss 1.61072755 epoch total loss 1.70698428\n",
      "Trained batch 273 batch loss 1.61595392 epoch total loss 1.70665085\n",
      "Trained batch 274 batch loss 1.57492185 epoch total loss 1.70617008\n",
      "Trained batch 275 batch loss 1.64617169 epoch total loss 1.70595193\n",
      "Trained batch 276 batch loss 1.65425849 epoch total loss 1.70576465\n",
      "Trained batch 277 batch loss 1.66098213 epoch total loss 1.705603\n",
      "Trained batch 278 batch loss 1.64960921 epoch total loss 1.70540154\n",
      "Trained batch 279 batch loss 1.48962235 epoch total loss 1.70462811\n",
      "Trained batch 280 batch loss 1.58333433 epoch total loss 1.70419502\n",
      "Trained batch 281 batch loss 1.52644074 epoch total loss 1.70356238\n",
      "Trained batch 282 batch loss 1.60907483 epoch total loss 1.70322728\n",
      "Trained batch 283 batch loss 1.59751678 epoch total loss 1.70285368\n",
      "Trained batch 284 batch loss 1.57917166 epoch total loss 1.70241821\n",
      "Trained batch 285 batch loss 1.59604204 epoch total loss 1.70204484\n",
      "Trained batch 286 batch loss 1.66559076 epoch total loss 1.70191741\n",
      "Trained batch 287 batch loss 1.69925356 epoch total loss 1.70190811\n",
      "Trained batch 288 batch loss 1.66989779 epoch total loss 1.70179701\n",
      "Trained batch 289 batch loss 1.62834883 epoch total loss 1.70154285\n",
      "Trained batch 290 batch loss 1.56584728 epoch total loss 1.70107496\n",
      "Trained batch 291 batch loss 1.56846559 epoch total loss 1.70061922\n",
      "Trained batch 292 batch loss 1.6276654 epoch total loss 1.70036936\n",
      "Trained batch 293 batch loss 1.67828083 epoch total loss 1.7002939\n",
      "Trained batch 294 batch loss 1.68229353 epoch total loss 1.70023263\n",
      "Trained batch 295 batch loss 1.665308 epoch total loss 1.70011437\n",
      "Trained batch 296 batch loss 1.75693798 epoch total loss 1.7003063\n",
      "Trained batch 297 batch loss 1.75806928 epoch total loss 1.70050073\n",
      "Trained batch 298 batch loss 1.69712007 epoch total loss 1.70048928\n",
      "Trained batch 299 batch loss 1.74784565 epoch total loss 1.70064771\n",
      "Trained batch 300 batch loss 1.79512608 epoch total loss 1.70096266\n",
      "Trained batch 301 batch loss 1.73267937 epoch total loss 1.70106804\n",
      "Trained batch 302 batch loss 1.69461441 epoch total loss 1.70104682\n",
      "Trained batch 303 batch loss 1.59026027 epoch total loss 1.70068121\n",
      "Trained batch 304 batch loss 1.59357405 epoch total loss 1.70032883\n",
      "Trained batch 305 batch loss 1.63475657 epoch total loss 1.70011389\n",
      "Trained batch 306 batch loss 1.63641024 epoch total loss 1.69990575\n",
      "Trained batch 307 batch loss 1.58015382 epoch total loss 1.69951558\n",
      "Trained batch 308 batch loss 1.54511929 epoch total loss 1.69901419\n",
      "Trained batch 309 batch loss 1.66454697 epoch total loss 1.69890273\n",
      "Trained batch 310 batch loss 1.53539121 epoch total loss 1.69837523\n",
      "Trained batch 311 batch loss 1.58491063 epoch total loss 1.69801044\n",
      "Trained batch 312 batch loss 1.52536333 epoch total loss 1.69745719\n",
      "Trained batch 313 batch loss 1.63663888 epoch total loss 1.69726288\n",
      "Trained batch 314 batch loss 1.69205284 epoch total loss 1.69724643\n",
      "Trained batch 315 batch loss 1.6898886 epoch total loss 1.69722295\n",
      "Trained batch 316 batch loss 1.66930771 epoch total loss 1.69713461\n",
      "Trained batch 317 batch loss 1.64606452 epoch total loss 1.69697356\n",
      "Trained batch 318 batch loss 1.72903895 epoch total loss 1.69707441\n",
      "Trained batch 319 batch loss 1.67530298 epoch total loss 1.69700623\n",
      "Trained batch 320 batch loss 1.65666676 epoch total loss 1.6968801\n",
      "Trained batch 321 batch loss 1.68528223 epoch total loss 1.6968441\n",
      "Trained batch 322 batch loss 1.70400846 epoch total loss 1.69686627\n",
      "Trained batch 323 batch loss 1.68215418 epoch total loss 1.69682062\n",
      "Trained batch 324 batch loss 1.63555515 epoch total loss 1.69663155\n",
      "Trained batch 325 batch loss 1.60228014 epoch total loss 1.69634128\n",
      "Trained batch 326 batch loss 1.53046513 epoch total loss 1.69583237\n",
      "Trained batch 327 batch loss 1.61793494 epoch total loss 1.69559419\n",
      "Trained batch 328 batch loss 1.58663666 epoch total loss 1.69526184\n",
      "Trained batch 329 batch loss 1.62240982 epoch total loss 1.69504058\n",
      "Trained batch 330 batch loss 1.67190242 epoch total loss 1.69497037\n",
      "Trained batch 331 batch loss 1.65381837 epoch total loss 1.69484603\n",
      "Trained batch 332 batch loss 1.63734806 epoch total loss 1.6946727\n",
      "Trained batch 333 batch loss 1.61556685 epoch total loss 1.69443512\n",
      "Trained batch 334 batch loss 1.64839029 epoch total loss 1.69429719\n",
      "Trained batch 335 batch loss 1.61260486 epoch total loss 1.69405341\n",
      "Trained batch 336 batch loss 1.66170168 epoch total loss 1.69395697\n",
      "Trained batch 337 batch loss 1.60567927 epoch total loss 1.69369495\n",
      "Trained batch 338 batch loss 1.62780976 epoch total loss 1.6935\n",
      "Trained batch 339 batch loss 1.58097315 epoch total loss 1.69316816\n",
      "Trained batch 340 batch loss 1.51377964 epoch total loss 1.69264054\n",
      "Trained batch 341 batch loss 1.43638277 epoch total loss 1.69188917\n",
      "Trained batch 342 batch loss 1.63801837 epoch total loss 1.69173157\n",
      "Trained batch 343 batch loss 1.56791925 epoch total loss 1.69137061\n",
      "Trained batch 344 batch loss 1.66530108 epoch total loss 1.69129479\n",
      "Trained batch 345 batch loss 1.57400525 epoch total loss 1.69095492\n",
      "Trained batch 346 batch loss 1.56935501 epoch total loss 1.69060349\n",
      "Trained batch 347 batch loss 1.51076579 epoch total loss 1.69008505\n",
      "Trained batch 348 batch loss 1.53797317 epoch total loss 1.68964803\n",
      "Trained batch 349 batch loss 1.62941086 epoch total loss 1.6894753\n",
      "Trained batch 350 batch loss 1.67451501 epoch total loss 1.6894325\n",
      "Trained batch 351 batch loss 1.64049 epoch total loss 1.68929315\n",
      "Trained batch 352 batch loss 1.55710828 epoch total loss 1.68891764\n",
      "Trained batch 353 batch loss 1.63122964 epoch total loss 1.6887542\n",
      "Trained batch 354 batch loss 1.54527164 epoch total loss 1.68834901\n",
      "Trained batch 355 batch loss 1.57669 epoch total loss 1.68803442\n",
      "Trained batch 356 batch loss 1.55810356 epoch total loss 1.6876694\n",
      "Trained batch 357 batch loss 1.58504963 epoch total loss 1.68738186\n",
      "Trained batch 358 batch loss 1.51647162 epoch total loss 1.68690443\n",
      "Trained batch 359 batch loss 1.57870007 epoch total loss 1.68660295\n",
      "Trained batch 360 batch loss 1.47585595 epoch total loss 1.68601751\n",
      "Trained batch 361 batch loss 1.58910394 epoch total loss 1.68574905\n",
      "Trained batch 362 batch loss 1.56240153 epoch total loss 1.68540823\n",
      "Trained batch 363 batch loss 1.5115118 epoch total loss 1.68492925\n",
      "Trained batch 364 batch loss 1.4571985 epoch total loss 1.68430376\n",
      "Trained batch 365 batch loss 1.55153096 epoch total loss 1.68393993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 366 batch loss 1.48406672 epoch total loss 1.68339384\n",
      "Trained batch 367 batch loss 1.59274232 epoch total loss 1.68314672\n",
      "Trained batch 368 batch loss 1.6145401 epoch total loss 1.68296039\n",
      "Trained batch 369 batch loss 1.59187508 epoch total loss 1.68271351\n",
      "Trained batch 370 batch loss 1.67742348 epoch total loss 1.6826992\n",
      "Trained batch 371 batch loss 1.43172038 epoch total loss 1.68202257\n",
      "Trained batch 372 batch loss 1.4152565 epoch total loss 1.68130565\n",
      "Trained batch 373 batch loss 1.42271698 epoch total loss 1.68061233\n",
      "Trained batch 374 batch loss 1.52113795 epoch total loss 1.68018591\n",
      "Trained batch 375 batch loss 1.72290301 epoch total loss 1.68029976\n",
      "Trained batch 376 batch loss 1.767416 epoch total loss 1.68053138\n",
      "Trained batch 377 batch loss 1.63780916 epoch total loss 1.68041813\n",
      "Trained batch 378 batch loss 1.66615629 epoch total loss 1.68038034\n",
      "Trained batch 379 batch loss 1.65994203 epoch total loss 1.68032634\n",
      "Trained batch 380 batch loss 1.63844228 epoch total loss 1.68021607\n",
      "Trained batch 381 batch loss 1.58572197 epoch total loss 1.679968\n",
      "Trained batch 382 batch loss 1.54197669 epoch total loss 1.6796068\n",
      "Trained batch 383 batch loss 1.59875703 epoch total loss 1.67939568\n",
      "Trained batch 384 batch loss 1.6098516 epoch total loss 1.6792146\n",
      "Trained batch 385 batch loss 1.61003685 epoch total loss 1.67903495\n",
      "Trained batch 386 batch loss 1.60705614 epoch total loss 1.67884851\n",
      "Trained batch 387 batch loss 1.59584224 epoch total loss 1.67863393\n",
      "Trained batch 388 batch loss 1.53967404 epoch total loss 1.67827582\n",
      "Trained batch 389 batch loss 1.6144613 epoch total loss 1.67811167\n",
      "Trained batch 390 batch loss 1.56992948 epoch total loss 1.67783439\n",
      "Trained batch 391 batch loss 1.57980943 epoch total loss 1.67758369\n",
      "Trained batch 392 batch loss 1.5687294 epoch total loss 1.67730606\n",
      "Trained batch 393 batch loss 1.58686531 epoch total loss 1.67707586\n",
      "Trained batch 394 batch loss 1.53056264 epoch total loss 1.67670405\n",
      "Trained batch 395 batch loss 1.60536587 epoch total loss 1.67652345\n",
      "Trained batch 396 batch loss 1.56350565 epoch total loss 1.67623794\n",
      "Trained batch 397 batch loss 1.59850538 epoch total loss 1.6760422\n",
      "Trained batch 398 batch loss 1.60907984 epoch total loss 1.67587388\n",
      "Trained batch 399 batch loss 1.59373403 epoch total loss 1.675668\n",
      "Trained batch 400 batch loss 1.519732 epoch total loss 1.67527819\n",
      "Trained batch 401 batch loss 1.58468294 epoch total loss 1.67505217\n",
      "Trained batch 402 batch loss 1.60085833 epoch total loss 1.67486751\n",
      "Trained batch 403 batch loss 1.50748 epoch total loss 1.6744523\n",
      "Trained batch 404 batch loss 1.59268212 epoch total loss 1.67424989\n",
      "Trained batch 405 batch loss 1.64969957 epoch total loss 1.67418933\n",
      "Trained batch 406 batch loss 1.68556392 epoch total loss 1.67421734\n",
      "Trained batch 407 batch loss 1.65995777 epoch total loss 1.6741823\n",
      "Trained batch 408 batch loss 1.8043474 epoch total loss 1.6745013\n",
      "Trained batch 409 batch loss 1.70393205 epoch total loss 1.67457318\n",
      "Trained batch 410 batch loss 1.72609591 epoch total loss 1.67469883\n",
      "Trained batch 411 batch loss 1.63423145 epoch total loss 1.67460036\n",
      "Trained batch 412 batch loss 1.56442666 epoch total loss 1.67433298\n",
      "Trained batch 413 batch loss 1.56594944 epoch total loss 1.6740706\n",
      "Trained batch 414 batch loss 1.58211374 epoch total loss 1.67384851\n",
      "Trained batch 415 batch loss 1.53360057 epoch total loss 1.67351055\n",
      "Trained batch 416 batch loss 1.59910059 epoch total loss 1.67333174\n",
      "Trained batch 417 batch loss 1.53635681 epoch total loss 1.67300332\n",
      "Trained batch 418 batch loss 1.59186959 epoch total loss 1.67280924\n",
      "Trained batch 419 batch loss 1.50078344 epoch total loss 1.67239869\n",
      "Trained batch 420 batch loss 1.41384792 epoch total loss 1.67178297\n",
      "Trained batch 421 batch loss 1.35949612 epoch total loss 1.67104125\n",
      "Trained batch 422 batch loss 1.24550354 epoch total loss 1.67003286\n",
      "Trained batch 423 batch loss 1.553684 epoch total loss 1.66975784\n",
      "Trained batch 424 batch loss 1.56432891 epoch total loss 1.66950917\n",
      "Trained batch 425 batch loss 1.71548212 epoch total loss 1.6696173\n",
      "Trained batch 426 batch loss 1.60575974 epoch total loss 1.66946745\n",
      "Trained batch 427 batch loss 1.60350358 epoch total loss 1.66931295\n",
      "Trained batch 428 batch loss 1.53417385 epoch total loss 1.66899717\n",
      "Trained batch 429 batch loss 1.49952412 epoch total loss 1.66860211\n",
      "Trained batch 430 batch loss 1.47744095 epoch total loss 1.66815758\n",
      "Trained batch 431 batch loss 1.50477958 epoch total loss 1.66777837\n",
      "Trained batch 432 batch loss 1.48408973 epoch total loss 1.66735315\n",
      "Trained batch 433 batch loss 1.49646533 epoch total loss 1.66695845\n",
      "Trained batch 434 batch loss 1.61358595 epoch total loss 1.66683555\n",
      "Trained batch 435 batch loss 1.43792617 epoch total loss 1.66630924\n",
      "Trained batch 436 batch loss 1.49484491 epoch total loss 1.66591609\n",
      "Trained batch 437 batch loss 1.63940477 epoch total loss 1.66585541\n",
      "Trained batch 438 batch loss 1.49537086 epoch total loss 1.66546619\n",
      "Trained batch 439 batch loss 1.50201619 epoch total loss 1.6650939\n",
      "Trained batch 440 batch loss 1.60563707 epoch total loss 1.66495872\n",
      "Trained batch 441 batch loss 1.55382466 epoch total loss 1.66470671\n",
      "Trained batch 442 batch loss 1.52942562 epoch total loss 1.6644007\n",
      "Trained batch 443 batch loss 1.49669218 epoch total loss 1.66402209\n",
      "Trained batch 444 batch loss 1.32078528 epoch total loss 1.66324914\n",
      "Trained batch 445 batch loss 1.36858249 epoch total loss 1.66258693\n",
      "Trained batch 446 batch loss 1.47479546 epoch total loss 1.66216588\n",
      "Trained batch 447 batch loss 1.54379272 epoch total loss 1.661901\n",
      "Trained batch 448 batch loss 1.50225782 epoch total loss 1.66154468\n",
      "Trained batch 449 batch loss 1.5449841 epoch total loss 1.66128504\n",
      "Trained batch 450 batch loss 1.63788497 epoch total loss 1.66123307\n",
      "Trained batch 451 batch loss 1.54645634 epoch total loss 1.66097856\n",
      "Trained batch 452 batch loss 1.52760077 epoch total loss 1.66068339\n",
      "Trained batch 453 batch loss 1.66638446 epoch total loss 1.66069603\n",
      "Trained batch 454 batch loss 1.72618866 epoch total loss 1.66084027\n",
      "Trained batch 455 batch loss 1.71439886 epoch total loss 1.66095805\n",
      "Trained batch 456 batch loss 1.58819747 epoch total loss 1.66079843\n",
      "Trained batch 457 batch loss 1.58755422 epoch total loss 1.66063809\n",
      "Trained batch 458 batch loss 1.52670503 epoch total loss 1.66034579\n",
      "Trained batch 459 batch loss 1.60982263 epoch total loss 1.66023564\n",
      "Trained batch 460 batch loss 1.4213661 epoch total loss 1.65971637\n",
      "Trained batch 461 batch loss 1.48071134 epoch total loss 1.6593281\n",
      "Trained batch 462 batch loss 1.55948222 epoch total loss 1.6591121\n",
      "Trained batch 463 batch loss 1.53884852 epoch total loss 1.65885222\n",
      "Trained batch 464 batch loss 1.5884037 epoch total loss 1.65870035\n",
      "Trained batch 465 batch loss 1.50982785 epoch total loss 1.65838015\n",
      "Trained batch 466 batch loss 1.62557673 epoch total loss 1.6583097\n",
      "Trained batch 467 batch loss 1.57252038 epoch total loss 1.658126\n",
      "Trained batch 468 batch loss 1.49376512 epoch total loss 1.65777481\n",
      "Trained batch 469 batch loss 1.5341177 epoch total loss 1.65751123\n",
      "Trained batch 470 batch loss 1.62634706 epoch total loss 1.65744483\n",
      "Trained batch 471 batch loss 1.54281437 epoch total loss 1.65720141\n",
      "Trained batch 472 batch loss 1.57403922 epoch total loss 1.65702522\n",
      "Trained batch 473 batch loss 1.65892434 epoch total loss 1.65702927\n",
      "Trained batch 474 batch loss 1.57262921 epoch total loss 1.65685117\n",
      "Trained batch 475 batch loss 1.5194124 epoch total loss 1.65656185\n",
      "Trained batch 476 batch loss 1.47700381 epoch total loss 1.65618455\n",
      "Trained batch 477 batch loss 1.66289067 epoch total loss 1.65619874\n",
      "Trained batch 478 batch loss 1.56109262 epoch total loss 1.65599978\n",
      "Trained batch 479 batch loss 1.59424686 epoch total loss 1.6558708\n",
      "Trained batch 480 batch loss 1.52096176 epoch total loss 1.6555897\n",
      "Trained batch 481 batch loss 1.56048596 epoch total loss 1.65539193\n",
      "Trained batch 482 batch loss 1.56789589 epoch total loss 1.65521038\n",
      "Trained batch 483 batch loss 1.53668857 epoch total loss 1.65496492\n",
      "Trained batch 484 batch loss 1.52798939 epoch total loss 1.65470266\n",
      "Trained batch 485 batch loss 1.53922307 epoch total loss 1.6544646\n",
      "Trained batch 486 batch loss 1.50566924 epoch total loss 1.65415847\n",
      "Trained batch 487 batch loss 1.51402402 epoch total loss 1.6538707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 488 batch loss 1.57375264 epoch total loss 1.65370655\n",
      "Trained batch 489 batch loss 1.50649095 epoch total loss 1.65340543\n",
      "Trained batch 490 batch loss 1.5341723 epoch total loss 1.65316212\n",
      "Trained batch 491 batch loss 1.58050144 epoch total loss 1.65301418\n",
      "Trained batch 492 batch loss 1.62065887 epoch total loss 1.65294838\n",
      "Trained batch 493 batch loss 1.66780055 epoch total loss 1.65297854\n",
      "Trained batch 494 batch loss 1.65012407 epoch total loss 1.65297282\n",
      "Trained batch 495 batch loss 1.55416179 epoch total loss 1.65277314\n",
      "Trained batch 496 batch loss 1.56386566 epoch total loss 1.65259385\n",
      "Trained batch 497 batch loss 1.61935139 epoch total loss 1.65252686\n",
      "Trained batch 498 batch loss 1.57529342 epoch total loss 1.65237176\n",
      "Trained batch 499 batch loss 1.61630964 epoch total loss 1.65229964\n",
      "Trained batch 500 batch loss 1.54213965 epoch total loss 1.65207922\n",
      "Trained batch 501 batch loss 1.54336369 epoch total loss 1.65186214\n",
      "Trained batch 502 batch loss 1.5719074 epoch total loss 1.65170288\n",
      "Trained batch 503 batch loss 1.53793645 epoch total loss 1.65147674\n",
      "Trained batch 504 batch loss 1.41934884 epoch total loss 1.65101624\n",
      "Trained batch 505 batch loss 1.56218576 epoch total loss 1.6508404\n",
      "Trained batch 506 batch loss 1.47449148 epoch total loss 1.65049183\n",
      "Trained batch 507 batch loss 1.5391891 epoch total loss 1.65027225\n",
      "Trained batch 508 batch loss 1.5115217 epoch total loss 1.64999914\n",
      "Trained batch 509 batch loss 1.45857298 epoch total loss 1.64962304\n",
      "Trained batch 510 batch loss 1.45219743 epoch total loss 1.64923596\n",
      "Trained batch 511 batch loss 1.60988426 epoch total loss 1.64915895\n",
      "Trained batch 512 batch loss 1.59879088 epoch total loss 1.64906061\n",
      "Trained batch 513 batch loss 1.54372299 epoch total loss 1.64885521\n",
      "Trained batch 514 batch loss 1.53687978 epoch total loss 1.64863729\n",
      "Trained batch 515 batch loss 1.487746 epoch total loss 1.64832497\n",
      "Trained batch 516 batch loss 1.50280106 epoch total loss 1.64804292\n",
      "Trained batch 517 batch loss 1.56739676 epoch total loss 1.64788687\n",
      "Trained batch 518 batch loss 1.52791035 epoch total loss 1.64765525\n",
      "Trained batch 519 batch loss 1.47798574 epoch total loss 1.64732826\n",
      "Trained batch 520 batch loss 1.57153428 epoch total loss 1.64718246\n",
      "Trained batch 521 batch loss 1.59747243 epoch total loss 1.6470871\n",
      "Trained batch 522 batch loss 1.59434462 epoch total loss 1.64698613\n",
      "Trained batch 523 batch loss 1.50403619 epoch total loss 1.64671278\n",
      "Trained batch 524 batch loss 1.52506375 epoch total loss 1.64648068\n",
      "Trained batch 525 batch loss 1.52561188 epoch total loss 1.64625049\n",
      "Trained batch 526 batch loss 1.57899928 epoch total loss 1.64612257\n",
      "Trained batch 527 batch loss 1.51444459 epoch total loss 1.64587271\n",
      "Trained batch 528 batch loss 1.44035685 epoch total loss 1.64548349\n",
      "Trained batch 529 batch loss 1.51294 epoch total loss 1.64523304\n",
      "Trained batch 530 batch loss 1.52353513 epoch total loss 1.64500344\n",
      "Trained batch 531 batch loss 1.635131 epoch total loss 1.64498484\n",
      "Trained batch 532 batch loss 1.60247087 epoch total loss 1.64490497\n",
      "Trained batch 533 batch loss 1.55475318 epoch total loss 1.64473581\n",
      "Trained batch 534 batch loss 1.55665302 epoch total loss 1.64457083\n",
      "Trained batch 535 batch loss 1.60047162 epoch total loss 1.64448833\n",
      "Trained batch 536 batch loss 1.61840272 epoch total loss 1.6444397\n",
      "Trained batch 537 batch loss 1.67310441 epoch total loss 1.6444931\n",
      "Trained batch 538 batch loss 1.56679082 epoch total loss 1.64434862\n",
      "Trained batch 539 batch loss 1.49302053 epoch total loss 1.64406788\n",
      "Trained batch 540 batch loss 1.53369439 epoch total loss 1.64386344\n",
      "Trained batch 541 batch loss 1.42468023 epoch total loss 1.64345837\n",
      "Trained batch 542 batch loss 1.51148045 epoch total loss 1.64321482\n",
      "Trained batch 543 batch loss 1.52860212 epoch total loss 1.64300382\n",
      "Trained batch 544 batch loss 1.5301764 epoch total loss 1.6427964\n",
      "Trained batch 545 batch loss 1.52874398 epoch total loss 1.64258707\n",
      "Trained batch 546 batch loss 1.48702395 epoch total loss 1.64230216\n",
      "Trained batch 547 batch loss 1.63256168 epoch total loss 1.64228427\n",
      "Trained batch 548 batch loss 1.48073149 epoch total loss 1.64198947\n",
      "Trained batch 549 batch loss 1.55532479 epoch total loss 1.64183164\n",
      "Trained batch 550 batch loss 1.45179081 epoch total loss 1.64148605\n",
      "Trained batch 551 batch loss 1.51812863 epoch total loss 1.64126217\n",
      "Trained batch 552 batch loss 1.50088906 epoch total loss 1.6410079\n",
      "Trained batch 553 batch loss 1.5426178 epoch total loss 1.64082992\n",
      "Trained batch 554 batch loss 1.58190954 epoch total loss 1.64072359\n",
      "Trained batch 555 batch loss 1.62127 epoch total loss 1.64068854\n",
      "Trained batch 556 batch loss 1.6379174 epoch total loss 1.64068365\n",
      "Trained batch 557 batch loss 1.71022248 epoch total loss 1.64080846\n",
      "Trained batch 558 batch loss 1.61664295 epoch total loss 1.64076507\n",
      "Trained batch 559 batch loss 1.63878465 epoch total loss 1.64076161\n",
      "Trained batch 560 batch loss 1.45200205 epoch total loss 1.64042461\n",
      "Trained batch 561 batch loss 1.39306188 epoch total loss 1.63998365\n",
      "Trained batch 562 batch loss 1.33831644 epoch total loss 1.63944685\n",
      "Trained batch 563 batch loss 1.27765775 epoch total loss 1.6388042\n",
      "Trained batch 564 batch loss 1.52032208 epoch total loss 1.63859415\n",
      "Trained batch 565 batch loss 1.51799417 epoch total loss 1.63838077\n",
      "Trained batch 566 batch loss 1.51692426 epoch total loss 1.63816607\n",
      "Trained batch 567 batch loss 1.5844698 epoch total loss 1.63807142\n",
      "Trained batch 568 batch loss 1.60928655 epoch total loss 1.63802075\n",
      "Trained batch 569 batch loss 1.57416832 epoch total loss 1.63790858\n",
      "Trained batch 570 batch loss 1.54571199 epoch total loss 1.63774681\n",
      "Trained batch 571 batch loss 1.57979536 epoch total loss 1.63764524\n",
      "Trained batch 572 batch loss 1.57533467 epoch total loss 1.63753629\n",
      "Trained batch 573 batch loss 1.52157569 epoch total loss 1.63733387\n",
      "Trained batch 574 batch loss 1.59508443 epoch total loss 1.63726032\n",
      "Trained batch 575 batch loss 1.64577365 epoch total loss 1.6372751\n",
      "Trained batch 576 batch loss 1.62917161 epoch total loss 1.63726091\n",
      "Trained batch 577 batch loss 1.64353347 epoch total loss 1.63727188\n",
      "Trained batch 578 batch loss 1.69471991 epoch total loss 1.63737118\n",
      "Trained batch 579 batch loss 1.59066892 epoch total loss 1.6372906\n",
      "Trained batch 580 batch loss 1.64626026 epoch total loss 1.63730609\n",
      "Trained batch 581 batch loss 1.58310366 epoch total loss 1.63721275\n",
      "Trained batch 582 batch loss 1.60274696 epoch total loss 1.63715351\n",
      "Trained batch 583 batch loss 1.64442253 epoch total loss 1.63716602\n",
      "Trained batch 584 batch loss 1.54539704 epoch total loss 1.63700891\n",
      "Trained batch 585 batch loss 1.48429906 epoch total loss 1.63674784\n",
      "Trained batch 586 batch loss 1.60104704 epoch total loss 1.63668704\n",
      "Trained batch 587 batch loss 1.60756123 epoch total loss 1.63663733\n",
      "Trained batch 588 batch loss 1.63208783 epoch total loss 1.63662958\n",
      "Trained batch 589 batch loss 1.75231469 epoch total loss 1.63682604\n",
      "Trained batch 590 batch loss 1.66708183 epoch total loss 1.63687718\n",
      "Trained batch 591 batch loss 1.54943395 epoch total loss 1.63672924\n",
      "Trained batch 592 batch loss 1.64226437 epoch total loss 1.63673866\n",
      "Trained batch 593 batch loss 1.49004114 epoch total loss 1.6364913\n",
      "Trained batch 594 batch loss 1.59848928 epoch total loss 1.6364274\n",
      "Trained batch 595 batch loss 1.59368742 epoch total loss 1.63635552\n",
      "Trained batch 596 batch loss 1.55837345 epoch total loss 1.63622463\n",
      "Trained batch 597 batch loss 1.59908164 epoch total loss 1.6361624\n",
      "Trained batch 598 batch loss 1.51209021 epoch total loss 1.63595486\n",
      "Trained batch 599 batch loss 1.48759294 epoch total loss 1.63570726\n",
      "Trained batch 600 batch loss 1.49004543 epoch total loss 1.63546443\n",
      "Trained batch 601 batch loss 1.57570553 epoch total loss 1.63536501\n",
      "Trained batch 602 batch loss 1.66974878 epoch total loss 1.63542211\n",
      "Trained batch 603 batch loss 1.77646482 epoch total loss 1.635656\n",
      "Trained batch 604 batch loss 1.65783393 epoch total loss 1.63569272\n",
      "Trained batch 605 batch loss 1.62320983 epoch total loss 1.63567221\n",
      "Trained batch 606 batch loss 1.66202486 epoch total loss 1.63571572\n",
      "Trained batch 607 batch loss 1.63497329 epoch total loss 1.63571441\n",
      "Trained batch 608 batch loss 1.34461653 epoch total loss 1.63523567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 609 batch loss 1.36580944 epoch total loss 1.63479316\n",
      "Trained batch 610 batch loss 1.55482757 epoch total loss 1.63466203\n",
      "Trained batch 611 batch loss 1.62492085 epoch total loss 1.63464618\n",
      "Trained batch 612 batch loss 1.63266027 epoch total loss 1.63464296\n",
      "Trained batch 613 batch loss 1.58890486 epoch total loss 1.63456833\n",
      "Trained batch 614 batch loss 1.6335566 epoch total loss 1.63456666\n",
      "Trained batch 615 batch loss 1.62499809 epoch total loss 1.63455117\n",
      "Trained batch 616 batch loss 1.5364852 epoch total loss 1.63439202\n",
      "Trained batch 617 batch loss 1.6471324 epoch total loss 1.63441265\n",
      "Trained batch 618 batch loss 1.57605231 epoch total loss 1.63431823\n",
      "Trained batch 619 batch loss 1.60656536 epoch total loss 1.63427341\n",
      "Trained batch 620 batch loss 1.59523463 epoch total loss 1.63421035\n",
      "Trained batch 621 batch loss 1.54673398 epoch total loss 1.63406956\n",
      "Trained batch 622 batch loss 1.3562386 epoch total loss 1.63362288\n",
      "Trained batch 623 batch loss 1.49752569 epoch total loss 1.63340437\n",
      "Trained batch 624 batch loss 1.51375675 epoch total loss 1.63321269\n",
      "Trained batch 625 batch loss 1.59014773 epoch total loss 1.63314378\n",
      "Trained batch 626 batch loss 1.59856677 epoch total loss 1.63308847\n",
      "Trained batch 627 batch loss 1.52958512 epoch total loss 1.63292348\n",
      "Trained batch 628 batch loss 1.63919246 epoch total loss 1.63293338\n",
      "Trained batch 629 batch loss 1.38910604 epoch total loss 1.63254583\n",
      "Trained batch 630 batch loss 1.48904133 epoch total loss 1.63231802\n",
      "Trained batch 631 batch loss 1.59962583 epoch total loss 1.63226616\n",
      "Trained batch 632 batch loss 1.65304375 epoch total loss 1.63229907\n",
      "Trained batch 633 batch loss 1.51181936 epoch total loss 1.63210881\n",
      "Trained batch 634 batch loss 1.55363297 epoch total loss 1.63198495\n",
      "Trained batch 635 batch loss 1.57662463 epoch total loss 1.63189781\n",
      "Trained batch 636 batch loss 1.53034019 epoch total loss 1.63173819\n",
      "Trained batch 637 batch loss 1.56113124 epoch total loss 1.63162744\n",
      "Trained batch 638 batch loss 1.60895979 epoch total loss 1.63159204\n",
      "Trained batch 639 batch loss 1.57369781 epoch total loss 1.63150144\n",
      "Trained batch 640 batch loss 1.44805145 epoch total loss 1.63121474\n",
      "Trained batch 641 batch loss 1.57416832 epoch total loss 1.63112581\n",
      "Trained batch 642 batch loss 1.44763017 epoch total loss 1.63084\n",
      "Trained batch 643 batch loss 1.51353598 epoch total loss 1.63065755\n",
      "Trained batch 644 batch loss 1.52678311 epoch total loss 1.63049614\n",
      "Trained batch 645 batch loss 1.58235359 epoch total loss 1.63042164\n",
      "Trained batch 646 batch loss 1.50429285 epoch total loss 1.63022637\n",
      "Trained batch 647 batch loss 1.49077857 epoch total loss 1.63001072\n",
      "Trained batch 648 batch loss 1.53518617 epoch total loss 1.62986434\n",
      "Trained batch 649 batch loss 1.65734613 epoch total loss 1.62990665\n",
      "Trained batch 650 batch loss 1.55889165 epoch total loss 1.62979734\n",
      "Trained batch 651 batch loss 1.54984474 epoch total loss 1.62967443\n",
      "Trained batch 652 batch loss 1.49262261 epoch total loss 1.62946439\n",
      "Trained batch 653 batch loss 1.52751279 epoch total loss 1.62930822\n",
      "Trained batch 654 batch loss 1.51377 epoch total loss 1.62913156\n",
      "Trained batch 655 batch loss 1.53556228 epoch total loss 1.62898862\n",
      "Trained batch 656 batch loss 1.60551763 epoch total loss 1.62895274\n",
      "Trained batch 657 batch loss 1.57622433 epoch total loss 1.62887239\n",
      "Trained batch 658 batch loss 1.52369678 epoch total loss 1.62871253\n",
      "Trained batch 659 batch loss 1.45066416 epoch total loss 1.62844241\n",
      "Trained batch 660 batch loss 1.45293641 epoch total loss 1.62817645\n",
      "Trained batch 661 batch loss 1.5856514 epoch total loss 1.6281122\n",
      "Trained batch 662 batch loss 1.57398629 epoch total loss 1.62803042\n",
      "Trained batch 663 batch loss 1.50365555 epoch total loss 1.62784278\n",
      "Trained batch 664 batch loss 1.56635356 epoch total loss 1.62775028\n",
      "Trained batch 665 batch loss 1.55685604 epoch total loss 1.6276437\n",
      "Trained batch 666 batch loss 1.52553415 epoch total loss 1.6274904\n",
      "Trained batch 667 batch loss 1.58863091 epoch total loss 1.62743211\n",
      "Trained batch 668 batch loss 1.39214158 epoch total loss 1.62707973\n",
      "Trained batch 669 batch loss 1.44161761 epoch total loss 1.62680256\n",
      "Trained batch 670 batch loss 1.52347088 epoch total loss 1.62664831\n",
      "Trained batch 671 batch loss 1.57687 epoch total loss 1.62657416\n",
      "Trained batch 672 batch loss 1.5803076 epoch total loss 1.62650537\n",
      "Trained batch 673 batch loss 1.55421937 epoch total loss 1.62639797\n",
      "Trained batch 674 batch loss 1.49945498 epoch total loss 1.62620962\n",
      "Trained batch 675 batch loss 1.48830533 epoch total loss 1.62600529\n",
      "Trained batch 676 batch loss 1.46339941 epoch total loss 1.62576473\n",
      "Trained batch 677 batch loss 1.57791233 epoch total loss 1.62569404\n",
      "Trained batch 678 batch loss 1.64980102 epoch total loss 1.62572956\n",
      "Trained batch 679 batch loss 1.68246007 epoch total loss 1.62581313\n",
      "Trained batch 680 batch loss 1.58407807 epoch total loss 1.62575185\n",
      "Trained batch 681 batch loss 1.66381073 epoch total loss 1.62580776\n",
      "Trained batch 682 batch loss 1.57937193 epoch total loss 1.62573957\n",
      "Trained batch 683 batch loss 1.61567128 epoch total loss 1.62572491\n",
      "Trained batch 684 batch loss 1.53503013 epoch total loss 1.62559235\n",
      "Trained batch 685 batch loss 1.54423237 epoch total loss 1.6254735\n",
      "Trained batch 686 batch loss 1.5377903 epoch total loss 1.62534571\n",
      "Trained batch 687 batch loss 1.53299522 epoch total loss 1.62521124\n",
      "Trained batch 688 batch loss 1.58615303 epoch total loss 1.6251545\n",
      "Trained batch 689 batch loss 1.6074245 epoch total loss 1.62512875\n",
      "Trained batch 690 batch loss 1.58177614 epoch total loss 1.62506604\n",
      "Trained batch 691 batch loss 1.48578262 epoch total loss 1.62486446\n",
      "Trained batch 692 batch loss 1.4713 epoch total loss 1.62464261\n",
      "Trained batch 693 batch loss 1.53972745 epoch total loss 1.62452\n",
      "Trained batch 694 batch loss 1.43453813 epoch total loss 1.62424624\n",
      "Trained batch 695 batch loss 1.52521729 epoch total loss 1.6241039\n",
      "Trained batch 696 batch loss 1.48960483 epoch total loss 1.62391067\n",
      "Trained batch 697 batch loss 1.54527593 epoch total loss 1.62379789\n",
      "Trained batch 698 batch loss 1.5467391 epoch total loss 1.62368751\n",
      "Trained batch 699 batch loss 1.46601951 epoch total loss 1.62346196\n",
      "Trained batch 700 batch loss 1.5833832 epoch total loss 1.62340474\n",
      "Trained batch 701 batch loss 1.60181308 epoch total loss 1.62337387\n",
      "Trained batch 702 batch loss 1.52861583 epoch total loss 1.6232388\n",
      "Trained batch 703 batch loss 1.58742595 epoch total loss 1.6231879\n",
      "Trained batch 704 batch loss 1.55473697 epoch total loss 1.62309062\n",
      "Trained batch 705 batch loss 1.52593648 epoch total loss 1.6229527\n",
      "Trained batch 706 batch loss 1.41364574 epoch total loss 1.62265623\n",
      "Trained batch 707 batch loss 1.38508189 epoch total loss 1.62232029\n",
      "Trained batch 708 batch loss 1.40019226 epoch total loss 1.62200654\n",
      "Trained batch 709 batch loss 1.49391854 epoch total loss 1.62182581\n",
      "Trained batch 710 batch loss 1.4695667 epoch total loss 1.62161148\n",
      "Trained batch 711 batch loss 1.45524168 epoch total loss 1.62137735\n",
      "Trained batch 712 batch loss 1.47728419 epoch total loss 1.62117505\n",
      "Trained batch 713 batch loss 1.46077943 epoch total loss 1.6209501\n",
      "Trained batch 714 batch loss 1.60608506 epoch total loss 1.62092924\n",
      "Trained batch 715 batch loss 1.27500486 epoch total loss 1.62044549\n",
      "Trained batch 716 batch loss 1.39422989 epoch total loss 1.62012959\n",
      "Trained batch 717 batch loss 1.52077401 epoch total loss 1.61999106\n",
      "Trained batch 718 batch loss 1.33778024 epoch total loss 1.61959791\n",
      "Trained batch 719 batch loss 1.31006765 epoch total loss 1.61916745\n",
      "Trained batch 720 batch loss 1.30305648 epoch total loss 1.61872852\n",
      "Trained batch 721 batch loss 1.3646934 epoch total loss 1.61837626\n",
      "Trained batch 722 batch loss 1.53879869 epoch total loss 1.61826599\n",
      "Trained batch 723 batch loss 1.50412047 epoch total loss 1.61810815\n",
      "Trained batch 724 batch loss 1.59970641 epoch total loss 1.61808276\n",
      "Trained batch 725 batch loss 1.69040346 epoch total loss 1.61818254\n",
      "Trained batch 726 batch loss 1.54508138 epoch total loss 1.61808181\n",
      "Trained batch 727 batch loss 1.55190051 epoch total loss 1.61799073\n",
      "Trained batch 728 batch loss 1.57933712 epoch total loss 1.61793768\n",
      "Trained batch 729 batch loss 1.56858134 epoch total loss 1.61787\n",
      "Trained batch 730 batch loss 1.60187912 epoch total loss 1.61784816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 731 batch loss 1.64277756 epoch total loss 1.61788237\n",
      "Trained batch 732 batch loss 1.63472915 epoch total loss 1.61790538\n",
      "Trained batch 733 batch loss 1.62537789 epoch total loss 1.61791563\n",
      "Trained batch 734 batch loss 1.60484147 epoch total loss 1.61789775\n",
      "Trained batch 735 batch loss 1.520751 epoch total loss 1.61776567\n",
      "Trained batch 736 batch loss 1.41140163 epoch total loss 1.61748517\n",
      "Trained batch 737 batch loss 1.32465744 epoch total loss 1.61708796\n",
      "Trained batch 738 batch loss 1.49927843 epoch total loss 1.61692834\n",
      "Trained batch 739 batch loss 1.56911778 epoch total loss 1.61686361\n",
      "Trained batch 740 batch loss 1.60534811 epoch total loss 1.61684799\n",
      "Trained batch 741 batch loss 1.59202254 epoch total loss 1.61681449\n",
      "Trained batch 742 batch loss 1.46356475 epoch total loss 1.61660802\n",
      "Trained batch 743 batch loss 1.30236816 epoch total loss 1.61618519\n",
      "Trained batch 744 batch loss 1.450454 epoch total loss 1.61596239\n",
      "Trained batch 745 batch loss 1.54449987 epoch total loss 1.61586654\n",
      "Trained batch 746 batch loss 1.54619086 epoch total loss 1.61577308\n",
      "Trained batch 747 batch loss 1.5758605 epoch total loss 1.61571956\n",
      "Trained batch 748 batch loss 1.65038252 epoch total loss 1.61576593\n",
      "Trained batch 749 batch loss 1.63657546 epoch total loss 1.6157937\n",
      "Trained batch 750 batch loss 1.63356173 epoch total loss 1.61581743\n",
      "Trained batch 751 batch loss 1.54879057 epoch total loss 1.61572814\n",
      "Trained batch 752 batch loss 1.50460315 epoch total loss 1.61558044\n",
      "Trained batch 753 batch loss 1.59221327 epoch total loss 1.61554933\n",
      "Trained batch 754 batch loss 1.56462264 epoch total loss 1.61548173\n",
      "Trained batch 755 batch loss 1.53304482 epoch total loss 1.61537266\n",
      "Trained batch 756 batch loss 1.56950521 epoch total loss 1.61531186\n",
      "Trained batch 757 batch loss 1.59925866 epoch total loss 1.61529064\n",
      "Trained batch 758 batch loss 1.54994583 epoch total loss 1.61520445\n",
      "Trained batch 759 batch loss 1.53474629 epoch total loss 1.61509848\n",
      "Trained batch 760 batch loss 1.51828635 epoch total loss 1.61497116\n",
      "Trained batch 761 batch loss 1.49662495 epoch total loss 1.61481559\n",
      "Trained batch 762 batch loss 1.49554849 epoch total loss 1.61465907\n",
      "Trained batch 763 batch loss 1.49484909 epoch total loss 1.61450207\n",
      "Trained batch 764 batch loss 1.53823757 epoch total loss 1.61440229\n",
      "Trained batch 765 batch loss 1.46053529 epoch total loss 1.61420119\n",
      "Trained batch 766 batch loss 1.43145764 epoch total loss 1.61396265\n",
      "Trained batch 767 batch loss 1.49080646 epoch total loss 1.61380219\n",
      "Trained batch 768 batch loss 1.44873595 epoch total loss 1.61358726\n",
      "Trained batch 769 batch loss 1.43631756 epoch total loss 1.61335671\n",
      "Trained batch 770 batch loss 1.49777937 epoch total loss 1.61320662\n",
      "Trained batch 771 batch loss 1.44552374 epoch total loss 1.61298919\n",
      "Trained batch 772 batch loss 1.42572451 epoch total loss 1.6127466\n",
      "Trained batch 773 batch loss 1.53197682 epoch total loss 1.61264217\n",
      "Trained batch 774 batch loss 1.43809664 epoch total loss 1.61241663\n",
      "Trained batch 775 batch loss 1.50718343 epoch total loss 1.61228085\n",
      "Trained batch 776 batch loss 1.59601927 epoch total loss 1.61226\n",
      "Trained batch 777 batch loss 1.57553637 epoch total loss 1.61221278\n",
      "Trained batch 778 batch loss 1.70731068 epoch total loss 1.61233497\n",
      "Trained batch 779 batch loss 1.53019261 epoch total loss 1.61222947\n",
      "Trained batch 780 batch loss 1.51972079 epoch total loss 1.61211097\n",
      "Trained batch 781 batch loss 1.5704329 epoch total loss 1.61205757\n",
      "Trained batch 782 batch loss 1.51553309 epoch total loss 1.61193407\n",
      "Trained batch 783 batch loss 1.47735715 epoch total loss 1.61176229\n",
      "Trained batch 784 batch loss 1.32587993 epoch total loss 1.61139774\n",
      "Trained batch 785 batch loss 1.45710325 epoch total loss 1.61120129\n",
      "Trained batch 786 batch loss 1.51215935 epoch total loss 1.61107528\n",
      "Trained batch 787 batch loss 1.64677167 epoch total loss 1.61112058\n",
      "Trained batch 788 batch loss 1.53840709 epoch total loss 1.61102831\n",
      "Trained batch 789 batch loss 1.40586197 epoch total loss 1.61076832\n",
      "Trained batch 790 batch loss 1.46848154 epoch total loss 1.61058831\n",
      "Trained batch 791 batch loss 1.52774262 epoch total loss 1.61048353\n",
      "Trained batch 792 batch loss 1.47733855 epoch total loss 1.61031532\n",
      "Trained batch 793 batch loss 1.47433424 epoch total loss 1.6101439\n",
      "Trained batch 794 batch loss 1.44117236 epoch total loss 1.60993111\n",
      "Trained batch 795 batch loss 1.40032339 epoch total loss 1.6096673\n",
      "Trained batch 796 batch loss 1.5273453 epoch total loss 1.60956395\n",
      "Trained batch 797 batch loss 1.50683856 epoch total loss 1.60943508\n",
      "Trained batch 798 batch loss 1.54247713 epoch total loss 1.60935116\n",
      "Trained batch 799 batch loss 1.47202289 epoch total loss 1.60917926\n",
      "Trained batch 800 batch loss 1.53175676 epoch total loss 1.60908246\n",
      "Trained batch 801 batch loss 1.49868262 epoch total loss 1.60894465\n",
      "Trained batch 802 batch loss 1.56588054 epoch total loss 1.60889101\n",
      "Trained batch 803 batch loss 1.5288837 epoch total loss 1.60879135\n",
      "Trained batch 804 batch loss 1.52694941 epoch total loss 1.60868967\n",
      "Trained batch 805 batch loss 1.43786538 epoch total loss 1.60847747\n",
      "Trained batch 806 batch loss 1.41866982 epoch total loss 1.60824203\n",
      "Trained batch 807 batch loss 1.49904943 epoch total loss 1.60810661\n",
      "Trained batch 808 batch loss 1.47718799 epoch total loss 1.60794461\n",
      "Trained batch 809 batch loss 1.52145612 epoch total loss 1.60783768\n",
      "Trained batch 810 batch loss 1.44692922 epoch total loss 1.60763907\n",
      "Trained batch 811 batch loss 1.52463269 epoch total loss 1.60753667\n",
      "Trained batch 812 batch loss 1.4090848 epoch total loss 1.60729229\n",
      "Trained batch 813 batch loss 1.53604102 epoch total loss 1.60720456\n",
      "Trained batch 814 batch loss 1.59404635 epoch total loss 1.60718834\n",
      "Trained batch 815 batch loss 1.53254545 epoch total loss 1.60709691\n",
      "Trained batch 816 batch loss 1.60832381 epoch total loss 1.60709834\n",
      "Trained batch 817 batch loss 1.63759863 epoch total loss 1.60713565\n",
      "Trained batch 818 batch loss 1.57125401 epoch total loss 1.60709178\n",
      "Trained batch 819 batch loss 1.44415224 epoch total loss 1.6068927\n",
      "Trained batch 820 batch loss 1.37549818 epoch total loss 1.60661054\n",
      "Trained batch 821 batch loss 1.5537647 epoch total loss 1.60654616\n",
      "Trained batch 822 batch loss 1.53431511 epoch total loss 1.60645819\n",
      "Trained batch 823 batch loss 1.60317516 epoch total loss 1.60645425\n",
      "Trained batch 824 batch loss 1.59713137 epoch total loss 1.60644293\n",
      "Trained batch 825 batch loss 1.56891584 epoch total loss 1.60639751\n",
      "Trained batch 826 batch loss 1.52533388 epoch total loss 1.6062994\n",
      "Trained batch 827 batch loss 1.43646336 epoch total loss 1.60609412\n",
      "Trained batch 828 batch loss 1.38290858 epoch total loss 1.60582459\n",
      "Trained batch 829 batch loss 1.45784688 epoch total loss 1.60564613\n",
      "Trained batch 830 batch loss 1.52135992 epoch total loss 1.60554469\n",
      "Trained batch 831 batch loss 1.53820586 epoch total loss 1.60546362\n",
      "Trained batch 832 batch loss 1.52393162 epoch total loss 1.60536563\n",
      "Trained batch 833 batch loss 1.62378 epoch total loss 1.60538769\n",
      "Trained batch 834 batch loss 1.45588148 epoch total loss 1.60520852\n",
      "Trained batch 835 batch loss 1.62451756 epoch total loss 1.60523164\n",
      "Trained batch 836 batch loss 1.57318425 epoch total loss 1.60519338\n",
      "Trained batch 837 batch loss 1.57446027 epoch total loss 1.60515666\n",
      "Trained batch 838 batch loss 1.50743759 epoch total loss 1.60504007\n",
      "Trained batch 839 batch loss 1.52734339 epoch total loss 1.60494745\n",
      "Trained batch 840 batch loss 1.62260818 epoch total loss 1.60496843\n",
      "Trained batch 841 batch loss 1.50607061 epoch total loss 1.60485089\n",
      "Trained batch 842 batch loss 1.52244937 epoch total loss 1.60475302\n",
      "Trained batch 843 batch loss 1.45234978 epoch total loss 1.6045723\n",
      "Trained batch 844 batch loss 1.51331711 epoch total loss 1.60446417\n",
      "Trained batch 845 batch loss 1.45222104 epoch total loss 1.60428405\n",
      "Trained batch 846 batch loss 1.49410343 epoch total loss 1.60415387\n"
     ]
    }
   ],
   "source": [
    "tfrecords_dir = os.getenv('HOME')+'/aiffel/mpii/tfrecords_mpii/'\n",
    "train_tfrecords = os.path.join(tfrecords_dir, 'train*')\n",
    "val_tfrecords = os.path.join(tfrecords_dir, 'val*')\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "tensorboard_dir = './logs/'\n",
    "learning_rate = 0.0007\n",
    "start_epoch = 1\n",
    "\n",
    "automatic_gpu_usage()\n",
    "\n",
    "pretrained_path = None # './models_old/model-v0.0.2-epoch-15-loss-1.1013.h5'\n",
    "\n",
    "history = train(epochs, start_epoch, learning_rate, tensorboard_dir, pretrained_path,\n",
    "      num_heatmap, batch_size, train_tfrecords, val_tfrecords, '0.0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplebaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')\n",
    "\n",
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # [[YOUR CODE]]\n",
    "\n",
    "    return seq_model\n",
    "\n",
    "upconv = _make_deconv_layer(3)\n",
    "\n",
    "final_layer = # [[YOUR CODE]]\n",
    "\n",
    "\n",
    "def Simplebaseline(input_shape=(256, 256, 3)):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # [[YOUR CODE]]\n",
    "\n",
    "    model = tf.keras.Model(inputs, out, name='simple_baseline')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Simplebaseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, start_epoch, learning_rate, tensorboard_dir, checkpoint,\n",
    "          num_heatmap, batch_size, train_tfrecords, val_tfrecords, version):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(os.path.join('./models')):\n",
    "        os.makedirs(os.path.join('./models/'))\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = Simplebaseline(IMAGE_SHAPE)\n",
    "        if checkpoint and os.path.exists(checkpoint):\n",
    "            model.load_weights(checkpoint)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate,\n",
    "            start_epoch=start_epoch,\n",
    "            version=version,\n",
    "            tensorboard_dir=tensorboard_dir)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords_dir = os.getenv('HOME')+'/aiffel/mpii/tfrecords_mpii/'\n",
    "train_tfrecords = os.path.join(tfrecords_dir, 'train*')\n",
    "val_tfrecords = os.path.join(tfrecords_dir, 'val*')\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "tensorboard_dir = './logs/'\n",
    "learning_rate = 0.0007\n",
    "start_epoch = 1\n",
    "\n",
    "automatic_gpu_usage()\n",
    "\n",
    "pretrained_path = None # './models_old/model-v0.0.2-epoch-15-loss-1.1013.h5'\n",
    "\n",
    "history2 = train(epochs, start_epoch, learning_rate, tensorboard_dir, pretrained_path,\n",
    "      num_heatmap, batch_size, train_tfrecords, val_tfrecords, '0.0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
