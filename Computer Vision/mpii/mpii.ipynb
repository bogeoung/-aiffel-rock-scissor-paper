{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 행동 스티커 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1' #CPU 사용\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "workdir = os.path.join(os.getenv('HOME'),'aiffel/mpii')\n",
    "os.chdir(workdir)\n",
    "\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "import ray\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### json 파싱\n",
    "앞서 다운 받은 `train.json`과 `validation.json`은 이미지에 담겨 있는 사람들의 pose keypoint 정보들을 가지고 있음. 이는 Pose Estimation을 위한 label로 삼을 수 있음.  \n",
    "이 json파일들이 어떻게 구성되어 있는지 확인하기 위해 샘플로 annotation정보를 1개만 출력함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "json_file_path = os.getenv('HOME')+'/aiffel/mpii/mpii_human_pose_v1_u12_2/train.json'\n",
    "\n",
    "with open(json_file_path) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2) # json beautify\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`joints`** \n",
    "\n",
    "`joints`가 label로 사용할 keypoint의 label임.  \n",
    "이미지 형상과 사람의 포즈에 따라 모든 label이 이미지에 나타나지 않기 떄문에 `joints_vis`를 이용해서 실제로 사용할 수 있는 keypoint인지를 나타냄.  \n",
    "`joints`의 순서는 다음과 같음.  \n",
    "0 - 오른쪽 발목  \n",
    "1 - 오른쪽 무릎  \n",
    "2 - 오른쪽 엉덩이  \n",
    "3 - 왼쪽 엉덩이  \n",
    "4 - 왼쪽 무릎  \n",
    "5 - 왼쪽 발목  \n",
    "6 - 골반  \n",
    "7 - 가슴(흉부)  \n",
    "8 - 목  \n",
    "9 - 머리 위  \n",
    "10 - 오른쪽 손목  \n",
    "11 - 오른쪽 팔꿈치  \n",
    "12 - 오른쪽 어깨  \n",
    "13 - 왼쪽 어깨  \n",
    "14 - 왼쪽 팔꿈치  \n",
    "15 - 왼쪽 손목  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`scale`**  \n",
    "높이 = scale * 200px  \n",
    "scale정보가 coco dataset에는 scale 값 또한 2차원으로 주어져서 bbox scale이 나오지만 mpii는 높이만 나옴   \n",
    "\n",
    "\n",
    "**`center`**  \n",
    "사람의 중심점을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json annotation을 파싱하는 함수 \n",
    "# image의 전체 path를 묶어 dict 타입의 label로 만듬. -> 이 label을 통해 학습 수행\n",
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfrecord 파일 만들기\n",
    "일반적으로 학습 과정에서 gpu의 연산 속도보다 HDD I/O의 속도가 느리기 때문에 병목 현상이 발생하고 효율성이 떨어지는 것을 관찰할 수 있음.  \n",
    "따라서 \"학습 데이터를 어떻게 빠르게 읽는가\"에 대한 고민이 생김.  \n",
    "\n",
    "학습 속도를 향상시키기 위해서 data read(또는 prefetch) 또는 데이터 변환 단계에서 gpu학습과 병렬적으로 수행되도록 prefetch를 적용해야함.  \n",
    "수행방법은 tf.data의 map함수를 이요하고 cache에 저장해두는 방법을 사용함.  \n",
    "\n",
    "tf는 데이터셋을 tfrecord 형태로 표현함으로써 위 변환을 자동화 해줌.  \n",
    "`tfrecord`는 binary record sequence를 저장하기 위한 형식으로, 내부적으로는 protocol buffer를 이용함.  \n",
    "\n",
    "protobuf 는 크로스플랫폼에서 사용할 수 있는 직렬화 데이터 라이브러리라고 생각하면 됨.  데이터셋 크기가 크기 때문에 빠른 학습을 위해서 이 정보를 tfrecord 파일로 변환함.  \n",
    "\n",
    "- annotation 을 total_shards 개수로 나눔(chunkify) (train : 64개, val : 8개)\n",
    "- build_single_tfrecord 함수를 통해 tfrecord 로 저장\n",
    "- 각 chunk 끼리 dependency 가 없기 때문에 병렬처리가 가능, ray를 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        # train_0001_of_0064.tfrecords\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, './tfrecords_mpii/{}_{}_of_{}.tfrecords'.format(\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**annotation을 적절한 개수로 나누는 함수 `chunkify`** \n",
    "- l 은 annotation, n은 shard 개수\n",
    "- shard 개수 단위로 annotation list 를 나누어서 새로운 list를 만듭니다.\n",
    "- numpy array 라고 가정하면 (size, shard, anno_content) 정도의 shape을 가짐. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tfrecord 1개를 저장하는 함수 `build_single_tfrecord`**\n",
    "- TFRecordWriter 를 이용해서 anno_list 를 shard 개수 단위로 작성함.\n",
    "- generate_tfexample 함수를 사용함.\n",
    "- [중요] write 할 때 string 으로 serialize 해야함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.example만드는 `generate_tfexample`**\n",
    "- 우리가 정의한 json 의 python type의 값들을 tfexample 에 사용할 수 있는 값으로 변환함.\n",
    "- image 파일은 byte 로 변환합니다. bitmap 으로 저장하게되면 파일용량이 상당히 커지기 때문에 만약 jpeg 타입이 아닌 경우 jpeg 으로 변환 후 content 로 불러서 저장함. (H,W,C)\n",
    "- 각 label 값을 tf.train.Feature 로 저장합니다. 이 때 데이터 타입에 주의해야 함.\n",
    "- 이미지는 byte 인코딩 된 값을 그대로 넣음.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray\n",
    "Ray는 파이썬을 위한 간단한 분산 어플리케이션 api임.  \n",
    "참고자료 : [https://docs.ray.io/en/latest/](https://docs.ray.io/en/latest/)  \n",
    "\n",
    "위 내용들을 모두 하나의 파일로 정리하면 다음과 같음.  \n",
    "\n",
    "**tfrecords_mpii.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-22 14:32:37,474\tINFO services.py:1269 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': './images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': './images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n",
      "\u001b[2m\u001b[36m(pid=9528)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9529)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9526)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9531)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9525)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9536)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9534)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9532)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9535)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9530)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9533)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9527)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9526)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9526)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9529)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9529)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9525)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9525)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9536)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9534)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9534)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9536)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9530)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9530)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9532)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9532)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9535)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9535)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9528)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9528)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9533)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9533)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9531)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9531)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9527)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9527)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9526)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9526)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9529)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9529)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9525)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9525)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9534)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9534)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9533)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9533)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9536)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9536)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9527)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9527)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9530)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9530)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9531)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9531)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9535)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9535)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9532)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9532)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9528)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9528)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9529)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9526)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9526)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9529)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9534)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9534)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9525)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9525)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9533)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9533)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9536)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9536)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0042_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9530)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9530)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9527)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9527)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9531)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9531)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9535)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9535)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9532)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9532)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9528)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9528)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9526)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9526)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9525)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9525)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9529)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9529)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9534)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9534)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9533)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9533)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9536)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9536)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9532)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9532)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9530)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9530)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9531)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9531)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9527)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9527)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9535)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9535)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9525)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9525)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9526)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9526)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9528)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9528)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9529)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9529)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9534)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9534)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9533)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9536)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9532)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9530)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9529)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9525)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9527)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9526)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9531)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9528)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9535)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9528)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9529)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9526)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9531)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9525)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9534)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9534)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9535)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9527)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9534)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9525)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9535)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9531)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9528)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9526)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=9527)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "Successfully wrote 25204 annotations to TF Records.\n",
      "\u001b[2m\u001b[36m(pid=9529)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0008_of_0008.tfrecords\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "import ray\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "ray.init()\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n - 1):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    results.append(l[start:])\n",
    "    return results\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy(\n",
    "        )  # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def generate_tfexample(anno):\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    # x = [\n",
    "    #     joint[0] / width if joint[0] >= 0 else joint[0]\n",
    "    #     for joint in anno['joints']\n",
    "    # ]\n",
    "    # y = [\n",
    "    #     joint[1] / height if joint[1] >= 0 else joint[0]\n",
    "    #     for joint in anno['joints']\n",
    "    # ]\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    # 0 - invisible, 1 - occluded, 2 - visible\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        # 'image/object/parts/x':\n",
    "        # tf.train.Feature(float_list=tf.train.FloatList(value=x)),\n",
    "        # 'image/object/parts/y':\n",
    "        # tf.train.Feature(float_list=tf.train.FloatList(value=y)),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno_list in chunk:\n",
    "            tf_example = generate_tfexample(anno_list)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)\n",
    "\n",
    "\n",
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        # train_0001_of_0064.tfrecords\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, './tfrecords_mpii/{}_{}_of_{}.tfrecords'.format(\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)\n",
    "\n",
    "\n",
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation\n",
    "\n",
    "\n",
    "def main():\n",
    "    print('Start to parse annotations.')\n",
    "    if not os.path.exists('./tfrecords_mpii'):\n",
    "        os.makedirs('./tfrecords_mpii')\n",
    "\n",
    "    with open(workdir + '/mpii_human_pose_v1_u12_2/train.json') as train_json:\n",
    "        train_annos = json.load(train_json)\n",
    "        train_annotations = [\n",
    "            parse_one_annotation(anno, './images/')\n",
    "            for anno in train_annos\n",
    "        ]\n",
    "        print('First train annotation: ', train_annotations[0])\n",
    "        del (train_annos)\n",
    "\n",
    "    with open(workdir + '/mpii_human_pose_v1_u12_2/validation.json') as val_json:\n",
    "        val_annos = json.load(val_json)\n",
    "        val_annotations = [\n",
    "            parse_one_annotation(anno, './images/') for anno in val_annos\n",
    "        ]\n",
    "        print('First val annotation: ', val_annotations[0])\n",
    "        del (val_annos)\n",
    "\n",
    "    print('Start to build TF Records.')\n",
    "    build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "    build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "    print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "        len(train_annotations) + len(val_annotations)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17      17     391\r\n"
     ]
    }
   ],
   "source": [
    "# 약 200mb 정도의 tfrecords들이 72개 만들어진 것을 확인할 수 있음. \n",
    "%ls | wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data label로 만들기\n",
    "tfrecords 파일을 읽고 전처리를 할 수 있는 dataloader를 만듬.  \n",
    "\n",
    "**`Preprocessor` class**  \n",
    "####  `__call__()` 메소드  \n",
    "`Preprocessor` 클래스 코드의 `__call__()` 메소드 내부에서 진행되는 주요 과정은 다음과 같음.  \n",
    "- tfrecord 파일이기 때문에 병렬로 읽는 것은 tf 가 지원해주고 있음. `self.parse_tfexample()` 에 구현되어 있고 이 함수를 통해 tf.tensor 로 이루어진 dictionary 형태의 features를 얻을 수 있음.  \n",
    "- 즉 image 는 `features['image/encoded']` 형태로 사용할 수 있고 tfrecord 를 저장할 때 jpeg encoding 된 값을 넣었으므로 `tf.io.decode_jpeg()`로 decoding 하여 tensor 형태의 이미지를 얻음.  \n",
    "- `crop_roi()` 메소드를 이용해 해당 이미지를 학습하기 편하도록 몇가지 트릭을 적용함. \n",
    "- `make_heatmaps()` 메소드를 이용해 label을 heatmap 으로 나타냄.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  `parse_tfexample` 메소드  \n",
    "\n",
    "- tfrecord 파일 형식을 우리가 저장한 data type feature 에 맞게 parsing 함.   \n",
    "- tf 가 자동으로 parsing 해주는 점은 아주 편하지만 feature description 을 정확하게 알고 있어야하는 단점이 있음.   \n",
    "- 즉, tfrecord 에서 사용할 key 값들과 data type 을 모르면 tfrecord 파일을 사용하기 굉장히 어려움. (serialize 되어있으므로..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  `crop_roi` 메소드  \n",
    "\n",
    "- 얻은 image 와 label 을 이용해서 적절한 학습형태로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `make_heatmaps` 메소드  \n",
    "\n",
    "- 우리가 알고 있는 것은 joints의 위치, center의 좌표, body height값임. 균일하게 학습하기 위해 body width도 적절히 정하는 것도 중요함.  \n",
    "\n",
    "- 높이 정보와 keypoint 위치를 이용해서 정사각형 박스를 사용하는 것을 기본으로 디자인 함. 이와 관련해서는 여러 방법이 있을 수 있지만, 우리가 임의로 조정한 crop box가 이미지 바깥으로 나가지 않는지 예외처리 하는 것을 더 중요하게 봄.  \n",
    "\n",
    "- (x,y)좌표로 되어있는 keypoint를 heatmap으로 변경시킴. \n",
    "- 16개의 점을 generate_2d_gaussian() 함수를 이용해서 64x64 의 map 에 표현함. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  `generate_2d_guassian` 메소드   \n",
    "\n",
    "- sigma 값이 1 이고 window size 7 인 필터를 이용해서 만듬.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 내용들을 하나의 py 파일로 정리하면 다음과 같음.  \n",
    "\n",
    "**preprocess.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y)\n",
    "\n",
    "        # print (image.shape, heatmaps.shape, type(heatmaps))\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "\n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "\n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "\n",
    "\n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \"\"\"\n",
    "        \"The same technique as Tompson et al. is used for supervision. A MeanSquared Error (MSE) loss is\n",
    "        applied comparing the predicted heatmap to a ground-truth heatmap consisting of a 2D gaussian\n",
    "        (with standard deviation of 1 px) centered on the keypoint location.\"\n",
    "\n",
    "        https://github.com/princeton-vl/pose-hg-train/blob/master/src/util/img.lua#L204\n",
    "        \"\"\"\n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        # this gaussian patch is 7x7, let's get four corners of it first\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "        # if the patch is out of image boundary we simply return nothing according to the source code\n",
    "        # [1]\"In these cases the joint is either truncated or severely occluded, so for\n",
    "        # supervision a ground truth heatmap of all zeros is provided.\"\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        # the center of the gaussian patch should be 1\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        # generate this 7x7 gaussian patch\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        # part of the patch could be out of the boundary, so we need to determine the valid range\n",
    "        # if xmin = -2, it means the 2 left-most columns are invalid, which is max(0, -(-2)) = 2\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        # if xmin = 59, xmax = 66, but our output is 64x64, then we should discard 2 right-most columns\n",
    "        # which is min(64, 66) - 59 = 5, and column 6 and 7 are discarded\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        # also, we need to determine where to put this patch in the whole heatmap\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        # finally, insert this patch into the heatmap\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "\n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * self.heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * self.heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "        num_heatmap = self.heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(self.heatmap_shape[1], self.heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "\n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example_proto):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example_proto,\n",
    "                                          image_feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hourglass 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Residual block module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,  # lift channels first\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hourglass module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \"\"\"\n",
    "    https://github.com/princeton-vl/pose-hg-train/blob/master/src/models/hg.lua#L3\n",
    "    \"\"\"\n",
    "    # Upper branch\n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    # Lower branch\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### intermediate output을 위한 linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked Hourglass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), num_stack=4, num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \"\"\"\n",
    "    https://github.com/princeton-vl/pose-hg-train/blob/master/src/models/hg.lua#L33\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # initial processing of the image\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        # predict 256 channels like a fully connected layer.\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        # predict final channels, which is also the number of predicted heatmap\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        # if it's not the last stack, we need to add predictions back\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackedHourglassNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 엔진 만들기\n",
    "학습 코드 `train.py`를 구현.  \n",
    "지금까지 제작한 `*.py` 모듈들은 여기서 참조(import)되어 사용될 것임. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from datetime import datetime\n",
    "\n",
    "import click\n",
    "import tensorflow as tf\n",
    "\n",
    "from hourglass104 import StackedHourglassNetwork\n",
    "from preprocess import Preprocessor\n",
    "\n",
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `automatic_gpu_usage` 메소드    \n",
    "- gpu memory growth 옵션을 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from datetime import datetime\n",
    "\n",
    "import click\n",
    "import tensorflow as tf\n",
    "\n",
    "from hourglass104 import StackedHourglassNetwork\n",
    "from preprocess import Preprocessor\n",
    "\n",
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def automatic_gpu_usage() :\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer class\n",
    "- loss : MSE (heatmap 을 pixel 단위 MSE 로 계산) → 실제 계산은 약간 달라, compute_loss() 에서 새로 구현함.\n",
    "- strategy : 분산학습용 tf.strategy 임. 사용 가능한 GPU가 1개뿐이라면 사용하지 않음.\n",
    "- optimizer : Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `lr_decay` 메소드  \n",
    "- learning rate : decay step에 따라 1/10씩 작아지도록 설정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `compute_loss` 메소드\n",
    "- loss function 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `train_step` , `val_step` 메소드\n",
    "이론대로라면 self.loss_object 를 사용해서 MSE 로 구현하는 것이 맞지만 사실 동일 weight MSE 는 수렴이 잘 되지 않음.   \n",
    "예측해야하는 positive (joint 들) 의 비율이 negative (배경이라고 할 수 있겠죠?) 에 비해 상당히 적은 비율로 등장하기 때문.  \n",
    "\n",
    "label이 배경이 아닌 경우(heatmap 값이 0보다 큰 경우)에 추가적인 weight를 줃도록 함.  \n",
    "weight가 82인 이유는 heatmap 전체 크기인 64x64에서 gaussian point 등장 비율이 7x7 패치이기 때문에 64/7 = 9.1 -> 9x9로 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate,\n",
    "                 version='0.0.1',\n",
    "                 start_epoch=1,\n",
    "                 tensorboard_dir='./logs'):\n",
    "        self.start_epoch = start_epoch\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        # \"we use rmsprop with a learning rate of 2.5e-4.\"\"\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.tensorboard_dir = tensorboard_dir\n",
    "        self.best_model = None\n",
    "        self.version = version\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.experimental_run_v2(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.experimental_run_v2(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        summary_writer = tf.summary.create_file_writer(self.tensorboard_dir)\n",
    "        summary_writer.set_as_default()\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.epochs + 1):\n",
    "            tf.summary.experimental.set_step(epoch)\n",
    "\n",
    "            self.lr_decay()\n",
    "            tf.summary.scalar('epoch learning rate',\n",
    "                              self.current_learning_rate)\n",
    "\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "            tf.summary.scalar('epoch train loss', train_loss)\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "            tf.summary.scalar('epoch val loss', val_loss)\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = './models/model-v{}-epoch-{}-loss-{:.4f}.h5'.format(\n",
    "            self.version, epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.dataset 만들기  \n",
    "##### `create_dataset` 메소드  \n",
    "tfrecord파일을 `tf.dataset`으로 만듬."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `train` 메소드  \n",
    "train함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def train(epochs, start_epoch, learning_rate, tensorboard_dir, checkpoint,\n",
    "          num_heatmap, batch_size, train_tfrecords, val_tfrecords, version):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(os.path.join('./models')):\n",
    "        os.makedirs(os.path.join('./models/'))\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "        if checkpoint and os.path.exists(checkpoint):\n",
    "            model.load_weights(checkpoint)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate,\n",
    "            start_epoch=start_epoch,\n",
    "            version=version,\n",
    "            tensorboard_dir=tensorboard_dir)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 5.03168106 epoch total loss 5.03168106\n",
      "Trained batch 2 batch loss 5.12402964 epoch total loss 5.07785511\n",
      "Trained batch 3 batch loss 4.79028845 epoch total loss 4.982\n",
      "Trained batch 4 batch loss 4.42440367 epoch total loss 4.84260082\n",
      "Trained batch 5 batch loss 4.64506721 epoch total loss 4.80309391\n",
      "Trained batch 6 batch loss 4.93397522 epoch total loss 4.82490778\n",
      "Trained batch 7 batch loss 3.7924664 epoch total loss 4.67741632\n",
      "Trained batch 8 batch loss 3.77604985 epoch total loss 4.56474543\n",
      "Trained batch 9 batch loss 3.87475538 epoch total loss 4.48808\n",
      "Trained batch 10 batch loss 4.51862478 epoch total loss 4.49113417\n",
      "Trained batch 11 batch loss 4.32199812 epoch total loss 4.47575808\n",
      "Trained batch 12 batch loss 4.27406168 epoch total loss 4.45895052\n",
      "Trained batch 13 batch loss 4.27509403 epoch total loss 4.44480753\n",
      "Trained batch 14 batch loss 3.72554255 epoch total loss 4.39343166\n",
      "Trained batch 15 batch loss 3.75184441 epoch total loss 4.35065937\n",
      "Trained batch 16 batch loss 3.5281775 epoch total loss 4.29925394\n",
      "Trained batch 17 batch loss 3.5057621 epoch total loss 4.25257778\n",
      "Trained batch 18 batch loss 3.49721622 epoch total loss 4.21061325\n",
      "Trained batch 19 batch loss 3.21167064 epoch total loss 4.15803719\n",
      "Trained batch 20 batch loss 3.57381272 epoch total loss 4.12882614\n",
      "Trained batch 21 batch loss 3.73140335 epoch total loss 4.10990095\n",
      "Trained batch 22 batch loss 3.91981983 epoch total loss 4.10126114\n",
      "Trained batch 23 batch loss 3.52058291 epoch total loss 4.07601452\n",
      "Trained batch 24 batch loss 3.14212561 epoch total loss 4.03710222\n",
      "Trained batch 25 batch loss 3.40738487 epoch total loss 4.01191378\n",
      "Trained batch 26 batch loss 3.93331242 epoch total loss 4.00889063\n",
      "Trained batch 27 batch loss 3.99429798 epoch total loss 4.00835037\n",
      "Trained batch 28 batch loss 3.57464361 epoch total loss 3.99286079\n",
      "Trained batch 29 batch loss 3.73399401 epoch total loss 3.9839344\n",
      "Trained batch 30 batch loss 3.76243854 epoch total loss 3.97655106\n",
      "Trained batch 31 batch loss 3.74280787 epoch total loss 3.96901083\n",
      "Trained batch 32 batch loss 3.59952593 epoch total loss 3.95746446\n",
      "Trained batch 33 batch loss 3.50345612 epoch total loss 3.94370651\n",
      "Trained batch 34 batch loss 3.38108969 epoch total loss 3.92715907\n",
      "Trained batch 35 batch loss 3.33374691 epoch total loss 3.91020417\n",
      "Trained batch 36 batch loss 3.55622339 epoch total loss 3.90037155\n",
      "Trained batch 37 batch loss 3.91163254 epoch total loss 3.90067601\n",
      "Trained batch 38 batch loss 3.82897329 epoch total loss 3.89878917\n",
      "Trained batch 39 batch loss 3.94315815 epoch total loss 3.8999269\n",
      "Trained batch 40 batch loss 4.11053276 epoch total loss 3.90519214\n",
      "Trained batch 41 batch loss 3.97265172 epoch total loss 3.9068377\n",
      "Trained batch 42 batch loss 3.88835263 epoch total loss 3.90639758\n",
      "Trained batch 43 batch loss 3.77952361 epoch total loss 3.90344691\n",
      "Trained batch 44 batch loss 3.84190226 epoch total loss 3.90204835\n",
      "Trained batch 45 batch loss 3.65681934 epoch total loss 3.89659858\n",
      "Trained batch 46 batch loss 3.38293457 epoch total loss 3.885432\n",
      "Trained batch 47 batch loss 3.72945881 epoch total loss 3.88211346\n",
      "Trained batch 48 batch loss 3.66718626 epoch total loss 3.87763596\n",
      "Trained batch 49 batch loss 3.60362935 epoch total loss 3.87204385\n",
      "Trained batch 50 batch loss 3.73586822 epoch total loss 3.86932039\n",
      "Trained batch 51 batch loss 3.59252977 epoch total loss 3.86389303\n",
      "Trained batch 52 batch loss 3.42113161 epoch total loss 3.85537839\n",
      "Trained batch 53 batch loss 3.00406313 epoch total loss 3.83931565\n",
      "Trained batch 54 batch loss 3.4200778 epoch total loss 3.83155203\n",
      "Trained batch 55 batch loss 3.38787651 epoch total loss 3.82348514\n",
      "Trained batch 56 batch loss 3.85693884 epoch total loss 3.82408261\n",
      "Trained batch 57 batch loss 3.67740059 epoch total loss 3.82150912\n",
      "Trained batch 58 batch loss 3.4377234 epoch total loss 3.81489229\n",
      "Trained batch 59 batch loss 3.71479106 epoch total loss 3.81319571\n",
      "Trained batch 60 batch loss 3.8506434 epoch total loss 3.81382\n",
      "Trained batch 61 batch loss 3.81534147 epoch total loss 3.81384468\n",
      "Trained batch 62 batch loss 3.7022891 epoch total loss 3.81204534\n",
      "Trained batch 63 batch loss 3.55624866 epoch total loss 3.80798507\n",
      "Trained batch 64 batch loss 3.7258811 epoch total loss 3.80670214\n",
      "Trained batch 65 batch loss 3.60514307 epoch total loss 3.80360126\n",
      "Trained batch 66 batch loss 3.21577263 epoch total loss 3.7946949\n",
      "Trained batch 67 batch loss 3.53911638 epoch total loss 3.79088044\n",
      "Trained batch 68 batch loss 3.56628346 epoch total loss 3.78757739\n",
      "Trained batch 69 batch loss 3.42813206 epoch total loss 3.78236818\n",
      "Trained batch 70 batch loss 3.18996358 epoch total loss 3.77390528\n",
      "Trained batch 71 batch loss 3.26656199 epoch total loss 3.76675963\n",
      "Trained batch 72 batch loss 3.59757352 epoch total loss 3.76440978\n",
      "Trained batch 73 batch loss 3.29778838 epoch total loss 3.75801778\n",
      "Trained batch 74 batch loss 3.57681274 epoch total loss 3.75556898\n",
      "Trained batch 75 batch loss 3.68576574 epoch total loss 3.7546382\n",
      "Trained batch 76 batch loss 3.62996197 epoch total loss 3.75299788\n",
      "Trained batch 77 batch loss 3.63358951 epoch total loss 3.75144696\n",
      "Trained batch 78 batch loss 3.67849326 epoch total loss 3.75051165\n",
      "Trained batch 79 batch loss 3.78036928 epoch total loss 3.75088954\n",
      "Trained batch 80 batch loss 3.72995543 epoch total loss 3.75062799\n",
      "Trained batch 81 batch loss 3.69258881 epoch total loss 3.74991155\n",
      "Trained batch 82 batch loss 3.79688 epoch total loss 3.75048423\n",
      "Trained batch 83 batch loss 3.63090563 epoch total loss 3.7490437\n",
      "Trained batch 84 batch loss 3.59940553 epoch total loss 3.74726224\n",
      "Trained batch 85 batch loss 3.5500226 epoch total loss 3.74494171\n",
      "Trained batch 86 batch loss 3.40063643 epoch total loss 3.74093795\n",
      "Trained batch 87 batch loss 3.16124105 epoch total loss 3.7342751\n",
      "Trained batch 88 batch loss 3.62892985 epoch total loss 3.733078\n",
      "Trained batch 89 batch loss 3.46896911 epoch total loss 3.73011041\n",
      "Trained batch 90 batch loss 3.41176772 epoch total loss 3.72657347\n",
      "Trained batch 91 batch loss 3.36091 epoch total loss 3.72255492\n",
      "Trained batch 92 batch loss 3.5794 epoch total loss 3.720999\n",
      "Trained batch 93 batch loss 3.31256199 epoch total loss 3.71660709\n",
      "Trained batch 94 batch loss 3.10591269 epoch total loss 3.71011066\n",
      "Trained batch 95 batch loss 3.55785155 epoch total loss 3.70850801\n",
      "Trained batch 96 batch loss 3.54244423 epoch total loss 3.70677829\n",
      "Trained batch 97 batch loss 3.64878583 epoch total loss 3.70618033\n",
      "Trained batch 98 batch loss 3.40983653 epoch total loss 3.70315647\n",
      "Trained batch 99 batch loss 3.61540556 epoch total loss 3.70227027\n",
      "Trained batch 100 batch loss 3.65122557 epoch total loss 3.70175958\n",
      "Trained batch 101 batch loss 3.4182694 epoch total loss 3.69895291\n",
      "Trained batch 102 batch loss 3.57006598 epoch total loss 3.69768929\n",
      "Trained batch 103 batch loss 3.43671679 epoch total loss 3.69515538\n",
      "Trained batch 104 batch loss 3.6633141 epoch total loss 3.69484925\n",
      "Trained batch 105 batch loss 3.71569276 epoch total loss 3.69504762\n",
      "Trained batch 106 batch loss 3.45832801 epoch total loss 3.69281435\n",
      "Trained batch 107 batch loss 3.12752724 epoch total loss 3.68753147\n",
      "Trained batch 108 batch loss 3.49524212 epoch total loss 3.68575096\n",
      "Trained batch 109 batch loss 3.50016093 epoch total loss 3.68404818\n",
      "Trained batch 110 batch loss 3.14606094 epoch total loss 3.67915726\n",
      "Trained batch 111 batch loss 3.16995668 epoch total loss 3.67456985\n",
      "Trained batch 112 batch loss 3.00734234 epoch total loss 3.66861272\n",
      "Trained batch 113 batch loss 2.98499513 epoch total loss 3.66256285\n",
      "Trained batch 114 batch loss 3.01861668 epoch total loss 3.65691423\n",
      "Trained batch 115 batch loss 2.87682271 epoch total loss 3.65013075\n",
      "Trained batch 116 batch loss 3.50225449 epoch total loss 3.64885616\n",
      "Trained batch 117 batch loss 3.49787259 epoch total loss 3.6475656\n",
      "Trained batch 118 batch loss 3.230865 epoch total loss 3.64403415\n",
      "Trained batch 119 batch loss 3.13451767 epoch total loss 3.63975263\n",
      "Trained batch 120 batch loss 3.47916508 epoch total loss 3.63841414\n",
      "Trained batch 121 batch loss 3.64199734 epoch total loss 3.63844395\n",
      "Trained batch 122 batch loss 3.2858212 epoch total loss 3.6355536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 123 batch loss 3.41429591 epoch total loss 3.63375473\n",
      "Trained batch 124 batch loss 3.27424121 epoch total loss 3.63085532\n",
      "Trained batch 125 batch loss 3.36970854 epoch total loss 3.6287663\n",
      "Trained batch 126 batch loss 2.85709548 epoch total loss 3.6226418\n",
      "Trained batch 127 batch loss 2.83217764 epoch total loss 3.61641788\n",
      "Trained batch 128 batch loss 3.54324126 epoch total loss 3.61584616\n",
      "Trained batch 129 batch loss 3.46509647 epoch total loss 3.61467743\n",
      "Trained batch 130 batch loss 3.59167337 epoch total loss 3.61450052\n",
      "Trained batch 131 batch loss 3.36966538 epoch total loss 3.61263156\n",
      "Trained batch 132 batch loss 3.28597641 epoch total loss 3.61015701\n",
      "Trained batch 133 batch loss 2.78738785 epoch total loss 3.60397053\n",
      "Trained batch 134 batch loss 3.28375196 epoch total loss 3.60158086\n",
      "Trained batch 135 batch loss 3.53306937 epoch total loss 3.6010735\n",
      "Trained batch 136 batch loss 3.49316669 epoch total loss 3.60028\n",
      "Trained batch 137 batch loss 3.25681162 epoch total loss 3.59777308\n",
      "Trained batch 138 batch loss 3.43654728 epoch total loss 3.59660482\n",
      "Trained batch 139 batch loss 3.41598725 epoch total loss 3.5953052\n",
      "Trained batch 140 batch loss 3.70744 epoch total loss 3.59610629\n",
      "Trained batch 141 batch loss 3.67648506 epoch total loss 3.59667611\n",
      "Trained batch 142 batch loss 3.52055287 epoch total loss 3.59614015\n",
      "Trained batch 143 batch loss 3.66265869 epoch total loss 3.59660554\n",
      "Trained batch 144 batch loss 3.27155781 epoch total loss 3.59434795\n",
      "Trained batch 145 batch loss 3.03747272 epoch total loss 3.59050751\n",
      "Trained batch 146 batch loss 3.20334935 epoch total loss 3.58785582\n",
      "Trained batch 147 batch loss 3.20869923 epoch total loss 3.5852766\n",
      "Trained batch 148 batch loss 3.27573156 epoch total loss 3.5831852\n",
      "Trained batch 149 batch loss 3.11141491 epoch total loss 3.58001876\n",
      "Trained batch 150 batch loss 3.17934275 epoch total loss 3.57734752\n",
      "Trained batch 151 batch loss 3.3779974 epoch total loss 3.57602715\n",
      "Trained batch 152 batch loss 3.56356096 epoch total loss 3.5759449\n",
      "Trained batch 153 batch loss 3.59116697 epoch total loss 3.57604456\n",
      "Trained batch 154 batch loss 3.61135554 epoch total loss 3.57627368\n",
      "Trained batch 155 batch loss 3.47186494 epoch total loss 3.57560015\n",
      "Trained batch 156 batch loss 3.53823137 epoch total loss 3.57536054\n",
      "Trained batch 157 batch loss 3.5365119 epoch total loss 3.57511282\n",
      "Trained batch 158 batch loss 3.25588179 epoch total loss 3.57309222\n",
      "Trained batch 159 batch loss 3.50243735 epoch total loss 3.57264805\n",
      "Trained batch 160 batch loss 3.56724954 epoch total loss 3.57261419\n",
      "Trained batch 161 batch loss 3.4263196 epoch total loss 3.57170558\n",
      "Trained batch 162 batch loss 3.52317595 epoch total loss 3.57140613\n",
      "Trained batch 163 batch loss 3.49125409 epoch total loss 3.57091451\n",
      "Trained batch 164 batch loss 3.50395584 epoch total loss 3.57050633\n",
      "Trained batch 165 batch loss 3.37723112 epoch total loss 3.56933522\n",
      "Trained batch 166 batch loss 3.46395206 epoch total loss 3.56870031\n",
      "Trained batch 167 batch loss 3.45302916 epoch total loss 3.56800747\n",
      "Trained batch 168 batch loss 3.46247816 epoch total loss 3.56737924\n",
      "Trained batch 169 batch loss 3.13721466 epoch total loss 3.56483388\n",
      "Trained batch 170 batch loss 3.33113718 epoch total loss 3.56345892\n",
      "Trained batch 171 batch loss 3.46316433 epoch total loss 3.56287217\n",
      "Trained batch 172 batch loss 3.34882021 epoch total loss 3.56162786\n",
      "Trained batch 173 batch loss 3.37920547 epoch total loss 3.56057334\n",
      "Trained batch 174 batch loss 3.18490362 epoch total loss 3.55841422\n",
      "Trained batch 175 batch loss 3.3736062 epoch total loss 3.55735803\n",
      "Trained batch 176 batch loss 3.26389027 epoch total loss 3.55569077\n",
      "Trained batch 177 batch loss 3.27362585 epoch total loss 3.55409718\n",
      "Trained batch 178 batch loss 3.33286929 epoch total loss 3.5528543\n",
      "Trained batch 179 batch loss 3.48710275 epoch total loss 3.55248713\n",
      "Trained batch 180 batch loss 3.48760295 epoch total loss 3.55212665\n",
      "Trained batch 181 batch loss 3.34043264 epoch total loss 3.5509572\n",
      "Trained batch 182 batch loss 3.15026379 epoch total loss 3.54875565\n",
      "Trained batch 183 batch loss 3.16324663 epoch total loss 3.54664922\n",
      "Trained batch 184 batch loss 2.98430634 epoch total loss 3.54359293\n",
      "Trained batch 185 batch loss 3.35327148 epoch total loss 3.54256415\n",
      "Trained batch 186 batch loss 3.32388353 epoch total loss 3.54138875\n",
      "Trained batch 187 batch loss 3.23150206 epoch total loss 3.5397315\n",
      "Trained batch 188 batch loss 3.22667933 epoch total loss 3.53806639\n",
      "Trained batch 189 batch loss 2.93505788 epoch total loss 3.53487587\n",
      "Trained batch 190 batch loss 3.14631319 epoch total loss 3.53283072\n",
      "Trained batch 191 batch loss 3.17934775 epoch total loss 3.53097987\n",
      "Trained batch 192 batch loss 3.37647438 epoch total loss 3.53017521\n",
      "Trained batch 193 batch loss 2.96976137 epoch total loss 3.52727175\n",
      "Trained batch 194 batch loss 3.08837 epoch total loss 3.52500939\n",
      "Trained batch 195 batch loss 3.04090691 epoch total loss 3.52252674\n",
      "Trained batch 196 batch loss 3.27934074 epoch total loss 3.52128601\n",
      "Trained batch 197 batch loss 3.37577915 epoch total loss 3.52054739\n",
      "Trained batch 198 batch loss 3.63272834 epoch total loss 3.52111411\n",
      "Trained batch 199 batch loss 3.35579729 epoch total loss 3.52028322\n",
      "Trained batch 200 batch loss 3.24770451 epoch total loss 3.51892018\n",
      "Trained batch 201 batch loss 3.35832357 epoch total loss 3.51812148\n",
      "Trained batch 202 batch loss 3.36918426 epoch total loss 3.51738405\n",
      "Trained batch 203 batch loss 3.40704918 epoch total loss 3.5168407\n",
      "Trained batch 204 batch loss 3.49644232 epoch total loss 3.5167408\n",
      "Trained batch 205 batch loss 2.99825191 epoch total loss 3.51421142\n",
      "Trained batch 206 batch loss 2.86434865 epoch total loss 3.51105666\n",
      "Trained batch 207 batch loss 3.51363325 epoch total loss 3.51106882\n",
      "Trained batch 208 batch loss 3.46138144 epoch total loss 3.51083\n",
      "Trained batch 209 batch loss 3.38201857 epoch total loss 3.51021361\n",
      "Trained batch 210 batch loss 3.31433058 epoch total loss 3.50928092\n",
      "Trained batch 211 batch loss 3.36619353 epoch total loss 3.50860286\n",
      "Trained batch 212 batch loss 3.01668096 epoch total loss 3.50628233\n",
      "Trained batch 213 batch loss 3.37942028 epoch total loss 3.50568652\n",
      "Trained batch 214 batch loss 3.36786652 epoch total loss 3.50504255\n",
      "Trained batch 215 batch loss 2.9522326 epoch total loss 3.50247121\n",
      "Trained batch 216 batch loss 3.3417511 epoch total loss 3.5017271\n",
      "Trained batch 217 batch loss 3.03615475 epoch total loss 3.49958158\n",
      "Trained batch 218 batch loss 3.16087675 epoch total loss 3.4980278\n",
      "Trained batch 219 batch loss 3.40025854 epoch total loss 3.49758148\n",
      "Trained batch 220 batch loss 3.28117943 epoch total loss 3.49659777\n",
      "Trained batch 221 batch loss 3.04346371 epoch total loss 3.49454737\n",
      "Trained batch 222 batch loss 2.89493322 epoch total loss 3.49184656\n",
      "Trained batch 223 batch loss 2.80539179 epoch total loss 3.48876834\n",
      "Trained batch 224 batch loss 2.83887744 epoch total loss 3.48586702\n",
      "Trained batch 225 batch loss 2.82290268 epoch total loss 3.48292041\n",
      "Trained batch 226 batch loss 2.82096052 epoch total loss 3.47999144\n",
      "Trained batch 227 batch loss 2.85427189 epoch total loss 3.47723508\n",
      "Trained batch 228 batch loss 3.21083307 epoch total loss 3.47606635\n",
      "Trained batch 229 batch loss 3.17429495 epoch total loss 3.47474885\n",
      "Trained batch 230 batch loss 3.36292553 epoch total loss 3.47426248\n",
      "Trained batch 231 batch loss 3.0898664 epoch total loss 3.47259831\n",
      "Trained batch 232 batch loss 3.50138688 epoch total loss 3.47272253\n",
      "Trained batch 233 batch loss 3.35061336 epoch total loss 3.47219825\n",
      "Trained batch 234 batch loss 3.76113081 epoch total loss 3.47343302\n",
      "Trained batch 235 batch loss 3.70448732 epoch total loss 3.47441626\n",
      "Trained batch 236 batch loss 3.29242158 epoch total loss 3.47364497\n",
      "Trained batch 237 batch loss 3.44626856 epoch total loss 3.47352958\n",
      "Trained batch 238 batch loss 3.67588377 epoch total loss 3.47437978\n",
      "Trained batch 239 batch loss 3.48435616 epoch total loss 3.47442174\n",
      "Trained batch 240 batch loss 3.55165863 epoch total loss 3.47474337\n",
      "Trained batch 241 batch loss 3.50540447 epoch total loss 3.47487068\n",
      "Trained batch 242 batch loss 3.26360512 epoch total loss 3.47399783\n",
      "Trained batch 243 batch loss 3.5512085 epoch total loss 3.4743154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 244 batch loss 3.34938884 epoch total loss 3.47380352\n",
      "Trained batch 245 batch loss 3.45953226 epoch total loss 3.47374511\n",
      "Trained batch 246 batch loss 3.50236368 epoch total loss 3.47386146\n",
      "Trained batch 247 batch loss 3.40180945 epoch total loss 3.47356987\n",
      "Trained batch 248 batch loss 3.39834046 epoch total loss 3.47326636\n",
      "Trained batch 249 batch loss 3.54538441 epoch total loss 3.47355604\n",
      "Trained batch 250 batch loss 3.60761452 epoch total loss 3.47409225\n",
      "Trained batch 251 batch loss 3.49608898 epoch total loss 3.47418\n",
      "Trained batch 252 batch loss 3.30500174 epoch total loss 3.4735086\n",
      "Trained batch 253 batch loss 3.26239276 epoch total loss 3.47267413\n",
      "Trained batch 254 batch loss 3.28704476 epoch total loss 3.47194338\n",
      "Trained batch 255 batch loss 3.29342628 epoch total loss 3.47124314\n",
      "Trained batch 256 batch loss 3.50219059 epoch total loss 3.47136402\n",
      "Trained batch 257 batch loss 3.00093627 epoch total loss 3.46953344\n",
      "Trained batch 258 batch loss 2.87089682 epoch total loss 3.46721315\n",
      "Trained batch 259 batch loss 2.89443517 epoch total loss 3.46500158\n",
      "Trained batch 260 batch loss 3.24227858 epoch total loss 3.46414495\n",
      "Trained batch 261 batch loss 3.38743019 epoch total loss 3.46385098\n",
      "Trained batch 262 batch loss 3.4171555 epoch total loss 3.46367288\n",
      "Trained batch 263 batch loss 3.43893 epoch total loss 3.4635787\n",
      "Trained batch 264 batch loss 3.45326614 epoch total loss 3.4635396\n",
      "Trained batch 265 batch loss 2.82938766 epoch total loss 3.46114659\n",
      "Trained batch 266 batch loss 2.77928686 epoch total loss 3.45858335\n",
      "Trained batch 267 batch loss 2.65279603 epoch total loss 3.45556521\n",
      "Trained batch 268 batch loss 2.69330168 epoch total loss 3.45272088\n",
      "Trained batch 269 batch loss 2.79007864 epoch total loss 3.45025778\n",
      "Trained batch 270 batch loss 3.33365417 epoch total loss 3.449826\n",
      "Trained batch 271 batch loss 3.22360611 epoch total loss 3.4489913\n",
      "Trained batch 272 batch loss 3.48428202 epoch total loss 3.449121\n",
      "Trained batch 273 batch loss 3.47518587 epoch total loss 3.44921637\n",
      "Trained batch 274 batch loss 3.19304323 epoch total loss 3.44828129\n",
      "Trained batch 275 batch loss 3.25710082 epoch total loss 3.44758606\n",
      "Trained batch 276 batch loss 3.24872541 epoch total loss 3.44686556\n",
      "Trained batch 277 batch loss 3.36075521 epoch total loss 3.4465549\n",
      "Trained batch 278 batch loss 3.2007165 epoch total loss 3.4456706\n",
      "Trained batch 279 batch loss 3.42300105 epoch total loss 3.4455893\n",
      "Trained batch 280 batch loss 3.45115 epoch total loss 3.44560909\n",
      "Trained batch 281 batch loss 3.44157672 epoch total loss 3.44559479\n",
      "Trained batch 282 batch loss 3.41746 epoch total loss 3.44549513\n",
      "Trained batch 283 batch loss 3.11755466 epoch total loss 3.44433641\n",
      "Trained batch 284 batch loss 3.08227015 epoch total loss 3.44306159\n",
      "Trained batch 285 batch loss 3.15256572 epoch total loss 3.44204235\n",
      "Trained batch 286 batch loss 3.0427537 epoch total loss 3.44064617\n",
      "Trained batch 287 batch loss 3.05875921 epoch total loss 3.43931556\n",
      "Trained batch 288 batch loss 3.21218491 epoch total loss 3.43852687\n",
      "Trained batch 289 batch loss 3.27406573 epoch total loss 3.43795776\n",
      "Trained batch 290 batch loss 3.37693524 epoch total loss 3.43774724\n",
      "Trained batch 291 batch loss 3.22659 epoch total loss 3.43702149\n",
      "Trained batch 292 batch loss 3.27637434 epoch total loss 3.43647146\n",
      "Trained batch 293 batch loss 3.19510937 epoch total loss 3.43564773\n",
      "Trained batch 294 batch loss 3.38683033 epoch total loss 3.43548179\n",
      "Trained batch 295 batch loss 3.48168135 epoch total loss 3.43563843\n",
      "Trained batch 296 batch loss 3.24073648 epoch total loss 3.43498\n",
      "Trained batch 297 batch loss 3.27257109 epoch total loss 3.43443298\n",
      "Trained batch 298 batch loss 3.24912381 epoch total loss 3.43381119\n",
      "Trained batch 299 batch loss 3.35624075 epoch total loss 3.43355179\n",
      "Trained batch 300 batch loss 3.38172579 epoch total loss 3.43337893\n",
      "Trained batch 301 batch loss 3.37228322 epoch total loss 3.43317604\n",
      "Trained batch 302 batch loss 3.33221531 epoch total loss 3.43284202\n",
      "Trained batch 303 batch loss 3.34096384 epoch total loss 3.43253851\n",
      "Trained batch 304 batch loss 3.44062185 epoch total loss 3.43256545\n",
      "Trained batch 305 batch loss 3.43615103 epoch total loss 3.43257713\n",
      "Trained batch 306 batch loss 3.4264853 epoch total loss 3.43255734\n",
      "Trained batch 307 batch loss 3.42528343 epoch total loss 3.43253374\n",
      "Trained batch 308 batch loss 3.49956417 epoch total loss 3.43275118\n",
      "Trained batch 309 batch loss 3.43034649 epoch total loss 3.43274331\n",
      "Trained batch 310 batch loss 3.35075855 epoch total loss 3.43247867\n",
      "Trained batch 311 batch loss 3.42634392 epoch total loss 3.43245912\n",
      "Trained batch 312 batch loss 3.24059701 epoch total loss 3.431844\n",
      "Trained batch 313 batch loss 3.22981739 epoch total loss 3.43119884\n",
      "Trained batch 314 batch loss 3.25159049 epoch total loss 3.43062663\n",
      "Trained batch 315 batch loss 3.24189949 epoch total loss 3.43002772\n",
      "Trained batch 316 batch loss 3.08386612 epoch total loss 3.42893219\n",
      "Trained batch 317 batch loss 3.31966591 epoch total loss 3.42858768\n",
      "Trained batch 318 batch loss 3.46917343 epoch total loss 3.42871523\n",
      "Trained batch 319 batch loss 3.41057181 epoch total loss 3.42865825\n",
      "Trained batch 320 batch loss 3.34734249 epoch total loss 3.42840385\n",
      "Trained batch 321 batch loss 3.40254879 epoch total loss 3.42832351\n",
      "Trained batch 322 batch loss 3.70565271 epoch total loss 3.42918491\n",
      "Trained batch 323 batch loss 3.53644896 epoch total loss 3.42951703\n",
      "Trained batch 324 batch loss 3.54985452 epoch total loss 3.42988825\n",
      "Trained batch 325 batch loss 3.41414022 epoch total loss 3.42984\n",
      "Trained batch 326 batch loss 3.42951584 epoch total loss 3.42983913\n",
      "Trained batch 327 batch loss 3.3871727 epoch total loss 3.42970872\n",
      "Trained batch 328 batch loss 3.32887459 epoch total loss 3.4294014\n",
      "Trained batch 329 batch loss 3.40722919 epoch total loss 3.42933393\n",
      "Trained batch 330 batch loss 3.41351271 epoch total loss 3.42928576\n",
      "Trained batch 331 batch loss 3.20124 epoch total loss 3.42859697\n",
      "Trained batch 332 batch loss 3.31049848 epoch total loss 3.42824149\n",
      "Trained batch 333 batch loss 3.16706085 epoch total loss 3.42745733\n",
      "Trained batch 334 batch loss 3.22545123 epoch total loss 3.42685246\n",
      "Trained batch 335 batch loss 3.29937029 epoch total loss 3.42647171\n",
      "Trained batch 336 batch loss 3.28888512 epoch total loss 3.42606235\n",
      "Trained batch 337 batch loss 3.49786663 epoch total loss 3.42627573\n",
      "Trained batch 338 batch loss 3.49572659 epoch total loss 3.42648125\n",
      "Trained batch 339 batch loss 3.43531775 epoch total loss 3.42650723\n",
      "Trained batch 340 batch loss 3.44033909 epoch total loss 3.42654777\n",
      "Trained batch 341 batch loss 3.42383075 epoch total loss 3.42654\n",
      "Trained batch 342 batch loss 3.35010242 epoch total loss 3.42631626\n",
      "Trained batch 343 batch loss 3.31530571 epoch total loss 3.42599273\n",
      "Trained batch 344 batch loss 3.14264154 epoch total loss 3.42516923\n",
      "Trained batch 345 batch loss 3.31555557 epoch total loss 3.42485142\n",
      "Trained batch 346 batch loss 3.34762979 epoch total loss 3.42462826\n",
      "Trained batch 347 batch loss 3.12627602 epoch total loss 3.42376828\n",
      "Trained batch 348 batch loss 3.25515246 epoch total loss 3.42328382\n",
      "Trained batch 349 batch loss 3.31745315 epoch total loss 3.42298055\n",
      "Trained batch 350 batch loss 3.38496184 epoch total loss 3.42287207\n",
      "Trained batch 351 batch loss 3.3275857 epoch total loss 3.42260075\n",
      "Trained batch 352 batch loss 3.42647552 epoch total loss 3.42261195\n",
      "Trained batch 353 batch loss 3.41415095 epoch total loss 3.42258811\n",
      "Trained batch 354 batch loss 3.36891818 epoch total loss 3.42243648\n",
      "Trained batch 355 batch loss 3.30959082 epoch total loss 3.42211843\n",
      "Trained batch 356 batch loss 3.21440744 epoch total loss 3.42153478\n",
      "Trained batch 357 batch loss 3.28793478 epoch total loss 3.4211607\n",
      "Trained batch 358 batch loss 3.185781 epoch total loss 3.42050314\n",
      "Trained batch 359 batch loss 3.25316262 epoch total loss 3.42003703\n",
      "Trained batch 360 batch loss 3.36390162 epoch total loss 3.41988111\n",
      "Trained batch 361 batch loss 3.2299242 epoch total loss 3.41935515\n",
      "Trained batch 362 batch loss 2.94561529 epoch total loss 3.41804624\n",
      "Trained batch 363 batch loss 3.0947237 epoch total loss 3.4171555\n",
      "Trained batch 364 batch loss 3.35000324 epoch total loss 3.41697097\n",
      "Trained batch 365 batch loss 3.30820274 epoch total loss 3.41667318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 366 batch loss 3.1496439 epoch total loss 3.41594362\n",
      "Trained batch 367 batch loss 3.20007944 epoch total loss 3.41535544\n",
      "Trained batch 368 batch loss 3.03381038 epoch total loss 3.41431856\n",
      "Trained batch 369 batch loss 3.40681076 epoch total loss 3.4142983\n",
      "Trained batch 370 batch loss 3.17637205 epoch total loss 3.41365528\n",
      "Trained batch 371 batch loss 3.0846138 epoch total loss 3.41276836\n",
      "Trained batch 372 batch loss 3.14076424 epoch total loss 3.41203713\n",
      "Trained batch 373 batch loss 3.12268591 epoch total loss 3.41126132\n",
      "Trained batch 374 batch loss 3.30079269 epoch total loss 3.41096592\n",
      "Trained batch 375 batch loss 3.09790087 epoch total loss 3.41013122\n",
      "Trained batch 376 batch loss 3.18670845 epoch total loss 3.40953708\n",
      "Trained batch 377 batch loss 3.1478405 epoch total loss 3.40884304\n",
      "Trained batch 378 batch loss 3.2162745 epoch total loss 3.40833354\n",
      "Trained batch 379 batch loss 3.05482769 epoch total loss 3.40740085\n",
      "Trained batch 380 batch loss 3.12438822 epoch total loss 3.40665603\n",
      "Trained batch 381 batch loss 2.77527452 epoch total loss 3.40499878\n",
      "Trained batch 382 batch loss 3.13359952 epoch total loss 3.40428829\n",
      "Trained batch 383 batch loss 3.25764656 epoch total loss 3.40390539\n",
      "Trained batch 384 batch loss 3.15459085 epoch total loss 3.40325618\n",
      "Trained batch 385 batch loss 3.15017748 epoch total loss 3.40259862\n",
      "Trained batch 386 batch loss 3.03864861 epoch total loss 3.40165591\n",
      "Trained batch 387 batch loss 3.08326316 epoch total loss 3.40083313\n",
      "Trained batch 388 batch loss 3.3133378 epoch total loss 3.40060782\n",
      "Trained batch 389 batch loss 3.34866118 epoch total loss 3.40047407\n",
      "Trained batch 390 batch loss 3.31729865 epoch total loss 3.40026069\n",
      "Trained batch 391 batch loss 3.63735962 epoch total loss 3.40086699\n",
      "Trained batch 392 batch loss 3.59458613 epoch total loss 3.40136123\n",
      "Trained batch 393 batch loss 3.69462299 epoch total loss 3.40210748\n",
      "Trained batch 394 batch loss 3.64391303 epoch total loss 3.40272117\n",
      "Trained batch 395 batch loss 3.3103621 epoch total loss 3.40248704\n",
      "Trained batch 396 batch loss 2.86475062 epoch total loss 3.40112925\n",
      "Trained batch 397 batch loss 3.02778983 epoch total loss 3.40018892\n",
      "Trained batch 398 batch loss 3.15918922 epoch total loss 3.39958334\n",
      "Trained batch 399 batch loss 3.10479069 epoch total loss 3.39884448\n",
      "Trained batch 400 batch loss 3.05045772 epoch total loss 3.3979733\n",
      "Trained batch 401 batch loss 3.37174201 epoch total loss 3.39790773\n",
      "Trained batch 402 batch loss 3.03630161 epoch total loss 3.39700818\n",
      "Trained batch 403 batch loss 3.26788974 epoch total loss 3.39668798\n",
      "Trained batch 404 batch loss 3.0823493 epoch total loss 3.39591\n",
      "Trained batch 405 batch loss 3.1313839 epoch total loss 3.39525676\n",
      "Trained batch 406 batch loss 3.07838583 epoch total loss 3.39447618\n",
      "Trained batch 407 batch loss 3.00637484 epoch total loss 3.3935225\n",
      "Trained batch 408 batch loss 3.29016662 epoch total loss 3.3932693\n",
      "Trained batch 409 batch loss 3.24773812 epoch total loss 3.39291334\n",
      "Trained batch 410 batch loss 3.14297247 epoch total loss 3.39230371\n",
      "Trained batch 411 batch loss 3.22713137 epoch total loss 3.39190173\n",
      "Trained batch 412 batch loss 3.36733198 epoch total loss 3.39184213\n",
      "Trained batch 413 batch loss 3.4484992 epoch total loss 3.39197922\n",
      "Trained batch 414 batch loss 3.36604309 epoch total loss 3.39191675\n",
      "Trained batch 415 batch loss 3.35017633 epoch total loss 3.39181638\n",
      "Trained batch 416 batch loss 3.37623 epoch total loss 3.39177871\n",
      "Trained batch 417 batch loss 3.22156787 epoch total loss 3.39137053\n",
      "Trained batch 418 batch loss 3.31490326 epoch total loss 3.39118767\n",
      "Trained batch 419 batch loss 3.17008829 epoch total loss 3.39066\n",
      "Trained batch 420 batch loss 3.20386505 epoch total loss 3.39021516\n",
      "Trained batch 421 batch loss 3.18465567 epoch total loss 3.38972712\n",
      "Trained batch 422 batch loss 3.32815862 epoch total loss 3.38958097\n",
      "Trained batch 423 batch loss 3.24640298 epoch total loss 3.38924265\n",
      "Trained batch 424 batch loss 3.2578609 epoch total loss 3.3889327\n",
      "Trained batch 425 batch loss 3.32788181 epoch total loss 3.38878918\n",
      "Trained batch 426 batch loss 3.06699109 epoch total loss 3.38803363\n",
      "Trained batch 427 batch loss 3.1078465 epoch total loss 3.3873775\n",
      "Trained batch 428 batch loss 3.09814644 epoch total loss 3.38670158\n",
      "Trained batch 429 batch loss 3.12489223 epoch total loss 3.38609123\n",
      "Trained batch 430 batch loss 3.16988921 epoch total loss 3.38558865\n",
      "Trained batch 431 batch loss 3.05445027 epoch total loss 3.38482022\n",
      "Trained batch 432 batch loss 3.08524561 epoch total loss 3.38412666\n",
      "Trained batch 433 batch loss 3.01697373 epoch total loss 3.38327885\n",
      "Trained batch 434 batch loss 3.30180955 epoch total loss 3.38309097\n",
      "Trained batch 435 batch loss 3.15213251 epoch total loss 3.38256\n",
      "Trained batch 436 batch loss 3.26124191 epoch total loss 3.38228178\n",
      "Trained batch 437 batch loss 3.1710844 epoch total loss 3.38179851\n",
      "Trained batch 438 batch loss 3.30191374 epoch total loss 3.38161612\n",
      "Trained batch 439 batch loss 3.20784903 epoch total loss 3.38122034\n",
      "Trained batch 440 batch loss 2.91932583 epoch total loss 3.38017058\n",
      "Trained batch 441 batch loss 2.87060046 epoch total loss 3.37901497\n",
      "Trained batch 442 batch loss 2.77246809 epoch total loss 3.37764263\n",
      "Trained batch 443 batch loss 2.78104496 epoch total loss 3.37629604\n",
      "Trained batch 444 batch loss 2.72941065 epoch total loss 3.37483883\n",
      "Trained batch 445 batch loss 2.92391467 epoch total loss 3.37382555\n",
      "Trained batch 446 batch loss 3.15204477 epoch total loss 3.37332845\n",
      "Trained batch 447 batch loss 2.91020203 epoch total loss 3.37229228\n",
      "Trained batch 448 batch loss 3.10344696 epoch total loss 3.37169218\n",
      "Trained batch 449 batch loss 3.37093616 epoch total loss 3.37169051\n",
      "Trained batch 450 batch loss 3.42177129 epoch total loss 3.37180185\n",
      "Trained batch 451 batch loss 3.19186044 epoch total loss 3.37140274\n",
      "Trained batch 452 batch loss 3.34653091 epoch total loss 3.3713479\n",
      "Trained batch 453 batch loss 3.27959442 epoch total loss 3.37114525\n",
      "Trained batch 454 batch loss 3.19983053 epoch total loss 3.37076783\n",
      "Trained batch 455 batch loss 3.13543153 epoch total loss 3.37025046\n",
      "Trained batch 456 batch loss 3.1303103 epoch total loss 3.36972451\n",
      "Trained batch 457 batch loss 3.33224249 epoch total loss 3.3696425\n",
      "Trained batch 458 batch loss 3.4274435 epoch total loss 3.36976886\n",
      "Trained batch 459 batch loss 3.36445165 epoch total loss 3.36975741\n",
      "Trained batch 460 batch loss 3.34831619 epoch total loss 3.36971068\n",
      "Trained batch 461 batch loss 3.26052952 epoch total loss 3.3694737\n",
      "Trained batch 462 batch loss 3.15146971 epoch total loss 3.36900187\n",
      "Trained batch 463 batch loss 3.14684367 epoch total loss 3.36852217\n",
      "Trained batch 464 batch loss 3.27585864 epoch total loss 3.36832237\n",
      "Trained batch 465 batch loss 3.18956375 epoch total loss 3.36793804\n",
      "Trained batch 466 batch loss 3.29061341 epoch total loss 3.3677721\n",
      "Trained batch 467 batch loss 3.21825743 epoch total loss 3.36745214\n",
      "Trained batch 468 batch loss 2.95563769 epoch total loss 3.36657214\n",
      "Trained batch 469 batch loss 3.15275431 epoch total loss 3.36611629\n",
      "Trained batch 470 batch loss 3.01390791 epoch total loss 3.36536694\n",
      "Trained batch 471 batch loss 3.11601925 epoch total loss 3.36483741\n",
      "Trained batch 472 batch loss 3.16736817 epoch total loss 3.36441898\n",
      "Trained batch 473 batch loss 3.25809026 epoch total loss 3.36419415\n",
      "Trained batch 474 batch loss 3.24570847 epoch total loss 3.36394405\n",
      "Trained batch 475 batch loss 2.97871494 epoch total loss 3.36313319\n",
      "Trained batch 476 batch loss 2.83673239 epoch total loss 3.36202741\n",
      "Trained batch 477 batch loss 2.9167254 epoch total loss 3.361094\n",
      "Trained batch 478 batch loss 2.69933558 epoch total loss 3.3597095\n",
      "Trained batch 479 batch loss 2.76053643 epoch total loss 3.35845852\n",
      "Trained batch 480 batch loss 3.12924361 epoch total loss 3.3579812\n",
      "Trained batch 481 batch loss 3.42119598 epoch total loss 3.35811234\n",
      "Trained batch 482 batch loss 3.11406446 epoch total loss 3.35760593\n",
      "Trained batch 483 batch loss 3.21399283 epoch total loss 3.35730863\n",
      "Trained batch 484 batch loss 3.41518641 epoch total loss 3.35742807\n",
      "Trained batch 485 batch loss 3.37164378 epoch total loss 3.35745764\n",
      "Trained batch 486 batch loss 3.17492628 epoch total loss 3.35708213\n",
      "Trained batch 487 batch loss 3.28229928 epoch total loss 3.35692859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 488 batch loss 3.38509679 epoch total loss 3.35698628\n",
      "Trained batch 489 batch loss 3.04830623 epoch total loss 3.35635519\n",
      "Trained batch 490 batch loss 3.12002158 epoch total loss 3.35587287\n",
      "Trained batch 491 batch loss 3.23748732 epoch total loss 3.35563159\n",
      "Trained batch 492 batch loss 3.04857922 epoch total loss 3.35500741\n",
      "Trained batch 493 batch loss 2.97511482 epoch total loss 3.35423684\n",
      "Trained batch 494 batch loss 3.36731482 epoch total loss 3.35426331\n",
      "Trained batch 495 batch loss 3.30293083 epoch total loss 3.35415983\n",
      "Trained batch 496 batch loss 3.22721577 epoch total loss 3.35390377\n",
      "Trained batch 497 batch loss 3.3040452 epoch total loss 3.3538034\n",
      "Trained batch 498 batch loss 3.09690833 epoch total loss 3.3532877\n",
      "Trained batch 499 batch loss 3.18139267 epoch total loss 3.35294318\n",
      "Trained batch 500 batch loss 3.27514672 epoch total loss 3.35278749\n",
      "Trained batch 501 batch loss 3.28421426 epoch total loss 3.35265064\n",
      "Trained batch 502 batch loss 3.29201579 epoch total loss 3.35252976\n",
      "Trained batch 503 batch loss 3.06429958 epoch total loss 3.35195684\n",
      "Trained batch 504 batch loss 3.00464749 epoch total loss 3.35126781\n",
      "Trained batch 505 batch loss 2.81920028 epoch total loss 3.35021424\n",
      "Trained batch 506 batch loss 3.06896114 epoch total loss 3.34965825\n",
      "Trained batch 507 batch loss 2.90283275 epoch total loss 3.34877706\n",
      "Trained batch 508 batch loss 3.0620718 epoch total loss 3.34821248\n",
      "Trained batch 509 batch loss 2.74751496 epoch total loss 3.34703255\n",
      "Trained batch 510 batch loss 2.67871213 epoch total loss 3.34572196\n",
      "Trained batch 511 batch loss 2.82600355 epoch total loss 3.3447051\n",
      "Trained batch 512 batch loss 2.89785171 epoch total loss 3.34383225\n",
      "Trained batch 513 batch loss 3.176579 epoch total loss 3.34350634\n",
      "Trained batch 514 batch loss 3.39617181 epoch total loss 3.34360862\n",
      "Trained batch 515 batch loss 3.33241916 epoch total loss 3.34358692\n",
      "Trained batch 516 batch loss 3.16125846 epoch total loss 3.34323359\n",
      "Trained batch 517 batch loss 3.32904458 epoch total loss 3.34320617\n",
      "Trained batch 518 batch loss 3.44992828 epoch total loss 3.3434124\n",
      "Trained batch 519 batch loss 3.52529812 epoch total loss 3.34376264\n",
      "Trained batch 520 batch loss 3.24670076 epoch total loss 3.34357595\n",
      "Trained batch 521 batch loss 3.42740822 epoch total loss 3.34373689\n",
      "Trained batch 522 batch loss 3.34117365 epoch total loss 3.34373188\n",
      "Trained batch 523 batch loss 3.15359902 epoch total loss 3.34336829\n",
      "Trained batch 524 batch loss 3.27968669 epoch total loss 3.3432467\n",
      "Trained batch 525 batch loss 3.16357088 epoch total loss 3.34290457\n",
      "Trained batch 526 batch loss 3.13571119 epoch total loss 3.3425107\n",
      "Trained batch 527 batch loss 3.13977814 epoch total loss 3.34212613\n",
      "Trained batch 528 batch loss 3.0685668 epoch total loss 3.34160805\n",
      "Trained batch 529 batch loss 3.23053122 epoch total loss 3.34139824\n",
      "Trained batch 530 batch loss 3.31519055 epoch total loss 3.34134865\n",
      "Trained batch 531 batch loss 3.3029902 epoch total loss 3.34127641\n",
      "Trained batch 532 batch loss 3.19126797 epoch total loss 3.34099436\n",
      "Trained batch 533 batch loss 3.17493916 epoch total loss 3.34068298\n",
      "Trained batch 534 batch loss 3.24432087 epoch total loss 3.34050226\n",
      "Trained batch 535 batch loss 3.19154406 epoch total loss 3.34022379\n",
      "Trained batch 536 batch loss 3.27044725 epoch total loss 3.34009385\n",
      "Trained batch 537 batch loss 3.25455523 epoch total loss 3.33993435\n",
      "Trained batch 538 batch loss 3.21465254 epoch total loss 3.33970141\n",
      "Trained batch 539 batch loss 3.00960636 epoch total loss 3.33908916\n",
      "Trained batch 540 batch loss 3.18228626 epoch total loss 3.33879876\n",
      "Trained batch 541 batch loss 3.31243753 epoch total loss 3.33875\n",
      "Trained batch 542 batch loss 3.16064477 epoch total loss 3.33842134\n",
      "Trained batch 543 batch loss 3.21510029 epoch total loss 3.33819413\n",
      "Trained batch 544 batch loss 3.18478107 epoch total loss 3.33791208\n",
      "Trained batch 545 batch loss 3.06292868 epoch total loss 3.33740783\n",
      "Trained batch 546 batch loss 2.81655741 epoch total loss 3.33645368\n",
      "Trained batch 547 batch loss 3.38726377 epoch total loss 3.33654642\n",
      "Trained batch 548 batch loss 3.26212049 epoch total loss 3.33641052\n",
      "Trained batch 549 batch loss 3.48718095 epoch total loss 3.33668518\n",
      "Trained batch 550 batch loss 3.36561275 epoch total loss 3.33673787\n",
      "Trained batch 551 batch loss 3.45105648 epoch total loss 3.3369453\n",
      "Trained batch 552 batch loss 3.10898328 epoch total loss 3.33653235\n",
      "Trained batch 553 batch loss 3.44045448 epoch total loss 3.33672023\n",
      "Trained batch 554 batch loss 3.42092848 epoch total loss 3.3368721\n",
      "Trained batch 555 batch loss 3.60779834 epoch total loss 3.33736038\n",
      "Trained batch 556 batch loss 3.58156538 epoch total loss 3.33779955\n",
      "Trained batch 557 batch loss 3.45070982 epoch total loss 3.3380022\n",
      "Trained batch 558 batch loss 3.43228555 epoch total loss 3.33817101\n",
      "Trained batch 559 batch loss 3.40848207 epoch total loss 3.33829689\n",
      "Trained batch 560 batch loss 3.23432541 epoch total loss 3.33811116\n",
      "Trained batch 561 batch loss 3.43242049 epoch total loss 3.33827925\n",
      "Trained batch 562 batch loss 3.1873095 epoch total loss 3.33801055\n",
      "Trained batch 563 batch loss 3.03965616 epoch total loss 3.33748055\n",
      "Trained batch 564 batch loss 3.33330154 epoch total loss 3.33747315\n",
      "Trained batch 565 batch loss 3.12382936 epoch total loss 3.33709478\n",
      "Trained batch 566 batch loss 3.29383278 epoch total loss 3.33701849\n",
      "Trained batch 567 batch loss 3.40747333 epoch total loss 3.33714271\n",
      "Trained batch 568 batch loss 3.11847687 epoch total loss 3.3367579\n",
      "Trained batch 569 batch loss 3.13495207 epoch total loss 3.33640337\n",
      "Trained batch 570 batch loss 3.32180381 epoch total loss 3.33637762\n",
      "Trained batch 571 batch loss 3.08422899 epoch total loss 3.33593607\n",
      "Trained batch 572 batch loss 2.79619026 epoch total loss 3.33499241\n",
      "Trained batch 573 batch loss 3.19205022 epoch total loss 3.33474278\n",
      "Trained batch 574 batch loss 3.00540519 epoch total loss 3.33416891\n",
      "Trained batch 575 batch loss 2.83495617 epoch total loss 3.33330083\n",
      "Trained batch 576 batch loss 3.13167763 epoch total loss 3.33295083\n",
      "Trained batch 577 batch loss 3.06361055 epoch total loss 3.33248401\n",
      "Trained batch 578 batch loss 3.04951572 epoch total loss 3.33199453\n",
      "Trained batch 579 batch loss 2.87818384 epoch total loss 3.33121061\n",
      "Trained batch 580 batch loss 3.18882322 epoch total loss 3.33096528\n",
      "Trained batch 581 batch loss 3.17282391 epoch total loss 3.33069301\n",
      "Trained batch 582 batch loss 3.15791035 epoch total loss 3.33039641\n",
      "Trained batch 583 batch loss 2.49623823 epoch total loss 3.32896543\n",
      "Trained batch 584 batch loss 2.84876156 epoch total loss 3.32814312\n",
      "Trained batch 585 batch loss 2.82036161 epoch total loss 3.32727504\n",
      "Trained batch 586 batch loss 2.81988454 epoch total loss 3.3264091\n",
      "Trained batch 587 batch loss 2.75437021 epoch total loss 3.32543468\n",
      "Trained batch 588 batch loss 2.92809224 epoch total loss 3.32475901\n",
      "Trained batch 589 batch loss 2.82358432 epoch total loss 3.32390809\n",
      "Trained batch 590 batch loss 2.80699849 epoch total loss 3.3230319\n",
      "Trained batch 591 batch loss 2.82584095 epoch total loss 3.32219076\n",
      "Trained batch 592 batch loss 3.07826114 epoch total loss 3.32177854\n",
      "Trained batch 593 batch loss 3.23845863 epoch total loss 3.32163787\n",
      "Trained batch 594 batch loss 3.08192253 epoch total loss 3.32123446\n",
      "Trained batch 595 batch loss 3.09189987 epoch total loss 3.32084894\n",
      "Trained batch 596 batch loss 3.13631701 epoch total loss 3.32053947\n",
      "Trained batch 597 batch loss 3.1897254 epoch total loss 3.32032037\n",
      "Trained batch 598 batch loss 3.16320705 epoch total loss 3.32005763\n",
      "Trained batch 599 batch loss 3.31169987 epoch total loss 3.32004356\n",
      "Trained batch 600 batch loss 3.31362486 epoch total loss 3.32003284\n",
      "Trained batch 601 batch loss 3.41560459 epoch total loss 3.32019186\n",
      "Trained batch 602 batch loss 3.22852015 epoch total loss 3.32003951\n",
      "Trained batch 603 batch loss 3.29096484 epoch total loss 3.31999135\n",
      "Trained batch 604 batch loss 3.29279089 epoch total loss 3.31994653\n",
      "Trained batch 605 batch loss 3.19202757 epoch total loss 3.31973505\n",
      "Trained batch 606 batch loss 3.30022335 epoch total loss 3.31970286\n",
      "Trained batch 607 batch loss 3.02491093 epoch total loss 3.31921697\n",
      "Trained batch 608 batch loss 3.29333973 epoch total loss 3.31917453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 609 batch loss 3.25492167 epoch total loss 3.31906891\n",
      "Trained batch 610 batch loss 3.23411179 epoch total loss 3.31892967\n",
      "Trained batch 611 batch loss 3.14022255 epoch total loss 3.31863737\n",
      "Trained batch 612 batch loss 3.35175157 epoch total loss 3.31869149\n",
      "Trained batch 613 batch loss 3.19419909 epoch total loss 3.31848836\n",
      "Trained batch 614 batch loss 3.31624079 epoch total loss 3.31848478\n",
      "Trained batch 615 batch loss 3.29217863 epoch total loss 3.31844211\n",
      "Trained batch 616 batch loss 3.16509986 epoch total loss 3.3181932\n",
      "Trained batch 617 batch loss 3.31788111 epoch total loss 3.31819248\n",
      "Trained batch 618 batch loss 3.14072466 epoch total loss 3.31790543\n",
      "Trained batch 619 batch loss 3.15820479 epoch total loss 3.31764746\n",
      "Trained batch 620 batch loss 3.21280527 epoch total loss 3.31747842\n",
      "Trained batch 621 batch loss 3.18144917 epoch total loss 3.31725931\n",
      "Trained batch 622 batch loss 3.27219081 epoch total loss 3.31718683\n",
      "Trained batch 623 batch loss 3.21870875 epoch total loss 3.317029\n",
      "Trained batch 624 batch loss 3.37310195 epoch total loss 3.31711864\n",
      "Trained batch 625 batch loss 3.3464303 epoch total loss 3.31716561\n",
      "Trained batch 626 batch loss 3.15819788 epoch total loss 3.3169117\n",
      "Trained batch 627 batch loss 3.15449691 epoch total loss 3.31665277\n",
      "Trained batch 628 batch loss 3.12125897 epoch total loss 3.31634164\n",
      "Trained batch 629 batch loss 3.13255215 epoch total loss 3.31604958\n",
      "Trained batch 630 batch loss 3.27423835 epoch total loss 3.31598306\n",
      "Trained batch 631 batch loss 3.14632559 epoch total loss 3.31571412\n",
      "Trained batch 632 batch loss 3.20036793 epoch total loss 3.31553173\n",
      "Trained batch 633 batch loss 3.04344368 epoch total loss 3.31510186\n",
      "Trained batch 634 batch loss 3.20473838 epoch total loss 3.31492782\n",
      "Trained batch 635 batch loss 3.09629869 epoch total loss 3.31458354\n",
      "Trained batch 636 batch loss 3.28927135 epoch total loss 3.31454372\n",
      "Trained batch 637 batch loss 3.14984107 epoch total loss 3.31428528\n",
      "Trained batch 638 batch loss 3.14996243 epoch total loss 3.31402755\n",
      "Trained batch 639 batch loss 3.17974591 epoch total loss 3.31381726\n",
      "Trained batch 640 batch loss 3.11127281 epoch total loss 3.31350088\n",
      "Trained batch 641 batch loss 3.21593 epoch total loss 3.31334853\n",
      "Trained batch 642 batch loss 3.29501796 epoch total loss 3.31332\n",
      "Trained batch 643 batch loss 3.38300157 epoch total loss 3.3134284\n",
      "Trained batch 644 batch loss 3.03809953 epoch total loss 3.31300068\n",
      "Trained batch 645 batch loss 3.25560284 epoch total loss 3.31291175\n",
      "Trained batch 646 batch loss 3.14637017 epoch total loss 3.31265426\n",
      "Trained batch 647 batch loss 3.34705639 epoch total loss 3.31270742\n",
      "Trained batch 648 batch loss 2.99831581 epoch total loss 3.31222224\n",
      "Trained batch 649 batch loss 3.42603064 epoch total loss 3.31239772\n",
      "Trained batch 650 batch loss 3.12011218 epoch total loss 3.31210184\n",
      "Trained batch 651 batch loss 3.12973142 epoch total loss 3.3118217\n",
      "Trained batch 652 batch loss 2.74162483 epoch total loss 3.31094718\n",
      "Trained batch 653 batch loss 2.46707153 epoch total loss 3.30965471\n",
      "Trained batch 654 batch loss 2.61931539 epoch total loss 3.30859923\n",
      "Trained batch 655 batch loss 2.46979642 epoch total loss 3.30731869\n",
      "Trained batch 656 batch loss 3.05459785 epoch total loss 3.3069334\n",
      "Trained batch 657 batch loss 2.84844375 epoch total loss 3.30623555\n",
      "Trained batch 658 batch loss 3.20562506 epoch total loss 3.30608249\n",
      "Trained batch 659 batch loss 3.29755378 epoch total loss 3.30606961\n",
      "Trained batch 660 batch loss 3.49754906 epoch total loss 3.30635977\n",
      "Trained batch 661 batch loss 3.71453428 epoch total loss 3.30697751\n",
      "Trained batch 662 batch loss 3.78370929 epoch total loss 3.30769753\n",
      "Trained batch 663 batch loss 3.1287632 epoch total loss 3.30742764\n",
      "Trained batch 664 batch loss 3.18796802 epoch total loss 3.30724764\n",
      "Trained batch 665 batch loss 2.97677159 epoch total loss 3.30675077\n",
      "Trained batch 666 batch loss 2.5185492 epoch total loss 3.30556726\n",
      "Trained batch 667 batch loss 2.97128677 epoch total loss 3.30506587\n",
      "Trained batch 668 batch loss 3.24480963 epoch total loss 3.30497575\n",
      "Trained batch 669 batch loss 3.42825484 epoch total loss 3.30516\n",
      "Trained batch 670 batch loss 3.35995197 epoch total loss 3.30524182\n",
      "Trained batch 671 batch loss 3.40241194 epoch total loss 3.30538654\n",
      "Trained batch 672 batch loss 3.35852814 epoch total loss 3.3054657\n",
      "Trained batch 673 batch loss 3.21815538 epoch total loss 3.30533624\n",
      "Trained batch 674 batch loss 3.24189353 epoch total loss 3.30524206\n",
      "Trained batch 675 batch loss 3.25966883 epoch total loss 3.30517459\n",
      "Trained batch 676 batch loss 3.18093538 epoch total loss 3.30499077\n",
      "Trained batch 677 batch loss 3.28559041 epoch total loss 3.3049624\n",
      "Trained batch 678 batch loss 3.32890415 epoch total loss 3.30499744\n",
      "Trained batch 679 batch loss 3.39378357 epoch total loss 3.30512834\n",
      "Trained batch 680 batch loss 3.11060452 epoch total loss 3.30484223\n",
      "Trained batch 681 batch loss 3.30495906 epoch total loss 3.30484247\n",
      "Trained batch 682 batch loss 3.10016298 epoch total loss 3.30454206\n",
      "Trained batch 683 batch loss 3.15006495 epoch total loss 3.30431604\n",
      "Trained batch 684 batch loss 2.97850847 epoch total loss 3.30383968\n",
      "Trained batch 685 batch loss 3.12399459 epoch total loss 3.30357718\n",
      "Trained batch 686 batch loss 3.13501692 epoch total loss 3.30333161\n",
      "Trained batch 687 batch loss 3.08600378 epoch total loss 3.30301523\n",
      "Trained batch 688 batch loss 3.22692585 epoch total loss 3.30290437\n",
      "Trained batch 689 batch loss 3.27008557 epoch total loss 3.30285668\n",
      "Trained batch 690 batch loss 3.29389668 epoch total loss 3.30284381\n",
      "Trained batch 691 batch loss 3.03059149 epoch total loss 3.3024497\n",
      "Trained batch 692 batch loss 3.09761572 epoch total loss 3.30215359\n",
      "Trained batch 693 batch loss 2.44622684 epoch total loss 3.30091858\n",
      "Trained batch 694 batch loss 3.06138062 epoch total loss 3.30057335\n",
      "Trained batch 695 batch loss 3.27620411 epoch total loss 3.30053806\n",
      "Trained batch 696 batch loss 3.31958437 epoch total loss 3.30056548\n",
      "Trained batch 697 batch loss 3.24179721 epoch total loss 3.30048108\n",
      "Trained batch 698 batch loss 2.87650537 epoch total loss 3.29987359\n",
      "Trained batch 699 batch loss 3.35566711 epoch total loss 3.29995346\n",
      "Trained batch 700 batch loss 3.15426421 epoch total loss 3.29974532\n",
      "Trained batch 701 batch loss 3.06739426 epoch total loss 3.29941392\n",
      "Trained batch 702 batch loss 3.28152609 epoch total loss 3.29938841\n",
      "Trained batch 703 batch loss 3.28377676 epoch total loss 3.299366\n",
      "Trained batch 704 batch loss 3.31437469 epoch total loss 3.29938745\n",
      "Trained batch 705 batch loss 3.35381413 epoch total loss 3.2994647\n",
      "Trained batch 706 batch loss 3.22664976 epoch total loss 3.29936147\n",
      "Trained batch 707 batch loss 3.33241796 epoch total loss 3.2994082\n",
      "Trained batch 708 batch loss 3.20401645 epoch total loss 3.29927373\n",
      "Trained batch 709 batch loss 3.19294548 epoch total loss 3.29912353\n",
      "Trained batch 710 batch loss 3.14697981 epoch total loss 3.29890919\n",
      "Trained batch 711 batch loss 2.91418219 epoch total loss 3.29836798\n",
      "Trained batch 712 batch loss 3.02245474 epoch total loss 3.29798055\n",
      "Trained batch 713 batch loss 2.74497175 epoch total loss 3.29720473\n",
      "Trained batch 714 batch loss 2.8757813 epoch total loss 3.29661441\n",
      "Trained batch 715 batch loss 2.71217608 epoch total loss 3.29579711\n",
      "Trained batch 716 batch loss 2.6557672 epoch total loss 3.29490304\n",
      "Trained batch 717 batch loss 2.89260721 epoch total loss 3.29434204\n",
      "Trained batch 718 batch loss 3.06681395 epoch total loss 3.29402518\n",
      "Trained batch 719 batch loss 2.73514533 epoch total loss 3.29324794\n",
      "Trained batch 720 batch loss 2.60900903 epoch total loss 3.2922976\n",
      "Trained batch 721 batch loss 2.60209155 epoch total loss 3.29134035\n",
      "Trained batch 722 batch loss 2.5435 epoch total loss 3.29030442\n",
      "Trained batch 723 batch loss 2.52286339 epoch total loss 3.28924322\n",
      "Trained batch 724 batch loss 2.49971676 epoch total loss 3.28815269\n",
      "Trained batch 725 batch loss 2.44789171 epoch total loss 3.28699398\n",
      "Trained batch 726 batch loss 2.37713766 epoch total loss 3.28574061\n",
      "Trained batch 727 batch loss 2.69830632 epoch total loss 3.28493261\n",
      "Trained batch 728 batch loss 3.38169289 epoch total loss 3.28506541\n",
      "Trained batch 729 batch loss 3.4131217 epoch total loss 3.28524089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 730 batch loss 3.35564232 epoch total loss 3.28533745\n",
      "Trained batch 731 batch loss 3.26584148 epoch total loss 3.28531098\n",
      "Trained batch 732 batch loss 3.24882674 epoch total loss 3.28526092\n",
      "Trained batch 733 batch loss 2.96360111 epoch total loss 3.28482223\n",
      "Trained batch 734 batch loss 3.16572595 epoch total loss 3.28466\n",
      "Trained batch 735 batch loss 2.73605132 epoch total loss 3.28391361\n",
      "Trained batch 736 batch loss 3.22394204 epoch total loss 3.28383207\n",
      "Trained batch 737 batch loss 3.2965169 epoch total loss 3.28384948\n",
      "Trained batch 738 batch loss 2.90873671 epoch total loss 3.28334117\n",
      "Trained batch 739 batch loss 3.17750263 epoch total loss 3.28319788\n",
      "Trained batch 740 batch loss 3.29742551 epoch total loss 3.28321695\n",
      "Trained batch 741 batch loss 3.21521378 epoch total loss 3.2831254\n",
      "Trained batch 742 batch loss 3.18361807 epoch total loss 3.28299117\n",
      "Trained batch 743 batch loss 3.34792066 epoch total loss 3.28307867\n",
      "Trained batch 744 batch loss 3.23147917 epoch total loss 3.28300929\n",
      "Trained batch 745 batch loss 3.00810957 epoch total loss 3.28264022\n",
      "Trained batch 746 batch loss 3.22722912 epoch total loss 3.28256583\n",
      "Trained batch 747 batch loss 2.81337214 epoch total loss 3.28193808\n",
      "Trained batch 748 batch loss 2.65868735 epoch total loss 3.2811048\n",
      "Trained batch 749 batch loss 2.57823563 epoch total loss 3.28016615\n",
      "Trained batch 750 batch loss 2.79262829 epoch total loss 3.27951622\n",
      "Trained batch 751 batch loss 3.03815842 epoch total loss 3.27919483\n",
      "Trained batch 752 batch loss 3.20317578 epoch total loss 3.27909374\n",
      "Trained batch 753 batch loss 3.34283733 epoch total loss 3.27917814\n",
      "Trained batch 754 batch loss 3.31346369 epoch total loss 3.27922368\n",
      "Trained batch 755 batch loss 3.20991516 epoch total loss 3.27913189\n",
      "Trained batch 756 batch loss 3.32662702 epoch total loss 3.27919483\n",
      "Trained batch 757 batch loss 3.20546889 epoch total loss 3.27909756\n",
      "Trained batch 758 batch loss 3.24196768 epoch total loss 3.27904844\n",
      "Trained batch 759 batch loss 3.41272569 epoch total loss 3.27922487\n",
      "Trained batch 760 batch loss 3.40407658 epoch total loss 3.27938914\n",
      "Trained batch 761 batch loss 3.42769623 epoch total loss 3.27958393\n",
      "Trained batch 762 batch loss 3.37445307 epoch total loss 3.27970862\n",
      "Trained batch 763 batch loss 3.2403388 epoch total loss 3.27965689\n",
      "Trained batch 764 batch loss 3.15580845 epoch total loss 3.27949476\n",
      "Trained batch 765 batch loss 3.20908618 epoch total loss 3.27940249\n",
      "Trained batch 766 batch loss 3.27884674 epoch total loss 3.27940178\n",
      "Trained batch 767 batch loss 3.22156262 epoch total loss 3.27932644\n",
      "Trained batch 768 batch loss 3.09699869 epoch total loss 3.27908897\n",
      "Trained batch 769 batch loss 3.24345684 epoch total loss 3.27904248\n",
      "Trained batch 770 batch loss 3.19866848 epoch total loss 3.27893829\n",
      "Trained batch 771 batch loss 3.2289989 epoch total loss 3.27887344\n",
      "Trained batch 772 batch loss 3.05805254 epoch total loss 3.27858758\n",
      "Trained batch 773 batch loss 3.22982645 epoch total loss 3.2785244\n",
      "Trained batch 774 batch loss 3.22467518 epoch total loss 3.27845478\n",
      "Trained batch 775 batch loss 3.24783325 epoch total loss 3.2784152\n",
      "Trained batch 776 batch loss 3.34382987 epoch total loss 3.27849936\n",
      "Trained batch 777 batch loss 3.22074175 epoch total loss 3.27842498\n",
      "Trained batch 778 batch loss 3.0405798 epoch total loss 3.27811909\n",
      "Trained batch 779 batch loss 3.06130815 epoch total loss 3.27784085\n",
      "Trained batch 780 batch loss 3.09005833 epoch total loss 3.2776\n",
      "Trained batch 781 batch loss 3.11980057 epoch total loss 3.27739811\n",
      "Trained batch 782 batch loss 3.05435658 epoch total loss 3.27711296\n",
      "Trained batch 783 batch loss 2.65423512 epoch total loss 3.2763176\n",
      "Trained batch 784 batch loss 3.12230468 epoch total loss 3.27612114\n",
      "Trained batch 785 batch loss 2.75129914 epoch total loss 3.27545261\n",
      "Trained batch 786 batch loss 3.23477864 epoch total loss 3.27540088\n",
      "Trained batch 787 batch loss 3.30123663 epoch total loss 3.27543378\n",
      "Trained batch 788 batch loss 3.1188519 epoch total loss 3.27523518\n",
      "Trained batch 789 batch loss 3.17766953 epoch total loss 3.27511144\n",
      "Trained batch 790 batch loss 3.06990814 epoch total loss 3.27485156\n",
      "Trained batch 791 batch loss 3.14252734 epoch total loss 3.27468443\n",
      "Trained batch 792 batch loss 3.14604545 epoch total loss 3.27452207\n",
      "Trained batch 793 batch loss 3.09064651 epoch total loss 3.27429\n",
      "Trained batch 794 batch loss 2.73768139 epoch total loss 3.27361441\n",
      "Trained batch 795 batch loss 2.71210384 epoch total loss 3.27290797\n",
      "Trained batch 796 batch loss 2.96819472 epoch total loss 3.27252531\n",
      "Trained batch 797 batch loss 3.28179193 epoch total loss 3.27253699\n",
      "Trained batch 798 batch loss 2.87935638 epoch total loss 3.27204418\n",
      "Trained batch 799 batch loss 2.98401141 epoch total loss 3.27168393\n",
      "Trained batch 800 batch loss 2.87255 epoch total loss 3.27118492\n",
      "Trained batch 801 batch loss 2.93047762 epoch total loss 3.27075958\n",
      "Trained batch 802 batch loss 2.98673534 epoch total loss 3.27040553\n",
      "Trained batch 803 batch loss 3.03538895 epoch total loss 3.27011275\n",
      "Trained batch 804 batch loss 2.8179419 epoch total loss 3.26955032\n",
      "Trained batch 805 batch loss 2.79159474 epoch total loss 3.26895642\n",
      "Trained batch 806 batch loss 2.80875444 epoch total loss 3.26838565\n",
      "Trained batch 807 batch loss 2.96798873 epoch total loss 3.26801348\n",
      "Trained batch 808 batch loss 2.98883057 epoch total loss 3.26766777\n",
      "Trained batch 809 batch loss 2.49079657 epoch total loss 3.26670742\n",
      "Trained batch 810 batch loss 2.64229822 epoch total loss 3.26593661\n",
      "Trained batch 811 batch loss 3.2147193 epoch total loss 3.26587343\n",
      "Trained batch 812 batch loss 3.05166245 epoch total loss 3.26560974\n",
      "Trained batch 813 batch loss 3.21594143 epoch total loss 3.26554847\n",
      "Trained batch 814 batch loss 3.01292276 epoch total loss 3.26523805\n",
      "Trained batch 815 batch loss 2.65309334 epoch total loss 3.26448703\n",
      "Trained batch 816 batch loss 3.16573143 epoch total loss 3.26436591\n",
      "Trained batch 817 batch loss 3.20656443 epoch total loss 3.26429534\n",
      "Trained batch 818 batch loss 3.11184692 epoch total loss 3.2641089\n",
      "Trained batch 819 batch loss 3.03920889 epoch total loss 3.26383424\n",
      "Trained batch 820 batch loss 3.18378186 epoch total loss 3.26373672\n",
      "Trained batch 821 batch loss 3.14516211 epoch total loss 3.26359248\n",
      "Trained batch 822 batch loss 3.20533466 epoch total loss 3.26352167\n",
      "Trained batch 823 batch loss 3.1400671 epoch total loss 3.26337171\n",
      "Trained batch 824 batch loss 3.12643814 epoch total loss 3.26320553\n",
      "Trained batch 825 batch loss 2.58668447 epoch total loss 3.26238537\n",
      "Trained batch 826 batch loss 2.54057455 epoch total loss 3.26151156\n",
      "Trained batch 827 batch loss 2.52615333 epoch total loss 3.26062226\n",
      "Trained batch 828 batch loss 2.55436778 epoch total loss 3.25976944\n",
      "Trained batch 829 batch loss 3.03459883 epoch total loss 3.25949788\n",
      "Trained batch 830 batch loss 3.29581046 epoch total loss 3.25954175\n",
      "Trained batch 831 batch loss 3.4174242 epoch total loss 3.25973177\n",
      "Trained batch 832 batch loss 3.19620061 epoch total loss 3.25965548\n",
      "Trained batch 833 batch loss 3.21184635 epoch total loss 3.25959826\n",
      "Trained batch 834 batch loss 3.29427385 epoch total loss 3.25963974\n",
      "Trained batch 835 batch loss 3.1703403 epoch total loss 3.25953293\n",
      "Trained batch 836 batch loss 3.07776046 epoch total loss 3.25931549\n",
      "Trained batch 837 batch loss 3.23482656 epoch total loss 3.2592864\n",
      "Trained batch 838 batch loss 3.48651075 epoch total loss 3.25955772\n",
      "Trained batch 839 batch loss 3.37134743 epoch total loss 3.25969076\n",
      "Trained batch 840 batch loss 3.24353743 epoch total loss 3.25967169\n",
      "Trained batch 841 batch loss 3.09941053 epoch total loss 3.25948119\n",
      "Trained batch 842 batch loss 2.95144367 epoch total loss 3.25911522\n",
      "Trained batch 843 batch loss 3.356076 epoch total loss 3.25923014\n",
      "Trained batch 844 batch loss 3.34925771 epoch total loss 3.25933695\n",
      "Trained batch 845 batch loss 3.40057564 epoch total loss 3.25950408\n",
      "Trained batch 846 batch loss 3.35108447 epoch total loss 3.25961232\n",
      "Trained batch 847 batch loss 3.22998452 epoch total loss 3.25957751\n",
      "Trained batch 848 batch loss 3.22053766 epoch total loss 3.25953126\n",
      "Trained batch 849 batch loss 2.99315357 epoch total loss 3.2592175\n",
      "Trained batch 850 batch loss 3.17019844 epoch total loss 3.25911283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 851 batch loss 3.08953357 epoch total loss 3.25891352\n",
      "Trained batch 852 batch loss 3.12805676 epoch total loss 3.25876021\n",
      "Trained batch 853 batch loss 2.79696894 epoch total loss 3.25821853\n",
      "Trained batch 854 batch loss 2.68381548 epoch total loss 3.25754595\n",
      "Trained batch 855 batch loss 2.75530338 epoch total loss 3.25695872\n",
      "Trained batch 856 batch loss 2.97025967 epoch total loss 3.25662374\n",
      "Trained batch 857 batch loss 2.9227047 epoch total loss 3.25623393\n",
      "Trained batch 858 batch loss 3.32266903 epoch total loss 3.25631142\n",
      "Trained batch 859 batch loss 3.33669615 epoch total loss 3.25640512\n",
      "Trained batch 860 batch loss 3.08624172 epoch total loss 3.25620723\n",
      "Trained batch 861 batch loss 3.18972683 epoch total loss 3.25613\n",
      "Trained batch 862 batch loss 2.92237 epoch total loss 3.25574279\n",
      "Trained batch 863 batch loss 3.21349669 epoch total loss 3.25569367\n",
      "Trained batch 864 batch loss 3.26385593 epoch total loss 3.25570321\n",
      "Trained batch 865 batch loss 3.15333295 epoch total loss 3.25558472\n",
      "Trained batch 866 batch loss 3.09364915 epoch total loss 3.2553978\n",
      "Trained batch 867 batch loss 3.11291814 epoch total loss 3.25523376\n",
      "Trained batch 868 batch loss 2.68397331 epoch total loss 3.25457573\n",
      "Trained batch 869 batch loss 2.95293307 epoch total loss 3.25422859\n",
      "Trained batch 870 batch loss 2.90532303 epoch total loss 3.25382733\n",
      "Trained batch 871 batch loss 3.11063337 epoch total loss 3.25366306\n",
      "Trained batch 872 batch loss 3.06406021 epoch total loss 3.25344539\n",
      "Trained batch 873 batch loss 3.1256547 epoch total loss 3.253299\n",
      "Trained batch 874 batch loss 3.14906168 epoch total loss 3.25318\n",
      "Trained batch 875 batch loss 3.12185884 epoch total loss 3.25302982\n",
      "Trained batch 876 batch loss 3.20924592 epoch total loss 3.25297976\n",
      "Trained batch 877 batch loss 3.42187619 epoch total loss 3.2531724\n",
      "Trained batch 878 batch loss 3.25606298 epoch total loss 3.25317574\n",
      "Trained batch 879 batch loss 3.22634888 epoch total loss 3.25314522\n",
      "Trained batch 880 batch loss 3.36964488 epoch total loss 3.25327754\n",
      "Trained batch 881 batch loss 3.33442974 epoch total loss 3.25336981\n",
      "Trained batch 882 batch loss 3.02123833 epoch total loss 3.25310659\n",
      "Trained batch 883 batch loss 3.11108494 epoch total loss 3.25294566\n",
      "Trained batch 884 batch loss 3.03752089 epoch total loss 3.25270224\n",
      "Trained batch 885 batch loss 3.12714672 epoch total loss 3.25256038\n",
      "Trained batch 886 batch loss 3.05892706 epoch total loss 3.25234175\n",
      "Trained batch 887 batch loss 3.1025548 epoch total loss 3.25217271\n",
      "Trained batch 888 batch loss 3.13918233 epoch total loss 3.25204539\n",
      "Trained batch 889 batch loss 3.17283535 epoch total loss 3.25195646\n",
      "Trained batch 890 batch loss 3.26521254 epoch total loss 3.25197124\n",
      "Trained batch 891 batch loss 3.23837852 epoch total loss 3.25195575\n",
      "Trained batch 892 batch loss 3.24448824 epoch total loss 3.2519474\n",
      "Trained batch 893 batch loss 3.29020309 epoch total loss 3.25199032\n",
      "Trained batch 894 batch loss 3.47074223 epoch total loss 3.25223494\n",
      "Trained batch 895 batch loss 3.30205393 epoch total loss 3.25229049\n",
      "Trained batch 896 batch loss 3.53946948 epoch total loss 3.25261116\n",
      "Trained batch 897 batch loss 3.3214488 epoch total loss 3.25268793\n",
      "Trained batch 898 batch loss 3.50105953 epoch total loss 3.2529645\n",
      "Trained batch 899 batch loss 3.32175088 epoch total loss 3.25304103\n",
      "Trained batch 900 batch loss 3.34979486 epoch total loss 3.25314856\n",
      "Trained batch 901 batch loss 3.3773067 epoch total loss 3.25328636\n",
      "Trained batch 902 batch loss 3.38275909 epoch total loss 3.25343\n",
      "Trained batch 903 batch loss 3.42748451 epoch total loss 3.25362253\n",
      "Trained batch 904 batch loss 3.39155316 epoch total loss 3.25377536\n",
      "Trained batch 905 batch loss 3.41714811 epoch total loss 3.25395584\n",
      "Trained batch 906 batch loss 3.36350727 epoch total loss 3.25407672\n",
      "Trained batch 907 batch loss 3.36938691 epoch total loss 3.25420403\n",
      "Trained batch 908 batch loss 3.12372494 epoch total loss 3.25406027\n",
      "Trained batch 909 batch loss 3.12530327 epoch total loss 3.25391865\n",
      "Trained batch 910 batch loss 2.91364908 epoch total loss 3.25354457\n",
      "Trained batch 911 batch loss 3.20187473 epoch total loss 3.25348783\n",
      "Trained batch 912 batch loss 3.23001051 epoch total loss 3.25346208\n",
      "Trained batch 913 batch loss 3.11858726 epoch total loss 3.2533145\n",
      "Trained batch 914 batch loss 3.24846482 epoch total loss 3.25330925\n",
      "Trained batch 915 batch loss 3.21241713 epoch total loss 3.25326467\n",
      "Trained batch 916 batch loss 3.22830653 epoch total loss 3.25323725\n",
      "Trained batch 917 batch loss 2.75277853 epoch total loss 3.25269151\n",
      "Trained batch 918 batch loss 3.04908776 epoch total loss 3.25246954\n",
      "Trained batch 919 batch loss 2.90064263 epoch total loss 3.25208664\n",
      "Trained batch 920 batch loss 3.12614369 epoch total loss 3.25195\n",
      "Trained batch 921 batch loss 3.00751829 epoch total loss 3.25168467\n",
      "Trained batch 922 batch loss 3.17315173 epoch total loss 3.25159931\n",
      "Trained batch 923 batch loss 3.06374121 epoch total loss 3.2513957\n",
      "Trained batch 924 batch loss 3.00695848 epoch total loss 3.2511313\n",
      "Trained batch 925 batch loss 3.18932 epoch total loss 3.25106454\n",
      "Trained batch 926 batch loss 2.74673367 epoch total loss 3.25052\n",
      "Trained batch 927 batch loss 2.91011739 epoch total loss 3.25015283\n",
      "Trained batch 928 batch loss 3.24674129 epoch total loss 3.25014925\n",
      "Trained batch 929 batch loss 3.38182116 epoch total loss 3.25029087\n",
      "Trained batch 930 batch loss 2.95807219 epoch total loss 3.24997663\n",
      "Trained batch 931 batch loss 2.73981571 epoch total loss 3.24942851\n",
      "Trained batch 932 batch loss 3.14078689 epoch total loss 3.24931216\n",
      "Trained batch 933 batch loss 3.02496266 epoch total loss 3.2490716\n",
      "Trained batch 934 batch loss 3.10465097 epoch total loss 3.2489171\n",
      "Trained batch 935 batch loss 3.16301632 epoch total loss 3.24882531\n",
      "Trained batch 936 batch loss 3.24487758 epoch total loss 3.24882102\n",
      "Trained batch 937 batch loss 3.30412126 epoch total loss 3.24888015\n",
      "Trained batch 938 batch loss 3.21366286 epoch total loss 3.24884248\n",
      "Trained batch 939 batch loss 3.44068098 epoch total loss 3.2490468\n",
      "Trained batch 940 batch loss 3.20680976 epoch total loss 3.24900198\n",
      "Trained batch 941 batch loss 3.37117124 epoch total loss 3.24913168\n",
      "Trained batch 942 batch loss 3.19700766 epoch total loss 3.24907637\n",
      "Trained batch 943 batch loss 3.19385958 epoch total loss 3.24901772\n",
      "Trained batch 944 batch loss 3.07723 epoch total loss 3.24883556\n",
      "Trained batch 945 batch loss 2.49855423 epoch total loss 3.24804163\n",
      "Trained batch 946 batch loss 2.68891668 epoch total loss 3.24745083\n",
      "Trained batch 947 batch loss 2.67622542 epoch total loss 3.24684763\n",
      "Trained batch 948 batch loss 3.01338124 epoch total loss 3.24660134\n",
      "Trained batch 949 batch loss 3.25938559 epoch total loss 3.24661469\n",
      "Trained batch 950 batch loss 3.33136988 epoch total loss 3.24670386\n",
      "Trained batch 951 batch loss 3.45985937 epoch total loss 3.24692798\n",
      "Trained batch 952 batch loss 3.19917297 epoch total loss 3.24687791\n",
      "Trained batch 953 batch loss 3.08855844 epoch total loss 3.24671197\n",
      "Trained batch 954 batch loss 3.2382381 epoch total loss 3.24670315\n",
      "Trained batch 955 batch loss 3.16136265 epoch total loss 3.24661374\n",
      "Trained batch 956 batch loss 3.1887393 epoch total loss 3.24655318\n",
      "Trained batch 957 batch loss 3.14946866 epoch total loss 3.24645162\n",
      "Trained batch 958 batch loss 3.15531707 epoch total loss 3.24635649\n",
      "Trained batch 959 batch loss 2.99479985 epoch total loss 3.24609423\n",
      "Trained batch 960 batch loss 2.87193823 epoch total loss 3.24570441\n",
      "Trained batch 961 batch loss 3.11156201 epoch total loss 3.24556494\n",
      "Trained batch 962 batch loss 3.03986883 epoch total loss 3.24535084\n",
      "Trained batch 963 batch loss 3.04005432 epoch total loss 3.24513769\n",
      "Trained batch 964 batch loss 3.09679222 epoch total loss 3.24498367\n",
      "Trained batch 965 batch loss 2.91316938 epoch total loss 3.24463987\n",
      "Trained batch 966 batch loss 3.03978109 epoch total loss 3.24442768\n",
      "Trained batch 967 batch loss 3.25064135 epoch total loss 3.24443436\n",
      "Trained batch 968 batch loss 3.15333796 epoch total loss 3.24434018\n",
      "Trained batch 969 batch loss 3.22254801 epoch total loss 3.24431777\n",
      "Trained batch 970 batch loss 3.15499878 epoch total loss 3.24422574\n",
      "Trained batch 971 batch loss 3.17203 epoch total loss 3.24415135\n",
      "Trained batch 972 batch loss 3.08431697 epoch total loss 3.24398685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 973 batch loss 3.11037207 epoch total loss 3.24384952\n",
      "Trained batch 974 batch loss 3.06657863 epoch total loss 3.2436676\n",
      "Trained batch 975 batch loss 2.99519634 epoch total loss 3.24341273\n",
      "Trained batch 976 batch loss 2.99867606 epoch total loss 3.24316216\n",
      "Trained batch 977 batch loss 3.11477089 epoch total loss 3.24303055\n",
      "Trained batch 978 batch loss 3.08009601 epoch total loss 3.24286389\n",
      "Trained batch 979 batch loss 3.33916855 epoch total loss 3.24296236\n",
      "Trained batch 980 batch loss 3.37684679 epoch total loss 3.24309897\n",
      "Trained batch 981 batch loss 3.24800396 epoch total loss 3.24310398\n",
      "Trained batch 982 batch loss 3.06230545 epoch total loss 3.24292\n",
      "Trained batch 983 batch loss 3.08561468 epoch total loss 3.24276\n",
      "Trained batch 984 batch loss 2.97109962 epoch total loss 3.24248409\n",
      "Trained batch 985 batch loss 3.06129646 epoch total loss 3.2423\n",
      "Trained batch 986 batch loss 3.02316141 epoch total loss 3.24207783\n",
      "Trained batch 987 batch loss 3.03695107 epoch total loss 3.24187\n",
      "Trained batch 988 batch loss 3.17368984 epoch total loss 3.24180079\n",
      "Trained batch 989 batch loss 2.81779766 epoch total loss 3.24137211\n",
      "Trained batch 990 batch loss 2.78669739 epoch total loss 3.24091268\n",
      "Trained batch 991 batch loss 2.68562365 epoch total loss 3.24035239\n",
      "Trained batch 992 batch loss 3.03109646 epoch total loss 3.24014139\n",
      "Trained batch 993 batch loss 3.28511047 epoch total loss 3.24018669\n",
      "Trained batch 994 batch loss 3.18668962 epoch total loss 3.24013305\n",
      "Trained batch 995 batch loss 3.17673969 epoch total loss 3.24006915\n",
      "Trained batch 996 batch loss 3.34514236 epoch total loss 3.24017477\n",
      "Trained batch 997 batch loss 3.17266774 epoch total loss 3.24010706\n",
      "Trained batch 998 batch loss 3.07279968 epoch total loss 3.23993945\n",
      "Trained batch 999 batch loss 2.85142612 epoch total loss 3.23955035\n",
      "Trained batch 1000 batch loss 2.82746768 epoch total loss 3.23913813\n",
      "Trained batch 1001 batch loss 3.08475089 epoch total loss 3.23898387\n",
      "Trained batch 1002 batch loss 2.82411671 epoch total loss 3.23857\n",
      "Trained batch 1003 batch loss 2.84517932 epoch total loss 3.23817778\n",
      "Trained batch 1004 batch loss 3.1136086 epoch total loss 3.23805356\n",
      "Trained batch 1005 batch loss 3.16004705 epoch total loss 3.23797607\n",
      "Trained batch 1006 batch loss 3.05725098 epoch total loss 3.23779631\n",
      "Trained batch 1007 batch loss 2.82176018 epoch total loss 3.23738313\n",
      "Trained batch 1008 batch loss 2.9110322 epoch total loss 3.23705959\n",
      "Trained batch 1009 batch loss 3.09416246 epoch total loss 3.23691797\n",
      "Trained batch 1010 batch loss 2.85116553 epoch total loss 3.23653603\n",
      "Trained batch 1011 batch loss 2.94649982 epoch total loss 3.23624921\n",
      "Trained batch 1012 batch loss 3.03297 epoch total loss 3.23604822\n",
      "Trained batch 1013 batch loss 3.50633335 epoch total loss 3.23631501\n",
      "Trained batch 1014 batch loss 3.25196242 epoch total loss 3.23633051\n",
      "Trained batch 1015 batch loss 3.37813234 epoch total loss 3.23647022\n",
      "Trained batch 1016 batch loss 3.51272941 epoch total loss 3.23674226\n",
      "Trained batch 1017 batch loss 3.1882081 epoch total loss 3.23669457\n",
      "Trained batch 1018 batch loss 3.6065464 epoch total loss 3.23705769\n",
      "Trained batch 1019 batch loss 3.27579212 epoch total loss 3.23709583\n",
      "Trained batch 1020 batch loss 3.27096653 epoch total loss 3.23712897\n",
      "Trained batch 1021 batch loss 3.26073575 epoch total loss 3.2371521\n",
      "Trained batch 1022 batch loss 3.03058362 epoch total loss 3.23695\n",
      "Trained batch 1023 batch loss 3.07831573 epoch total loss 3.23679495\n",
      "Trained batch 1024 batch loss 3.35560489 epoch total loss 3.23691106\n",
      "Trained batch 1025 batch loss 3.08697963 epoch total loss 3.23676467\n",
      "Trained batch 1026 batch loss 2.97023606 epoch total loss 3.23650503\n",
      "Trained batch 1027 batch loss 3.21857738 epoch total loss 3.23648739\n",
      "Trained batch 1028 batch loss 3.10224319 epoch total loss 3.23635697\n",
      "Trained batch 1029 batch loss 3.05789328 epoch total loss 3.2361834\n",
      "Trained batch 1030 batch loss 3.16898823 epoch total loss 3.23611808\n",
      "Trained batch 1031 batch loss 3.18371511 epoch total loss 3.23606706\n",
      "Trained batch 1032 batch loss 3.10562325 epoch total loss 3.23594093\n",
      "Trained batch 1033 batch loss 3.14966941 epoch total loss 3.23585725\n",
      "Trained batch 1034 batch loss 3.0294323 epoch total loss 3.23565769\n",
      "Trained batch 1035 batch loss 3.27410579 epoch total loss 3.23569512\n",
      "Trained batch 1036 batch loss 3.38575983 epoch total loss 3.23583984\n",
      "Trained batch 1037 batch loss 3.25048923 epoch total loss 3.23585391\n",
      "Trained batch 1038 batch loss 3.32317114 epoch total loss 3.23593807\n",
      "Trained batch 1039 batch loss 3.1305027 epoch total loss 3.23583674\n",
      "Trained batch 1040 batch loss 2.93012309 epoch total loss 3.23554277\n",
      "Trained batch 1041 batch loss 2.92947054 epoch total loss 3.2352488\n",
      "Trained batch 1042 batch loss 2.78201532 epoch total loss 3.23481393\n",
      "Trained batch 1043 batch loss 3.2034831 epoch total loss 3.23478365\n",
      "Trained batch 1044 batch loss 3.01995039 epoch total loss 3.23457789\n",
      "Trained batch 1045 batch loss 2.99056292 epoch total loss 3.23434448\n",
      "Trained batch 1046 batch loss 3.16697168 epoch total loss 3.23428\n",
      "Trained batch 1047 batch loss 2.91363144 epoch total loss 3.23397374\n",
      "Trained batch 1048 batch loss 3.14112926 epoch total loss 3.23388505\n",
      "Trained batch 1049 batch loss 3.1346302 epoch total loss 3.2337904\n",
      "Trained batch 1050 batch loss 3.21386027 epoch total loss 3.23377132\n",
      "Trained batch 1051 batch loss 3.03024316 epoch total loss 3.23357773\n",
      "Trained batch 1052 batch loss 3.12982225 epoch total loss 3.23347926\n",
      "Trained batch 1053 batch loss 3.12111068 epoch total loss 3.23337245\n",
      "Trained batch 1054 batch loss 3.01622248 epoch total loss 3.23316646\n",
      "Trained batch 1055 batch loss 3.17645645 epoch total loss 3.23311257\n",
      "Trained batch 1056 batch loss 3.14288545 epoch total loss 3.23302722\n",
      "Trained batch 1057 batch loss 3.38804626 epoch total loss 3.23317361\n",
      "Trained batch 1058 batch loss 3.15633345 epoch total loss 3.23310089\n",
      "Trained batch 1059 batch loss 2.95041537 epoch total loss 3.2328341\n",
      "Trained batch 1060 batch loss 2.79691648 epoch total loss 3.23242283\n",
      "Trained batch 1061 batch loss 2.81361389 epoch total loss 3.23202825\n",
      "Trained batch 1062 batch loss 3.62275934 epoch total loss 3.23239613\n",
      "Trained batch 1063 batch loss 3.12099195 epoch total loss 3.23229146\n",
      "Trained batch 1064 batch loss 3.23876643 epoch total loss 3.23229742\n",
      "Trained batch 1065 batch loss 3.09051824 epoch total loss 3.23216438\n",
      "Trained batch 1066 batch loss 3.25049543 epoch total loss 3.23218155\n",
      "Trained batch 1067 batch loss 3.11281729 epoch total loss 3.23206973\n",
      "Trained batch 1068 batch loss 2.91950464 epoch total loss 3.23177695\n",
      "Trained batch 1069 batch loss 2.84113932 epoch total loss 3.23141146\n",
      "Trained batch 1070 batch loss 2.80890536 epoch total loss 3.23101664\n",
      "Trained batch 1071 batch loss 3.12270927 epoch total loss 3.23091555\n",
      "Trained batch 1072 batch loss 3.04929781 epoch total loss 3.23074603\n",
      "Trained batch 1073 batch loss 2.77504253 epoch total loss 3.23032165\n",
      "Trained batch 1074 batch loss 3.00802684 epoch total loss 3.2301147\n",
      "Trained batch 1075 batch loss 3.11717844 epoch total loss 3.23000956\n",
      "Trained batch 1076 batch loss 3.09760189 epoch total loss 3.22988653\n",
      "Trained batch 1077 batch loss 3.05922079 epoch total loss 3.22972822\n",
      "Trained batch 1078 batch loss 2.98177671 epoch total loss 3.22949815\n",
      "Trained batch 1079 batch loss 3.16188526 epoch total loss 3.22943544\n",
      "Trained batch 1080 batch loss 3.16260171 epoch total loss 3.22937346\n",
      "Trained batch 1081 batch loss 2.76959085 epoch total loss 3.22894812\n",
      "Trained batch 1082 batch loss 2.72558284 epoch total loss 3.22848296\n",
      "Trained batch 1083 batch loss 2.91235423 epoch total loss 3.2281909\n",
      "Trained batch 1084 batch loss 3.0024395 epoch total loss 3.22798276\n",
      "Trained batch 1085 batch loss 2.97554088 epoch total loss 3.22775\n",
      "Trained batch 1086 batch loss 2.94045782 epoch total loss 3.22748566\n",
      "Trained batch 1087 batch loss 2.88005757 epoch total loss 3.22716594\n",
      "Trained batch 1088 batch loss 3.11332941 epoch total loss 3.22706127\n",
      "Trained batch 1089 batch loss 2.81072664 epoch total loss 3.22667909\n",
      "Trained batch 1090 batch loss 2.44250655 epoch total loss 3.22595978\n",
      "Trained batch 1091 batch loss 2.91455889 epoch total loss 3.22567439\n",
      "Trained batch 1092 batch loss 3.20966411 epoch total loss 3.22565961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1093 batch loss 3.10514164 epoch total loss 3.22554946\n",
      "Trained batch 1094 batch loss 3.10561657 epoch total loss 3.22544\n",
      "Trained batch 1095 batch loss 3.34464407 epoch total loss 3.22554898\n",
      "Trained batch 1096 batch loss 2.95460796 epoch total loss 3.22530174\n",
      "Trained batch 1097 batch loss 2.80874467 epoch total loss 3.22492218\n",
      "Trained batch 1098 batch loss 2.98877573 epoch total loss 3.22470689\n",
      "Trained batch 1099 batch loss 3.14905024 epoch total loss 3.22463822\n",
      "Trained batch 1100 batch loss 3.11492848 epoch total loss 3.22453856\n",
      "Trained batch 1101 batch loss 2.99510622 epoch total loss 3.22433019\n",
      "Trained batch 1102 batch loss 3.1049974 epoch total loss 3.22422194\n",
      "Trained batch 1103 batch loss 3.0489068 epoch total loss 3.22406292\n",
      "Trained batch 1104 batch loss 3.08587122 epoch total loss 3.22393775\n",
      "Trained batch 1105 batch loss 3.06745434 epoch total loss 3.22379613\n",
      "Trained batch 1106 batch loss 3.09015846 epoch total loss 3.22367525\n",
      "Trained batch 1107 batch loss 3.13151193 epoch total loss 3.22359204\n",
      "Trained batch 1108 batch loss 3.15061355 epoch total loss 3.22352624\n",
      "Trained batch 1109 batch loss 3.14112425 epoch total loss 3.22345185\n",
      "Trained batch 1110 batch loss 2.91084456 epoch total loss 3.22317028\n",
      "Trained batch 1111 batch loss 2.45819974 epoch total loss 3.22248173\n",
      "Trained batch 1112 batch loss 2.74705672 epoch total loss 3.22205424\n",
      "Trained batch 1113 batch loss 3.40709281 epoch total loss 3.22222042\n",
      "Trained batch 1114 batch loss 3.03226709 epoch total loss 3.22205\n",
      "Trained batch 1115 batch loss 3.13033819 epoch total loss 3.2219677\n",
      "Trained batch 1116 batch loss 3.03091264 epoch total loss 3.22179651\n",
      "Trained batch 1117 batch loss 3.11492157 epoch total loss 3.22170091\n",
      "Trained batch 1118 batch loss 2.97986794 epoch total loss 3.22148466\n",
      "Trained batch 1119 batch loss 3.23670816 epoch total loss 3.22149849\n",
      "Trained batch 1120 batch loss 2.92275476 epoch total loss 3.2212317\n",
      "Trained batch 1121 batch loss 3.05014086 epoch total loss 3.22107911\n",
      "Trained batch 1122 batch loss 3.13898802 epoch total loss 3.22100592\n",
      "Trained batch 1123 batch loss 3.0811069 epoch total loss 3.22088122\n",
      "Trained batch 1124 batch loss 2.87019253 epoch total loss 3.22056913\n",
      "Trained batch 1125 batch loss 3.18309927 epoch total loss 3.22053576\n",
      "Trained batch 1126 batch loss 3.13749242 epoch total loss 3.22046208\n",
      "Trained batch 1127 batch loss 3.29240727 epoch total loss 3.22052598\n",
      "Trained batch 1128 batch loss 3.01501894 epoch total loss 3.22034383\n",
      "Trained batch 1129 batch loss 3.07809711 epoch total loss 3.22021794\n",
      "Trained batch 1130 batch loss 3.07421017 epoch total loss 3.22008872\n",
      "Trained batch 1131 batch loss 3.23724675 epoch total loss 3.22010398\n",
      "Trained batch 1132 batch loss 2.9377346 epoch total loss 3.21985435\n",
      "Trained batch 1133 batch loss 3.15651131 epoch total loss 3.21979856\n",
      "Trained batch 1134 batch loss 2.91876483 epoch total loss 3.21953297\n",
      "Trained batch 1135 batch loss 3.25277185 epoch total loss 3.21956229\n",
      "Trained batch 1136 batch loss 3.00637865 epoch total loss 3.21937442\n",
      "Trained batch 1137 batch loss 2.89917326 epoch total loss 3.21909285\n",
      "Trained batch 1138 batch loss 3.29267287 epoch total loss 3.2191577\n",
      "Trained batch 1139 batch loss 3.09722137 epoch total loss 3.21905041\n",
      "Trained batch 1140 batch loss 3.04786205 epoch total loss 3.21890044\n",
      "Trained batch 1141 batch loss 3.32240987 epoch total loss 3.21899104\n",
      "Trained batch 1142 batch loss 3.43011522 epoch total loss 3.21917605\n",
      "Trained batch 1143 batch loss 2.89924622 epoch total loss 3.21889615\n",
      "Trained batch 1144 batch loss 3.25248909 epoch total loss 3.21892548\n",
      "Trained batch 1145 batch loss 3.07808971 epoch total loss 3.21880245\n",
      "Trained batch 1146 batch loss 2.7801466 epoch total loss 3.21841955\n",
      "Trained batch 1147 batch loss 3.00737524 epoch total loss 3.21823549\n",
      "Trained batch 1148 batch loss 3.1697855 epoch total loss 3.21819329\n",
      "Trained batch 1149 batch loss 3.09387016 epoch total loss 3.21808505\n",
      "Trained batch 1150 batch loss 2.93651414 epoch total loss 3.21784019\n",
      "Trained batch 1151 batch loss 2.96173215 epoch total loss 3.21761751\n",
      "Trained batch 1152 batch loss 3.11285043 epoch total loss 3.21752644\n",
      "Trained batch 1153 batch loss 3.0412786 epoch total loss 3.21737361\n",
      "Trained batch 1154 batch loss 3.32644892 epoch total loss 3.21746826\n",
      "Trained batch 1155 batch loss 3.20819402 epoch total loss 3.21746016\n",
      "Trained batch 1156 batch loss 3.2892549 epoch total loss 3.21752238\n",
      "Trained batch 1157 batch loss 3.31099463 epoch total loss 3.21760321\n",
      "Trained batch 1158 batch loss 3.24092579 epoch total loss 3.21762323\n",
      "Trained batch 1159 batch loss 3.16494417 epoch total loss 3.21757793\n",
      "Trained batch 1160 batch loss 3.17241073 epoch total loss 3.21753907\n",
      "Trained batch 1161 batch loss 3.11942458 epoch total loss 3.21745443\n",
      "Trained batch 1162 batch loss 3.2214694 epoch total loss 3.21745777\n",
      "Trained batch 1163 batch loss 3.25674725 epoch total loss 3.21749163\n",
      "Trained batch 1164 batch loss 3.02833939 epoch total loss 3.21732926\n",
      "Trained batch 1165 batch loss 3.23345923 epoch total loss 3.21734309\n",
      "Trained batch 1166 batch loss 3.07440734 epoch total loss 3.21722054\n",
      "Trained batch 1167 batch loss 3.08748364 epoch total loss 3.2171092\n",
      "Trained batch 1168 batch loss 3.20251703 epoch total loss 3.21709681\n",
      "Trained batch 1169 batch loss 2.87456942 epoch total loss 3.21680379\n",
      "Trained batch 1170 batch loss 3.18179202 epoch total loss 3.21677399\n",
      "Trained batch 1171 batch loss 3.10577822 epoch total loss 3.2166791\n",
      "Trained batch 1172 batch loss 3.09340405 epoch total loss 3.21657395\n",
      "Trained batch 1173 batch loss 3.00251436 epoch total loss 3.21639132\n",
      "Trained batch 1174 batch loss 2.92006207 epoch total loss 3.21613908\n",
      "Trained batch 1175 batch loss 3.18331289 epoch total loss 3.21611118\n",
      "Trained batch 1176 batch loss 3.11685705 epoch total loss 3.21602678\n",
      "Trained batch 1177 batch loss 3.12647486 epoch total loss 3.21595073\n",
      "Trained batch 1178 batch loss 3.17246532 epoch total loss 3.21591377\n",
      "Trained batch 1179 batch loss 3.00105762 epoch total loss 3.21573138\n",
      "Trained batch 1180 batch loss 3.4116416 epoch total loss 3.21589756\n",
      "Trained batch 1181 batch loss 3.40938902 epoch total loss 3.21606135\n",
      "Trained batch 1182 batch loss 3.4461422 epoch total loss 3.2162559\n",
      "Trained batch 1183 batch loss 3.29388952 epoch total loss 3.21632171\n",
      "Trained batch 1184 batch loss 3.25134706 epoch total loss 3.21635127\n",
      "Trained batch 1185 batch loss 3.37501 epoch total loss 3.21648526\n",
      "Trained batch 1186 batch loss 3.34763455 epoch total loss 3.21659565\n",
      "Trained batch 1187 batch loss 3.13756704 epoch total loss 3.21652913\n",
      "Trained batch 1188 batch loss 3.16809058 epoch total loss 3.21648812\n",
      "Trained batch 1189 batch loss 2.91111231 epoch total loss 3.21623135\n",
      "Trained batch 1190 batch loss 3.06143284 epoch total loss 3.21610141\n",
      "Trained batch 1191 batch loss 3.05835366 epoch total loss 3.21596885\n",
      "Trained batch 1192 batch loss 3.02364826 epoch total loss 3.21580768\n",
      "Trained batch 1193 batch loss 2.87580681 epoch total loss 3.21552253\n",
      "Trained batch 1194 batch loss 3.17066455 epoch total loss 3.2154851\n",
      "Trained batch 1195 batch loss 3.24095941 epoch total loss 3.21550632\n",
      "Trained batch 1196 batch loss 3.22536755 epoch total loss 3.21551442\n",
      "Trained batch 1197 batch loss 3.31601572 epoch total loss 3.21559834\n",
      "Trained batch 1198 batch loss 2.95498753 epoch total loss 3.21538091\n",
      "Trained batch 1199 batch loss 3.02604032 epoch total loss 3.21522307\n",
      "Trained batch 1200 batch loss 2.86458445 epoch total loss 3.21493077\n",
      "Trained batch 1201 batch loss 3.08724833 epoch total loss 3.21482444\n",
      "Trained batch 1202 batch loss 2.97884321 epoch total loss 3.21462798\n",
      "Trained batch 1203 batch loss 2.91726685 epoch total loss 3.21438074\n",
      "Trained batch 1204 batch loss 2.95718241 epoch total loss 3.21416736\n",
      "Trained batch 1205 batch loss 2.81932282 epoch total loss 3.21383953\n",
      "Trained batch 1206 batch loss 2.99437 epoch total loss 3.21365762\n",
      "Trained batch 1207 batch loss 3.06517744 epoch total loss 3.21353459\n",
      "Trained batch 1208 batch loss 3.35374379 epoch total loss 3.2136507\n",
      "Trained batch 1209 batch loss 3.47310877 epoch total loss 3.21386528\n",
      "Trained batch 1210 batch loss 3.45597792 epoch total loss 3.21406555\n",
      "Trained batch 1211 batch loss 3.33993769 epoch total loss 3.2141695\n",
      "Trained batch 1212 batch loss 3.48524117 epoch total loss 3.21439314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1213 batch loss 3.2513454 epoch total loss 3.21442366\n",
      "Trained batch 1214 batch loss 3.11301899 epoch total loss 3.21434021\n",
      "Trained batch 1215 batch loss 2.97467518 epoch total loss 3.2141428\n",
      "Trained batch 1216 batch loss 3.23696971 epoch total loss 3.21416163\n",
      "Trained batch 1217 batch loss 3.40445352 epoch total loss 3.21431804\n",
      "Trained batch 1218 batch loss 3.49581099 epoch total loss 3.2145493\n",
      "Trained batch 1219 batch loss 3.48548985 epoch total loss 3.21477175\n",
      "Trained batch 1220 batch loss 3.15564585 epoch total loss 3.21472335\n",
      "Trained batch 1221 batch loss 2.84933448 epoch total loss 3.21442413\n",
      "Trained batch 1222 batch loss 3.05975747 epoch total loss 3.21429753\n",
      "Trained batch 1223 batch loss 3.00798035 epoch total loss 3.21412897\n",
      "Trained batch 1224 batch loss 3.05053258 epoch total loss 3.21399522\n",
      "Trained batch 1225 batch loss 2.96415281 epoch total loss 3.21379137\n",
      "Trained batch 1226 batch loss 2.9917345 epoch total loss 3.21361017\n",
      "Trained batch 1227 batch loss 3.19133782 epoch total loss 3.21359205\n",
      "Trained batch 1228 batch loss 3.35847664 epoch total loss 3.21370983\n",
      "Trained batch 1229 batch loss 3.23157215 epoch total loss 3.21372461\n",
      "Trained batch 1230 batch loss 3.2980113 epoch total loss 3.21379304\n",
      "Trained batch 1231 batch loss 3.23908758 epoch total loss 3.21381354\n",
      "Trained batch 1232 batch loss 3.46087098 epoch total loss 3.21401429\n",
      "Trained batch 1233 batch loss 3.41753197 epoch total loss 3.21417928\n",
      "Trained batch 1234 batch loss 3.17794609 epoch total loss 3.21415\n",
      "Trained batch 1235 batch loss 2.65671468 epoch total loss 3.21369863\n",
      "Trained batch 1236 batch loss 2.95525599 epoch total loss 3.21348953\n",
      "Trained batch 1237 batch loss 2.84098101 epoch total loss 3.21318841\n",
      "Trained batch 1238 batch loss 2.82644916 epoch total loss 3.21287608\n",
      "Trained batch 1239 batch loss 2.95396519 epoch total loss 3.21266699\n",
      "Trained batch 1240 batch loss 3.01542234 epoch total loss 3.21250796\n",
      "Trained batch 1241 batch loss 3.1689477 epoch total loss 3.21247268\n",
      "Trained batch 1242 batch loss 2.99234605 epoch total loss 3.21229553\n",
      "Trained batch 1243 batch loss 2.73541069 epoch total loss 3.21191192\n",
      "Trained batch 1244 batch loss 3.00829768 epoch total loss 3.21174812\n",
      "Trained batch 1245 batch loss 2.99541807 epoch total loss 3.21157432\n",
      "Trained batch 1246 batch loss 3.01207209 epoch total loss 3.21141434\n",
      "Trained batch 1247 batch loss 3.06327391 epoch total loss 3.21129537\n",
      "Trained batch 1248 batch loss 3.02018619 epoch total loss 3.2111423\n",
      "Trained batch 1249 batch loss 2.99506235 epoch total loss 3.21096945\n",
      "Trained batch 1250 batch loss 2.99184322 epoch total loss 3.21079421\n",
      "Trained batch 1251 batch loss 2.78658748 epoch total loss 3.21045518\n",
      "Trained batch 1252 batch loss 2.92850399 epoch total loss 3.21022987\n",
      "Trained batch 1253 batch loss 2.85806704 epoch total loss 3.20994878\n",
      "Trained batch 1254 batch loss 2.88064098 epoch total loss 3.20968628\n",
      "Trained batch 1255 batch loss 2.99503398 epoch total loss 3.20951533\n",
      "Trained batch 1256 batch loss 3.13639641 epoch total loss 3.20945716\n",
      "Trained batch 1257 batch loss 3.1479044 epoch total loss 3.20940828\n",
      "Trained batch 1258 batch loss 3.00499821 epoch total loss 3.20924568\n",
      "Trained batch 1259 batch loss 2.9231317 epoch total loss 3.20901823\n",
      "Trained batch 1260 batch loss 2.8789835 epoch total loss 3.20875621\n",
      "Trained batch 1261 batch loss 2.92643404 epoch total loss 3.20853257\n",
      "Trained batch 1262 batch loss 2.81741667 epoch total loss 3.20822263\n",
      "Trained batch 1263 batch loss 2.78682137 epoch total loss 3.20788884\n",
      "Trained batch 1264 batch loss 2.94435906 epoch total loss 3.20768046\n",
      "Trained batch 1265 batch loss 2.88759398 epoch total loss 3.2074275\n",
      "Trained batch 1266 batch loss 2.91565847 epoch total loss 3.20719719\n",
      "Trained batch 1267 batch loss 2.87215018 epoch total loss 3.20693254\n",
      "Trained batch 1268 batch loss 2.9424541 epoch total loss 3.20672393\n",
      "Trained batch 1269 batch loss 2.77628398 epoch total loss 3.2063849\n",
      "Trained batch 1270 batch loss 2.81250405 epoch total loss 3.20607471\n",
      "Trained batch 1271 batch loss 3.06932759 epoch total loss 3.20596719\n",
      "Trained batch 1272 batch loss 3.1507318 epoch total loss 3.20592356\n",
      "Trained batch 1273 batch loss 3.09189558 epoch total loss 3.20583391\n",
      "Trained batch 1274 batch loss 2.95240569 epoch total loss 3.20563507\n",
      "Trained batch 1275 batch loss 3.29343534 epoch total loss 3.20570397\n",
      "Trained batch 1276 batch loss 3.16262317 epoch total loss 3.20567\n",
      "Trained batch 1277 batch loss 2.93476677 epoch total loss 3.20545793\n",
      "Trained batch 1278 batch loss 3.13914633 epoch total loss 3.20540595\n",
      "Trained batch 1279 batch loss 2.77036381 epoch total loss 3.20506597\n",
      "Trained batch 1280 batch loss 2.65793228 epoch total loss 3.20463824\n",
      "Trained batch 1281 batch loss 2.81939125 epoch total loss 3.2043376\n",
      "Trained batch 1282 batch loss 2.64832711 epoch total loss 3.20390391\n",
      "Trained batch 1283 batch loss 3.00849771 epoch total loss 3.20375133\n",
      "Trained batch 1284 batch loss 2.98817921 epoch total loss 3.20358372\n",
      "Trained batch 1285 batch loss 2.97118139 epoch total loss 3.20340276\n",
      "Trained batch 1286 batch loss 2.9256711 epoch total loss 3.20318699\n",
      "Trained batch 1287 batch loss 3.12348127 epoch total loss 3.203125\n",
      "Trained batch 1288 batch loss 3.27267623 epoch total loss 3.20317888\n",
      "Trained batch 1289 batch loss 3.23902082 epoch total loss 3.20320678\n",
      "Trained batch 1290 batch loss 2.96724081 epoch total loss 3.20302391\n",
      "Trained batch 1291 batch loss 2.9162631 epoch total loss 3.20280194\n",
      "Trained batch 1292 batch loss 2.87792635 epoch total loss 3.20255065\n",
      "Trained batch 1293 batch loss 3.1284647 epoch total loss 3.20249319\n",
      "Trained batch 1294 batch loss 3.06519938 epoch total loss 3.20238733\n",
      "Trained batch 1295 batch loss 3.09284091 epoch total loss 3.20230269\n",
      "Trained batch 1296 batch loss 2.8696928 epoch total loss 3.20204592\n",
      "Trained batch 1297 batch loss 3.08041978 epoch total loss 3.20195222\n",
      "Trained batch 1298 batch loss 3.01203966 epoch total loss 3.20180607\n",
      "Trained batch 1299 batch loss 2.95574903 epoch total loss 3.20161653\n",
      "Trained batch 1300 batch loss 3.01340055 epoch total loss 3.20147157\n",
      "Trained batch 1301 batch loss 2.82953739 epoch total loss 3.2011857\n",
      "Trained batch 1302 batch loss 2.86869287 epoch total loss 3.20093036\n",
      "Trained batch 1303 batch loss 2.98908305 epoch total loss 3.20076799\n",
      "Trained batch 1304 batch loss 2.79814482 epoch total loss 3.20045924\n",
      "Trained batch 1305 batch loss 3.39345884 epoch total loss 3.2006073\n",
      "Trained batch 1306 batch loss 3.05733156 epoch total loss 3.20049739\n",
      "Trained batch 1307 batch loss 2.82574606 epoch total loss 3.20021057\n",
      "Trained batch 1308 batch loss 2.79595566 epoch total loss 3.19990158\n",
      "Trained batch 1309 batch loss 2.68869853 epoch total loss 3.19951081\n",
      "Trained batch 1310 batch loss 2.71722889 epoch total loss 3.19914269\n",
      "Trained batch 1311 batch loss 2.96170473 epoch total loss 3.19896173\n",
      "Trained batch 1312 batch loss 2.98334289 epoch total loss 3.19879746\n",
      "Trained batch 1313 batch loss 3.07573843 epoch total loss 3.19870377\n",
      "Trained batch 1314 batch loss 2.83701825 epoch total loss 3.19842839\n",
      "Trained batch 1315 batch loss 2.98768663 epoch total loss 3.19826818\n",
      "Trained batch 1316 batch loss 2.93515348 epoch total loss 3.19806814\n",
      "Trained batch 1317 batch loss 3.1156776 epoch total loss 3.19800568\n",
      "Trained batch 1318 batch loss 3.27708316 epoch total loss 3.19806552\n",
      "Trained batch 1319 batch loss 3.28275156 epoch total loss 3.19812965\n",
      "Trained batch 1320 batch loss 3.00439501 epoch total loss 3.19798279\n",
      "Trained batch 1321 batch loss 3.04160142 epoch total loss 3.19786453\n",
      "Trained batch 1322 batch loss 2.92318392 epoch total loss 3.19765687\n",
      "Trained batch 1323 batch loss 3.23227 epoch total loss 3.1976831\n",
      "Trained batch 1324 batch loss 3.20646238 epoch total loss 3.19768977\n",
      "Trained batch 1325 batch loss 3.17076826 epoch total loss 3.19766951\n",
      "Trained batch 1326 batch loss 3.2055068 epoch total loss 3.19767547\n",
      "Trained batch 1327 batch loss 3.1014173 epoch total loss 3.19760299\n",
      "Trained batch 1328 batch loss 2.91277647 epoch total loss 3.19738841\n",
      "Trained batch 1329 batch loss 2.99575 epoch total loss 3.19723654\n",
      "Trained batch 1330 batch loss 2.86968017 epoch total loss 3.19699025\n",
      "Trained batch 1331 batch loss 2.93323493 epoch total loss 3.19679213\n",
      "Trained batch 1332 batch loss 3.13092756 epoch total loss 3.19674253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1333 batch loss 3.0392735 epoch total loss 3.19662428\n",
      "Trained batch 1334 batch loss 3.55537963 epoch total loss 3.19689298\n",
      "Trained batch 1335 batch loss 2.9957 epoch total loss 3.1967423\n",
      "Trained batch 1336 batch loss 3.06589079 epoch total loss 3.19664431\n",
      "Trained batch 1337 batch loss 2.83453584 epoch total loss 3.19637346\n",
      "Trained batch 1338 batch loss 2.9703567 epoch total loss 3.19620442\n",
      "Trained batch 1339 batch loss 2.81728935 epoch total loss 3.19592142\n",
      "Trained batch 1340 batch loss 2.83925962 epoch total loss 3.19565535\n",
      "Trained batch 1341 batch loss 2.85868788 epoch total loss 3.19540429\n",
      "Trained batch 1342 batch loss 2.92336059 epoch total loss 3.19520164\n",
      "Trained batch 1343 batch loss 2.89523196 epoch total loss 3.194978\n",
      "Trained batch 1344 batch loss 2.77447152 epoch total loss 3.19466519\n",
      "Trained batch 1345 batch loss 2.87277365 epoch total loss 3.19442558\n",
      "Trained batch 1346 batch loss 2.92572021 epoch total loss 3.19422603\n",
      "Trained batch 1347 batch loss 3.11394024 epoch total loss 3.19416618\n",
      "Trained batch 1348 batch loss 2.86370277 epoch total loss 3.19392109\n",
      "Trained batch 1349 batch loss 2.93852043 epoch total loss 3.19373178\n",
      "Trained batch 1350 batch loss 2.86253786 epoch total loss 3.19348621\n",
      "Trained batch 1351 batch loss 2.86796808 epoch total loss 3.19324541\n",
      "Trained batch 1352 batch loss 2.83407164 epoch total loss 3.19297981\n",
      "Trained batch 1353 batch loss 2.78547072 epoch total loss 3.19267869\n",
      "Trained batch 1354 batch loss 3.11649704 epoch total loss 3.19262266\n",
      "Trained batch 1355 batch loss 3.61608672 epoch total loss 3.19293523\n",
      "Trained batch 1356 batch loss 3.14806294 epoch total loss 3.19290209\n",
      "Trained batch 1357 batch loss 2.76424599 epoch total loss 3.19258618\n",
      "Trained batch 1358 batch loss 2.90885353 epoch total loss 3.19237709\n",
      "Trained batch 1359 batch loss 2.69010735 epoch total loss 3.1920073\n",
      "Trained batch 1360 batch loss 2.79862046 epoch total loss 3.19171834\n",
      "Trained batch 1361 batch loss 2.92831802 epoch total loss 3.19152474\n",
      "Trained batch 1362 batch loss 2.79827309 epoch total loss 3.19123602\n",
      "Trained batch 1363 batch loss 3.02390957 epoch total loss 3.19111323\n",
      "Trained batch 1364 batch loss 3.07426238 epoch total loss 3.1910274\n",
      "Trained batch 1365 batch loss 2.90403914 epoch total loss 3.19081712\n",
      "Trained batch 1366 batch loss 3.27969599 epoch total loss 3.19088221\n",
      "Trained batch 1367 batch loss 3.37810969 epoch total loss 3.19101906\n",
      "Trained batch 1368 batch loss 2.88664103 epoch total loss 3.19079661\n",
      "Trained batch 1369 batch loss 3.16189432 epoch total loss 3.19077563\n",
      "Trained batch 1370 batch loss 2.92015862 epoch total loss 3.19057798\n",
      "Trained batch 1371 batch loss 2.99421 epoch total loss 3.19043469\n",
      "Trained batch 1372 batch loss 3.0246489 epoch total loss 3.19031358\n",
      "Trained batch 1373 batch loss 2.97038889 epoch total loss 3.19015336\n",
      "Trained batch 1374 batch loss 2.95567608 epoch total loss 3.18998265\n",
      "Trained batch 1375 batch loss 2.97979259 epoch total loss 3.18982983\n",
      "Trained batch 1376 batch loss 2.9038868 epoch total loss 3.18962193\n",
      "Trained batch 1377 batch loss 2.73068619 epoch total loss 3.18928862\n",
      "Trained batch 1378 batch loss 2.89784193 epoch total loss 3.18907714\n",
      "Trained batch 1379 batch loss 2.9382472 epoch total loss 3.18889546\n",
      "Trained batch 1380 batch loss 3.08821654 epoch total loss 3.18882251\n",
      "Trained batch 1381 batch loss 3.14396477 epoch total loss 3.18879\n",
      "Trained batch 1382 batch loss 3.59184647 epoch total loss 3.18908191\n",
      "Trained batch 1383 batch loss 3.31109476 epoch total loss 3.18917\n",
      "Trained batch 1384 batch loss 3.11212015 epoch total loss 3.18911433\n",
      "Trained batch 1385 batch loss 3.17399693 epoch total loss 3.18910336\n",
      "Trained batch 1386 batch loss 2.65645027 epoch total loss 3.18871903\n",
      "Trained batch 1387 batch loss 2.95572686 epoch total loss 3.18855095\n",
      "Trained batch 1388 batch loss 3.04364061 epoch total loss 3.18844628\n",
      "Trained batch 1389 batch loss 2.81801891 epoch total loss 3.18817949\n",
      "Trained batch 1390 batch loss 2.7758975 epoch total loss 3.1878829\n",
      "Trained batch 1391 batch loss 2.97294712 epoch total loss 3.1877284\n",
      "Trained batch 1392 batch loss 3.06283045 epoch total loss 3.187639\n",
      "Trained batch 1393 batch loss 3.19272447 epoch total loss 3.18764257\n",
      "Trained batch 1394 batch loss 3.00800705 epoch total loss 3.18751359\n",
      "Trained batch 1395 batch loss 3.02221012 epoch total loss 3.1873951\n",
      "Trained batch 1396 batch loss 3.28209686 epoch total loss 3.18746281\n",
      "Trained batch 1397 batch loss 2.98623061 epoch total loss 3.18731904\n",
      "Trained batch 1398 batch loss 3.19154716 epoch total loss 3.1873219\n",
      "Trained batch 1399 batch loss 3.06756 epoch total loss 3.18723607\n",
      "Trained batch 1400 batch loss 3.2397809 epoch total loss 3.18727374\n",
      "Trained batch 1401 batch loss 3.07838488 epoch total loss 3.18719602\n",
      "Trained batch 1402 batch loss 3.0592804 epoch total loss 3.1871047\n",
      "Trained batch 1403 batch loss 2.91562366 epoch total loss 3.18691111\n",
      "Trained batch 1404 batch loss 3.1514802 epoch total loss 3.18688583\n",
      "Trained batch 1405 batch loss 2.98559022 epoch total loss 3.18674231\n",
      "Trained batch 1406 batch loss 3.07015657 epoch total loss 3.18665957\n",
      "Trained batch 1407 batch loss 3.04346681 epoch total loss 3.18655777\n",
      "Trained batch 1408 batch loss 3.18109226 epoch total loss 3.18655396\n",
      "Trained batch 1409 batch loss 3.15596247 epoch total loss 3.18653202\n",
      "Trained batch 1410 batch loss 2.96614838 epoch total loss 3.18637586\n",
      "Trained batch 1411 batch loss 3.18333316 epoch total loss 3.18637371\n",
      "Trained batch 1412 batch loss 3.09587789 epoch total loss 3.18630934\n",
      "Trained batch 1413 batch loss 3.18646 epoch total loss 3.18630958\n",
      "Trained batch 1414 batch loss 3.06398 epoch total loss 3.18622303\n",
      "Trained batch 1415 batch loss 3.05671954 epoch total loss 3.18613148\n",
      "Trained batch 1416 batch loss 3.06341648 epoch total loss 3.18604469\n",
      "Trained batch 1417 batch loss 3.00776839 epoch total loss 3.18591905\n",
      "Trained batch 1418 batch loss 3.06377935 epoch total loss 3.18583298\n",
      "Trained batch 1419 batch loss 3.05371857 epoch total loss 3.18574\n",
      "Trained batch 1420 batch loss 3.10222387 epoch total loss 3.18568087\n",
      "Trained batch 1421 batch loss 3.09472895 epoch total loss 3.18561697\n",
      "Trained batch 1422 batch loss 3.07557583 epoch total loss 3.18553972\n",
      "Trained batch 1423 batch loss 3.02198672 epoch total loss 3.1854248\n",
      "Trained batch 1424 batch loss 2.88342333 epoch total loss 3.18521261\n",
      "Trained batch 1425 batch loss 3.13860464 epoch total loss 3.18518\n",
      "Trained batch 1426 batch loss 2.85732651 epoch total loss 3.18495\n",
      "Trained batch 1427 batch loss 3.24225521 epoch total loss 3.18499017\n",
      "Trained batch 1428 batch loss 2.85968518 epoch total loss 3.18476248\n",
      "Trained batch 1429 batch loss 3.02006912 epoch total loss 3.18464708\n",
      "Trained batch 1430 batch loss 3.07428861 epoch total loss 3.18457\n",
      "Trained batch 1431 batch loss 2.99274135 epoch total loss 3.18443584\n",
      "Trained batch 1432 batch loss 2.90976787 epoch total loss 3.18424392\n",
      "Trained batch 1433 batch loss 2.85014749 epoch total loss 3.18401074\n",
      "Trained batch 1434 batch loss 2.73420143 epoch total loss 3.18369722\n",
      "Trained batch 1435 batch loss 2.4165504 epoch total loss 3.18316269\n",
      "Trained batch 1436 batch loss 2.71563888 epoch total loss 3.18283725\n",
      "Trained batch 1437 batch loss 2.79107213 epoch total loss 3.1825645\n",
      "Trained batch 1438 batch loss 2.97233438 epoch total loss 3.18241811\n",
      "Trained batch 1439 batch loss 2.91552114 epoch total loss 3.18223286\n",
      "Trained batch 1440 batch loss 3.04340601 epoch total loss 3.1821363\n",
      "Trained batch 1441 batch loss 2.95982075 epoch total loss 3.18198228\n",
      "Trained batch 1442 batch loss 3.15448093 epoch total loss 3.18196297\n",
      "Trained batch 1443 batch loss 2.97678852 epoch total loss 3.18182063\n",
      "Trained batch 1444 batch loss 3.09987736 epoch total loss 3.18176413\n",
      "Trained batch 1445 batch loss 3.15002346 epoch total loss 3.18174195\n",
      "Trained batch 1446 batch loss 2.96863127 epoch total loss 3.18159461\n",
      "Trained batch 1447 batch loss 2.93288 epoch total loss 3.18142295\n",
      "Trained batch 1448 batch loss 3.01162767 epoch total loss 3.18130589\n",
      "Trained batch 1449 batch loss 2.68017125 epoch total loss 3.18096\n",
      "Trained batch 1450 batch loss 2.86647415 epoch total loss 3.18074322\n",
      "Trained batch 1451 batch loss 2.98290634 epoch total loss 3.18060684\n",
      "Trained batch 1452 batch loss 2.89921856 epoch total loss 3.18041325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1453 batch loss 3.07679343 epoch total loss 3.18034172\n",
      "Trained batch 1454 batch loss 3.14021301 epoch total loss 3.18031406\n",
      "Trained batch 1455 batch loss 3.14499736 epoch total loss 3.18029\n",
      "Trained batch 1456 batch loss 3.10282707 epoch total loss 3.18023682\n",
      "Trained batch 1457 batch loss 3.13812733 epoch total loss 3.18020797\n",
      "Trained batch 1458 batch loss 3.09527063 epoch total loss 3.18014956\n",
      "Trained batch 1459 batch loss 2.74915242 epoch total loss 3.17985415\n",
      "Trained batch 1460 batch loss 2.66417575 epoch total loss 3.17950082\n",
      "Trained batch 1461 batch loss 2.86559415 epoch total loss 3.179286\n",
      "Trained batch 1462 batch loss 2.62102342 epoch total loss 3.17890429\n",
      "Trained batch 1463 batch loss 2.82731414 epoch total loss 3.17866397\n",
      "Trained batch 1464 batch loss 3.0445075 epoch total loss 3.17857218\n",
      "Trained batch 1465 batch loss 3.10272098 epoch total loss 3.1785202\n",
      "Trained batch 1466 batch loss 3.27644873 epoch total loss 3.17858696\n",
      "Trained batch 1467 batch loss 3.20446897 epoch total loss 3.17860484\n",
      "Trained batch 1468 batch loss 2.89184618 epoch total loss 3.17840958\n",
      "Trained batch 1469 batch loss 3.03614664 epoch total loss 3.17831278\n",
      "Trained batch 1470 batch loss 2.95844412 epoch total loss 3.17816329\n",
      "Trained batch 1471 batch loss 3.23450518 epoch total loss 3.17820144\n",
      "Trained batch 1472 batch loss 3.20858145 epoch total loss 3.17822194\n",
      "Trained batch 1473 batch loss 3.20484209 epoch total loss 3.1782403\n",
      "Trained batch 1474 batch loss 3.0694778 epoch total loss 3.17816639\n",
      "Trained batch 1475 batch loss 2.95088959 epoch total loss 3.17801213\n",
      "Trained batch 1476 batch loss 3.13162827 epoch total loss 3.1779809\n",
      "Trained batch 1477 batch loss 3.1631546 epoch total loss 3.17797065\n",
      "Trained batch 1478 batch loss 2.87008286 epoch total loss 3.17776251\n",
      "Trained batch 1479 batch loss 2.96298623 epoch total loss 3.17761707\n",
      "Trained batch 1480 batch loss 3.03204298 epoch total loss 3.17751884\n",
      "Trained batch 1481 batch loss 2.92333865 epoch total loss 3.17734742\n",
      "Trained batch 1482 batch loss 3.0413022 epoch total loss 3.17725563\n",
      "Trained batch 1483 batch loss 3.08022499 epoch total loss 3.17719\n",
      "Trained batch 1484 batch loss 3.1634059 epoch total loss 3.17718101\n",
      "Trained batch 1485 batch loss 3.20406747 epoch total loss 3.17719913\n",
      "Trained batch 1486 batch loss 3.19334 epoch total loss 3.17720985\n",
      "Trained batch 1487 batch loss 3.0541153 epoch total loss 3.17712712\n",
      "Trained batch 1488 batch loss 2.97220612 epoch total loss 3.17698956\n",
      "Trained batch 1489 batch loss 3.58711529 epoch total loss 3.17726469\n",
      "Trained batch 1490 batch loss 3.59799767 epoch total loss 3.17754722\n",
      "Trained batch 1491 batch loss 3.41452169 epoch total loss 3.17770624\n",
      "Trained batch 1492 batch loss 3.17784858 epoch total loss 3.17770624\n",
      "Trained batch 1493 batch loss 2.81026483 epoch total loss 3.17746\n",
      "Trained batch 1494 batch loss 2.80484152 epoch total loss 3.17721057\n",
      "Trained batch 1495 batch loss 2.86728334 epoch total loss 3.17700315\n",
      "Trained batch 1496 batch loss 2.831707 epoch total loss 3.17677212\n",
      "Trained batch 1497 batch loss 2.77891922 epoch total loss 3.17650628\n",
      "Trained batch 1498 batch loss 2.63120508 epoch total loss 3.17614245\n",
      "Trained batch 1499 batch loss 2.35981894 epoch total loss 3.17559791\n",
      "Trained batch 1500 batch loss 2.33231878 epoch total loss 3.17503572\n",
      "Trained batch 1501 batch loss 2.88366413 epoch total loss 3.17484188\n",
      "Trained batch 1502 batch loss 2.86370325 epoch total loss 3.1746347\n",
      "Trained batch 1503 batch loss 2.94684076 epoch total loss 3.17448306\n",
      "Trained batch 1504 batch loss 2.61249566 epoch total loss 3.17410922\n",
      "Trained batch 1505 batch loss 2.4428978 epoch total loss 3.17362332\n",
      "Trained batch 1506 batch loss 2.2645123 epoch total loss 3.17302\n",
      "Trained batch 1507 batch loss 2.47202682 epoch total loss 3.17255473\n",
      "Trained batch 1508 batch loss 2.52991962 epoch total loss 3.17212844\n",
      "Trained batch 1509 batch loss 2.55458093 epoch total loss 3.17171931\n",
      "Trained batch 1510 batch loss 2.55224252 epoch total loss 3.17130899\n",
      "Trained batch 1511 batch loss 2.7138052 epoch total loss 3.17100644\n",
      "Trained batch 1512 batch loss 2.71025133 epoch total loss 3.17070174\n",
      "Trained batch 1513 batch loss 3.16160583 epoch total loss 3.17069578\n",
      "Trained batch 1514 batch loss 2.85473919 epoch total loss 3.17048717\n",
      "Trained batch 1515 batch loss 2.91492033 epoch total loss 3.1703186\n",
      "Trained batch 1516 batch loss 3.09231949 epoch total loss 3.17026711\n",
      "Trained batch 1517 batch loss 3.14097285 epoch total loss 3.17024803\n",
      "Trained batch 1518 batch loss 3.25779104 epoch total loss 3.17030573\n",
      "Trained batch 1519 batch loss 3.2749722 epoch total loss 3.17037439\n",
      "Trained batch 1520 batch loss 3.44984674 epoch total loss 3.17055821\n",
      "Trained batch 1521 batch loss 3.18433666 epoch total loss 3.17056751\n",
      "Trained batch 1522 batch loss 3.31378412 epoch total loss 3.17066169\n",
      "Trained batch 1523 batch loss 3.23795891 epoch total loss 3.1707058\n",
      "Trained batch 1524 batch loss 3.67610025 epoch total loss 3.17103744\n",
      "Trained batch 1525 batch loss 3.40248561 epoch total loss 3.17118907\n",
      "Trained batch 1526 batch loss 3.17791367 epoch total loss 3.17119336\n",
      "Trained batch 1527 batch loss 3.12104273 epoch total loss 3.1711607\n",
      "Trained batch 1528 batch loss 3.25906205 epoch total loss 3.1712184\n",
      "Trained batch 1529 batch loss 2.83884501 epoch total loss 3.17100096\n",
      "Trained batch 1530 batch loss 3.03973055 epoch total loss 3.17091513\n",
      "Trained batch 1531 batch loss 3.10865331 epoch total loss 3.1708746\n",
      "Trained batch 1532 batch loss 3.21339774 epoch total loss 3.17090225\n",
      "Trained batch 1533 batch loss 3.41971064 epoch total loss 3.17106462\n",
      "Trained batch 1534 batch loss 2.91402268 epoch total loss 3.17089725\n",
      "Trained batch 1535 batch loss 3.17936277 epoch total loss 3.17090249\n",
      "Trained batch 1536 batch loss 2.88712335 epoch total loss 3.17071795\n",
      "Trained batch 1537 batch loss 2.89791799 epoch total loss 3.17054033\n",
      "Trained batch 1538 batch loss 2.92401266 epoch total loss 3.17037988\n",
      "Trained batch 1539 batch loss 2.92647409 epoch total loss 3.17022133\n",
      "Trained batch 1540 batch loss 2.98081708 epoch total loss 3.17009854\n",
      "Trained batch 1541 batch loss 2.97908926 epoch total loss 3.16997457\n",
      "Trained batch 1542 batch loss 2.95257568 epoch total loss 3.16983342\n",
      "Trained batch 1543 batch loss 2.88133717 epoch total loss 3.1696465\n",
      "Trained batch 1544 batch loss 3.14271951 epoch total loss 3.1696291\n",
      "Trained batch 1545 batch loss 2.86000252 epoch total loss 3.16942859\n",
      "Trained batch 1546 batch loss 3.03383541 epoch total loss 3.16934085\n",
      "Trained batch 1547 batch loss 3.10310221 epoch total loss 3.16929793\n",
      "Trained batch 1548 batch loss 3.18354821 epoch total loss 3.16930699\n",
      "Trained batch 1549 batch loss 3.22977161 epoch total loss 3.16934633\n",
      "Trained batch 1550 batch loss 3.28255582 epoch total loss 3.16941953\n",
      "Trained batch 1551 batch loss 3.02317905 epoch total loss 3.16932487\n",
      "Trained batch 1552 batch loss 3.1081295 epoch total loss 3.1692853\n",
      "Trained batch 1553 batch loss 3.41013 epoch total loss 3.16944051\n",
      "Trained batch 1554 batch loss 3.09242201 epoch total loss 3.16939092\n",
      "Trained batch 1555 batch loss 2.85519338 epoch total loss 3.16918874\n",
      "Trained batch 1556 batch loss 2.9782908 epoch total loss 3.16906619\n",
      "Trained batch 1557 batch loss 2.95688033 epoch total loss 3.16893\n",
      "Trained batch 1558 batch loss 2.95726824 epoch total loss 3.16879392\n",
      "Trained batch 1559 batch loss 2.69716072 epoch total loss 3.1684916\n",
      "Trained batch 1560 batch loss 2.97443271 epoch total loss 3.16836715\n",
      "Trained batch 1561 batch loss 2.678931 epoch total loss 3.16805363\n",
      "Trained batch 1562 batch loss 2.82884479 epoch total loss 3.16783619\n",
      "Trained batch 1563 batch loss 3.0630362 epoch total loss 3.16776919\n",
      "Trained batch 1564 batch loss 3.08906198 epoch total loss 3.16771865\n",
      "Trained batch 1565 batch loss 2.82095623 epoch total loss 3.16749692\n",
      "Trained batch 1566 batch loss 2.86843801 epoch total loss 3.16730618\n",
      "Trained batch 1567 batch loss 3.05227971 epoch total loss 3.16723275\n",
      "Trained batch 1568 batch loss 2.89079428 epoch total loss 3.16705632\n",
      "Trained batch 1569 batch loss 2.8912816 epoch total loss 3.16688037\n",
      "Trained batch 1570 batch loss 2.96855116 epoch total loss 3.16675425\n",
      "Trained batch 1571 batch loss 2.83650231 epoch total loss 3.16654396\n",
      "Trained batch 1572 batch loss 2.92776394 epoch total loss 3.16639209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1573 batch loss 2.91171336 epoch total loss 3.1662302\n",
      "Trained batch 1574 batch loss 2.86126852 epoch total loss 3.16603637\n",
      "Trained batch 1575 batch loss 2.75827575 epoch total loss 3.16577744\n",
      "Trained batch 1576 batch loss 2.92105103 epoch total loss 3.16562223\n",
      "Trained batch 1577 batch loss 2.74455595 epoch total loss 3.16535521\n",
      "Trained batch 1578 batch loss 2.86757135 epoch total loss 3.16516662\n",
      "Trained batch 1579 batch loss 3.13247108 epoch total loss 3.16514564\n",
      "Trained batch 1580 batch loss 3.1090827 epoch total loss 3.16511\n",
      "Trained batch 1581 batch loss 3.22509933 epoch total loss 3.16514802\n",
      "Trained batch 1582 batch loss 3.01830649 epoch total loss 3.16505504\n",
      "Trained batch 1583 batch loss 2.65965271 epoch total loss 3.16473579\n",
      "Trained batch 1584 batch loss 2.70708156 epoch total loss 3.16444683\n",
      "Trained batch 1585 batch loss 3.011904 epoch total loss 3.16435051\n",
      "Trained batch 1586 batch loss 2.76902056 epoch total loss 3.16410136\n",
      "Trained batch 1587 batch loss 2.79510593 epoch total loss 3.16386867\n",
      "Trained batch 1588 batch loss 2.80180192 epoch total loss 3.16364074\n",
      "Trained batch 1589 batch loss 3.09999228 epoch total loss 3.16360068\n",
      "Trained batch 1590 batch loss 3.06754208 epoch total loss 3.16354012\n",
      "Trained batch 1591 batch loss 2.97559094 epoch total loss 3.16342211\n",
      "Trained batch 1592 batch loss 2.68607664 epoch total loss 3.16312218\n",
      "Trained batch 1593 batch loss 2.89132285 epoch total loss 3.16295147\n",
      "Trained batch 1594 batch loss 2.66256881 epoch total loss 3.16263747\n",
      "Trained batch 1595 batch loss 2.93134451 epoch total loss 3.16249228\n",
      "Trained batch 1596 batch loss 2.98871136 epoch total loss 3.16238356\n",
      "Trained batch 1597 batch loss 2.99247408 epoch total loss 3.16227722\n",
      "Trained batch 1598 batch loss 3.01331472 epoch total loss 3.162184\n",
      "Trained batch 1599 batch loss 3.03352356 epoch total loss 3.16210365\n",
      "Trained batch 1600 batch loss 2.97579193 epoch total loss 3.16198707\n",
      "Trained batch 1601 batch loss 2.83667493 epoch total loss 3.16178393\n",
      "Trained batch 1602 batch loss 3.03285766 epoch total loss 3.16170335\n",
      "Trained batch 1603 batch loss 2.81647372 epoch total loss 3.16148806\n",
      "Trained batch 1604 batch loss 3.3012042 epoch total loss 3.16157508\n",
      "Trained batch 1605 batch loss 2.88350773 epoch total loss 3.16140175\n",
      "Trained batch 1606 batch loss 3.06487656 epoch total loss 3.16134167\n",
      "Trained batch 1607 batch loss 3.22912121 epoch total loss 3.16138387\n",
      "Trained batch 1608 batch loss 2.90662813 epoch total loss 3.16122532\n",
      "Trained batch 1609 batch loss 2.82162237 epoch total loss 3.16101456\n",
      "Trained batch 1610 batch loss 2.77146101 epoch total loss 3.16077256\n",
      "Trained batch 1611 batch loss 2.85609365 epoch total loss 3.16058326\n",
      "Trained batch 1612 batch loss 2.95440364 epoch total loss 3.16045547\n",
      "Trained batch 1613 batch loss 2.96933126 epoch total loss 3.16033697\n",
      "Trained batch 1614 batch loss 3.21518064 epoch total loss 3.16037107\n",
      "Trained batch 1615 batch loss 2.91362572 epoch total loss 3.16021824\n",
      "Trained batch 1616 batch loss 3.33866143 epoch total loss 3.16032887\n",
      "Trained batch 1617 batch loss 3.35620928 epoch total loss 3.16045\n",
      "Trained batch 1618 batch loss 3.54056168 epoch total loss 3.16068506\n",
      "Trained batch 1619 batch loss 3.45189977 epoch total loss 3.16086459\n",
      "Trained batch 1620 batch loss 3.34126067 epoch total loss 3.16097617\n",
      "Trained batch 1621 batch loss 3.06435823 epoch total loss 3.16091657\n",
      "Trained batch 1622 batch loss 3.15882707 epoch total loss 3.16091514\n",
      "Trained batch 1623 batch loss 3.17047501 epoch total loss 3.1609211\n",
      "Trained batch 1624 batch loss 2.96149969 epoch total loss 3.16079807\n",
      "Trained batch 1625 batch loss 3.13442039 epoch total loss 3.16078186\n",
      "Trained batch 1626 batch loss 3.09224987 epoch total loss 3.16073966\n",
      "Trained batch 1627 batch loss 2.98742294 epoch total loss 3.16063309\n",
      "Trained batch 1628 batch loss 2.77333713 epoch total loss 3.16039538\n",
      "Trained batch 1629 batch loss 2.35000825 epoch total loss 3.1598978\n",
      "Trained batch 1630 batch loss 3.00997925 epoch total loss 3.15980577\n",
      "Trained batch 1631 batch loss 2.98688054 epoch total loss 3.15969968\n",
      "Trained batch 1632 batch loss 3.09678578 epoch total loss 3.15966105\n",
      "Trained batch 1633 batch loss 3.07758284 epoch total loss 3.15961075\n",
      "Trained batch 1634 batch loss 3.15736 epoch total loss 3.15960932\n",
      "Trained batch 1635 batch loss 3.33918357 epoch total loss 3.15971923\n",
      "Trained batch 1636 batch loss 2.78080511 epoch total loss 3.15948772\n",
      "Trained batch 1637 batch loss 2.80524492 epoch total loss 3.15927124\n",
      "Trained batch 1638 batch loss 2.8576591 epoch total loss 3.15908694\n",
      "Trained batch 1639 batch loss 2.50996709 epoch total loss 3.15869093\n",
      "Trained batch 1640 batch loss 2.91632867 epoch total loss 3.15854311\n",
      "Trained batch 1641 batch loss 2.95850086 epoch total loss 3.15842128\n",
      "Trained batch 1642 batch loss 2.94887781 epoch total loss 3.15829349\n",
      "Trained batch 1643 batch loss 3.05495358 epoch total loss 3.15823078\n",
      "Trained batch 1644 batch loss 2.65305185 epoch total loss 3.15792346\n",
      "Trained batch 1645 batch loss 2.76507521 epoch total loss 3.15768456\n",
      "Trained batch 1646 batch loss 2.81796 epoch total loss 3.15747809\n",
      "Trained batch 1647 batch loss 2.84585619 epoch total loss 3.15728879\n",
      "Trained batch 1648 batch loss 3.05058622 epoch total loss 3.15722418\n",
      "Trained batch 1649 batch loss 2.6712184 epoch total loss 3.15692949\n",
      "Trained batch 1650 batch loss 2.82163143 epoch total loss 3.15672636\n",
      "Trained batch 1651 batch loss 2.76515675 epoch total loss 3.15648937\n",
      "Trained batch 1652 batch loss 2.95656347 epoch total loss 3.15636826\n",
      "Trained batch 1653 batch loss 3.11458421 epoch total loss 3.15634298\n",
      "Trained batch 1654 batch loss 3.28488684 epoch total loss 3.15642071\n",
      "Trained batch 1655 batch loss 3.46127081 epoch total loss 3.15660501\n",
      "Trained batch 1656 batch loss 2.93508387 epoch total loss 3.15647125\n",
      "Trained batch 1657 batch loss 2.87572575 epoch total loss 3.1563015\n",
      "Trained batch 1658 batch loss 3.14314437 epoch total loss 3.15629363\n",
      "Trained batch 1659 batch loss 3.16390252 epoch total loss 3.15629816\n",
      "Trained batch 1660 batch loss 2.79108 epoch total loss 3.1560781\n",
      "Trained batch 1661 batch loss 3.05914617 epoch total loss 3.15602\n",
      "Trained batch 1662 batch loss 2.71690655 epoch total loss 3.15575552\n",
      "Trained batch 1663 batch loss 3.14567327 epoch total loss 3.15574932\n",
      "Trained batch 1664 batch loss 2.94434977 epoch total loss 3.15562224\n",
      "Trained batch 1665 batch loss 3.14217949 epoch total loss 3.15561414\n",
      "Trained batch 1666 batch loss 2.9486146 epoch total loss 3.15549\n",
      "Trained batch 1667 batch loss 2.88955712 epoch total loss 3.15533066\n",
      "Trained batch 1668 batch loss 3.07659721 epoch total loss 3.15528345\n",
      "Trained batch 1669 batch loss 2.84560585 epoch total loss 3.15509796\n",
      "Trained batch 1670 batch loss 3.22335625 epoch total loss 3.15513873\n",
      "Trained batch 1671 batch loss 3.12146807 epoch total loss 3.15511847\n",
      "Trained batch 1672 batch loss 3.19805384 epoch total loss 3.15514445\n",
      "Trained batch 1673 batch loss 3.23728 epoch total loss 3.15519357\n",
      "Trained batch 1674 batch loss 3.2412622 epoch total loss 3.15524483\n",
      "Trained batch 1675 batch loss 3.1200211 epoch total loss 3.15522385\n",
      "Trained batch 1676 batch loss 3.04991484 epoch total loss 3.1551609\n",
      "Trained batch 1677 batch loss 2.85593 epoch total loss 3.15498257\n",
      "Trained batch 1678 batch loss 3.19588494 epoch total loss 3.15500689\n",
      "Trained batch 1679 batch loss 2.98415327 epoch total loss 3.15490532\n",
      "Trained batch 1680 batch loss 2.7559588 epoch total loss 3.15466785\n",
      "Trained batch 1681 batch loss 2.77730799 epoch total loss 3.15444326\n",
      "Trained batch 1682 batch loss 2.84357691 epoch total loss 3.15425849\n",
      "Trained batch 1683 batch loss 2.95398259 epoch total loss 3.15413952\n",
      "Trained batch 1684 batch loss 2.87710238 epoch total loss 3.15397501\n",
      "Trained batch 1685 batch loss 3.05756593 epoch total loss 3.15391779\n",
      "Trained batch 1686 batch loss 2.921772 epoch total loss 3.15378022\n",
      "Trained batch 1687 batch loss 2.87851691 epoch total loss 3.15361691\n",
      "Trained batch 1688 batch loss 2.80898809 epoch total loss 3.15341282\n",
      "Trained batch 1689 batch loss 2.91501641 epoch total loss 3.15327168\n",
      "Trained batch 1690 batch loss 3.09281301 epoch total loss 3.15323591\n",
      "Trained batch 1691 batch loss 3.07630014 epoch total loss 3.15319037\n",
      "Trained batch 1692 batch loss 2.74320793 epoch total loss 3.15294814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1693 batch loss 3.01525402 epoch total loss 3.1528666\n",
      "Trained batch 1694 batch loss 2.95930767 epoch total loss 3.1527524\n",
      "Trained batch 1695 batch loss 2.77160859 epoch total loss 3.15252757\n",
      "Trained batch 1696 batch loss 3.03875852 epoch total loss 3.15246034\n",
      "Trained batch 1697 batch loss 2.99318528 epoch total loss 3.1523664\n",
      "Trained batch 1698 batch loss 2.8725481 epoch total loss 3.15220165\n",
      "Trained batch 1699 batch loss 2.90357208 epoch total loss 3.1520555\n",
      "Trained batch 1700 batch loss 3.16365671 epoch total loss 3.15206218\n",
      "Trained batch 1701 batch loss 2.91820812 epoch total loss 3.15192461\n",
      "Trained batch 1702 batch loss 2.85330415 epoch total loss 3.15174937\n",
      "Trained batch 1703 batch loss 2.81618404 epoch total loss 3.15155244\n",
      "Trained batch 1704 batch loss 2.96278214 epoch total loss 3.15144157\n",
      "Trained batch 1705 batch loss 2.71770525 epoch total loss 3.15118742\n",
      "Trained batch 1706 batch loss 2.94412041 epoch total loss 3.15106606\n",
      "Trained batch 1707 batch loss 2.71736741 epoch total loss 3.15081191\n",
      "Trained batch 1708 batch loss 2.88772535 epoch total loss 3.15065789\n",
      "Trained batch 1709 batch loss 2.937428 epoch total loss 3.1505332\n",
      "Trained batch 1710 batch loss 2.83978486 epoch total loss 3.15035152\n",
      "Trained batch 1711 batch loss 2.75717425 epoch total loss 3.15012169\n",
      "Trained batch 1712 batch loss 2.67373514 epoch total loss 3.14984369\n",
      "Trained batch 1713 batch loss 2.73671699 epoch total loss 3.14960241\n",
      "Trained batch 1714 batch loss 3.04062629 epoch total loss 3.14953876\n",
      "Trained batch 1715 batch loss 2.86687613 epoch total loss 3.14937401\n",
      "Trained batch 1716 batch loss 2.82400823 epoch total loss 3.14918447\n",
      "Trained batch 1717 batch loss 2.73416924 epoch total loss 3.14894295\n",
      "Trained batch 1718 batch loss 2.94607425 epoch total loss 3.14882493\n",
      "Trained batch 1719 batch loss 2.9679172 epoch total loss 3.14871955\n",
      "Trained batch 1720 batch loss 2.68471193 epoch total loss 3.14844966\n",
      "Trained batch 1721 batch loss 2.85932684 epoch total loss 3.14828181\n",
      "Trained batch 1722 batch loss 2.69156742 epoch total loss 3.14801645\n",
      "Trained batch 1723 batch loss 3.07026863 epoch total loss 3.14797139\n",
      "Trained batch 1724 batch loss 2.92405653 epoch total loss 3.14784122\n",
      "Trained batch 1725 batch loss 2.96743155 epoch total loss 3.14773655\n",
      "Trained batch 1726 batch loss 3.08648586 epoch total loss 3.14770103\n",
      "Trained batch 1727 batch loss 3.03883529 epoch total loss 3.14763832\n",
      "Trained batch 1728 batch loss 3.23277617 epoch total loss 3.14768767\n",
      "Trained batch 1729 batch loss 3.29514813 epoch total loss 3.14777279\n",
      "Trained batch 1730 batch loss 3.01163244 epoch total loss 3.14769411\n",
      "Trained batch 1731 batch loss 2.9854672 epoch total loss 3.14760017\n",
      "Trained batch 1732 batch loss 2.84254646 epoch total loss 3.14742422\n",
      "Trained batch 1733 batch loss 3.22485948 epoch total loss 3.14746904\n",
      "Trained batch 1734 batch loss 2.99867535 epoch total loss 3.14738321\n",
      "Trained batch 1735 batch loss 2.85369539 epoch total loss 3.14721394\n",
      "Trained batch 1736 batch loss 2.97815704 epoch total loss 3.14711642\n",
      "Trained batch 1737 batch loss 2.99544811 epoch total loss 3.14702916\n",
      "Trained batch 1738 batch loss 2.91879463 epoch total loss 3.14689803\n",
      "Trained batch 1739 batch loss 2.90463805 epoch total loss 3.14675879\n",
      "Trained batch 1740 batch loss 2.82554483 epoch total loss 3.14657426\n",
      "Trained batch 1741 batch loss 2.99452305 epoch total loss 3.146487\n",
      "Trained batch 1742 batch loss 2.92027569 epoch total loss 3.14635706\n",
      "Trained batch 1743 batch loss 3.05326939 epoch total loss 3.14630365\n",
      "Trained batch 1744 batch loss 3.02218819 epoch total loss 3.14623237\n",
      "Trained batch 1745 batch loss 3.08167434 epoch total loss 3.14619541\n",
      "Trained batch 1746 batch loss 3.07861185 epoch total loss 3.14615655\n",
      "Trained batch 1747 batch loss 2.88623261 epoch total loss 3.14600778\n",
      "Trained batch 1748 batch loss 2.92114973 epoch total loss 3.14587927\n",
      "Trained batch 1749 batch loss 2.86814904 epoch total loss 3.14572048\n",
      "Trained batch 1750 batch loss 2.83579111 epoch total loss 3.14554358\n",
      "Trained batch 1751 batch loss 2.9027524 epoch total loss 3.14540482\n",
      "Trained batch 1752 batch loss 3.00728416 epoch total loss 3.14532614\n",
      "Trained batch 1753 batch loss 3.07199717 epoch total loss 3.14528418\n",
      "Trained batch 1754 batch loss 2.95761919 epoch total loss 3.14517713\n",
      "Trained batch 1755 batch loss 3.27731872 epoch total loss 3.14525247\n",
      "Trained batch 1756 batch loss 2.90892434 epoch total loss 3.14511776\n",
      "Trained batch 1757 batch loss 2.92078114 epoch total loss 3.14499\n",
      "Trained batch 1758 batch loss 2.81721663 epoch total loss 3.14480376\n",
      "Trained batch 1759 batch loss 2.85050178 epoch total loss 3.14463639\n",
      "Trained batch 1760 batch loss 3.13894248 epoch total loss 3.14463329\n",
      "Trained batch 1761 batch loss 3.15312958 epoch total loss 3.1446383\n",
      "Trained batch 1762 batch loss 3.50548697 epoch total loss 3.1448431\n",
      "Trained batch 1763 batch loss 2.86466837 epoch total loss 3.14468408\n",
      "Trained batch 1764 batch loss 2.50522804 epoch total loss 3.14432168\n",
      "Trained batch 1765 batch loss 3.1874218 epoch total loss 3.14434624\n",
      "Trained batch 1766 batch loss 3.0000813 epoch total loss 3.14426446\n",
      "Trained batch 1767 batch loss 3.01660395 epoch total loss 3.14419222\n",
      "Trained batch 1768 batch loss 2.89604688 epoch total loss 3.14405179\n",
      "Trained batch 1769 batch loss 2.84558582 epoch total loss 3.14388323\n",
      "Trained batch 1770 batch loss 3.1839385 epoch total loss 3.14390588\n",
      "Trained batch 1771 batch loss 2.89832211 epoch total loss 3.14376736\n",
      "Trained batch 1772 batch loss 3.28977132 epoch total loss 3.14384961\n",
      "Trained batch 1773 batch loss 3.29235435 epoch total loss 3.1439333\n",
      "Trained batch 1774 batch loss 3.17998648 epoch total loss 3.1439538\n",
      "Trained batch 1775 batch loss 2.92037392 epoch total loss 3.14382792\n",
      "Trained batch 1776 batch loss 2.81828547 epoch total loss 3.14364457\n",
      "Trained batch 1777 batch loss 2.85129261 epoch total loss 3.14348\n",
      "Trained batch 1778 batch loss 3.2482264 epoch total loss 3.14353871\n",
      "Trained batch 1779 batch loss 2.82358551 epoch total loss 3.14335895\n",
      "Trained batch 1780 batch loss 2.93603063 epoch total loss 3.1432426\n",
      "Trained batch 1781 batch loss 2.87598586 epoch total loss 3.14309239\n",
      "Trained batch 1782 batch loss 3.3416872 epoch total loss 3.14320397\n",
      "Trained batch 1783 batch loss 3.09723806 epoch total loss 3.14317822\n",
      "Trained batch 1784 batch loss 2.84138203 epoch total loss 3.14300895\n",
      "Trained batch 1785 batch loss 3.01296878 epoch total loss 3.14293623\n",
      "Trained batch 1786 batch loss 2.89296174 epoch total loss 3.14279628\n",
      "Trained batch 1787 batch loss 2.76082373 epoch total loss 3.14258242\n",
      "Trained batch 1788 batch loss 3.02847886 epoch total loss 3.14251852\n",
      "Trained batch 1789 batch loss 3.05114841 epoch total loss 3.1424675\n",
      "Trained batch 1790 batch loss 2.79167986 epoch total loss 3.14227152\n",
      "Trained batch 1791 batch loss 2.61145186 epoch total loss 3.14197516\n",
      "Trained batch 1792 batch loss 2.69240713 epoch total loss 3.14172411\n",
      "Trained batch 1793 batch loss 2.53558826 epoch total loss 3.14138627\n",
      "Trained batch 1794 batch loss 2.7615664 epoch total loss 3.14117455\n",
      "Trained batch 1795 batch loss 2.96715808 epoch total loss 3.14107776\n",
      "Trained batch 1796 batch loss 2.97968793 epoch total loss 3.14098763\n",
      "Trained batch 1797 batch loss 2.62152123 epoch total loss 3.14069867\n",
      "Trained batch 1798 batch loss 2.85651922 epoch total loss 3.1405406\n",
      "Trained batch 1799 batch loss 2.90656567 epoch total loss 3.14041066\n",
      "Trained batch 1800 batch loss 3.07156849 epoch total loss 3.14037251\n",
      "Trained batch 1801 batch loss 3.03337383 epoch total loss 3.14031291\n",
      "Trained batch 1802 batch loss 3.11294222 epoch total loss 3.14029765\n",
      "Trained batch 1803 batch loss 3.08675599 epoch total loss 3.14026809\n",
      "Trained batch 1804 batch loss 2.96676517 epoch total loss 3.140172\n",
      "Trained batch 1805 batch loss 2.92038727 epoch total loss 3.14005017\n",
      "Trained batch 1806 batch loss 2.8572824 epoch total loss 3.13989377\n",
      "Trained batch 1807 batch loss 2.83891702 epoch total loss 3.13972712\n",
      "Trained batch 1808 batch loss 2.73548651 epoch total loss 3.13950348\n",
      "Trained batch 1809 batch loss 2.80997515 epoch total loss 3.13932133\n",
      "Trained batch 1810 batch loss 2.93990922 epoch total loss 3.13921118\n",
      "Trained batch 1811 batch loss 2.8470788 epoch total loss 3.13904977\n",
      "Trained batch 1812 batch loss 2.99621367 epoch total loss 3.13897109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1813 batch loss 2.80847549 epoch total loss 3.1387887\n",
      "Trained batch 1814 batch loss 2.39418173 epoch total loss 3.13837814\n",
      "Trained batch 1815 batch loss 2.90479469 epoch total loss 3.1382494\n",
      "Trained batch 1816 batch loss 3.18806958 epoch total loss 3.13827682\n",
      "Trained batch 1817 batch loss 2.96774554 epoch total loss 3.13818312\n",
      "Trained batch 1818 batch loss 2.95500326 epoch total loss 3.13808227\n",
      "Trained batch 1819 batch loss 2.96824265 epoch total loss 3.13798904\n",
      "Trained batch 1820 batch loss 2.7150588 epoch total loss 3.13775659\n",
      "Trained batch 1821 batch loss 2.86721539 epoch total loss 3.13760781\n",
      "Trained batch 1822 batch loss 2.87453461 epoch total loss 3.13746357\n",
      "Trained batch 1823 batch loss 2.81644392 epoch total loss 3.13728738\n",
      "Trained batch 1824 batch loss 2.81284356 epoch total loss 3.13710952\n",
      "Trained batch 1825 batch loss 2.70724607 epoch total loss 3.13687396\n",
      "Trained batch 1826 batch loss 2.74885273 epoch total loss 3.13666153\n",
      "Trained batch 1827 batch loss 3.0137372 epoch total loss 3.1365943\n",
      "Trained batch 1828 batch loss 3.06080151 epoch total loss 3.13655281\n",
      "Trained batch 1829 batch loss 3.06484556 epoch total loss 3.13651371\n",
      "Trained batch 1830 batch loss 2.81932 epoch total loss 3.13634038\n",
      "Trained batch 1831 batch loss 2.7107861 epoch total loss 3.13610816\n",
      "Trained batch 1832 batch loss 2.98565531 epoch total loss 3.13602614\n",
      "Trained batch 1833 batch loss 3.2395072 epoch total loss 3.13608265\n",
      "Trained batch 1834 batch loss 2.83500051 epoch total loss 3.13591838\n",
      "Trained batch 1835 batch loss 3.2068038 epoch total loss 3.13595724\n",
      "Trained batch 1836 batch loss 2.66542149 epoch total loss 3.13570094\n",
      "Trained batch 1837 batch loss 2.61790562 epoch total loss 3.13541889\n",
      "Trained batch 1838 batch loss 2.72098446 epoch total loss 3.13519359\n",
      "Trained batch 1839 batch loss 2.95828 epoch total loss 3.1350975\n",
      "Trained batch 1840 batch loss 2.67460394 epoch total loss 3.1348474\n",
      "Trained batch 1841 batch loss 2.98890209 epoch total loss 3.13476801\n",
      "Trained batch 1842 batch loss 2.87428713 epoch total loss 3.13462663\n",
      "Trained batch 1843 batch loss 3.09665394 epoch total loss 3.13460612\n",
      "Trained batch 1844 batch loss 2.91776395 epoch total loss 3.13448858\n",
      "Trained batch 1845 batch loss 2.83132911 epoch total loss 3.13432455\n",
      "Trained batch 1846 batch loss 2.86135459 epoch total loss 3.13417649\n",
      "Trained batch 1847 batch loss 2.85379863 epoch total loss 3.13402486\n",
      "Trained batch 1848 batch loss 2.95098805 epoch total loss 3.13392591\n",
      "Trained batch 1849 batch loss 2.99937129 epoch total loss 3.1338532\n",
      "Trained batch 1850 batch loss 2.90526295 epoch total loss 3.1337297\n",
      "Trained batch 1851 batch loss 2.77095652 epoch total loss 3.13353372\n",
      "Trained batch 1852 batch loss 2.65753436 epoch total loss 3.1332767\n",
      "Trained batch 1853 batch loss 2.80085802 epoch total loss 3.13309741\n",
      "Trained batch 1854 batch loss 2.9081037 epoch total loss 3.13297606\n",
      "Trained batch 1855 batch loss 2.92889524 epoch total loss 3.13286591\n",
      "Trained batch 1856 batch loss 2.91872811 epoch total loss 3.13275075\n",
      "Trained batch 1857 batch loss 3.20725942 epoch total loss 3.13279057\n",
      "Trained batch 1858 batch loss 3.17130327 epoch total loss 3.13281155\n",
      "Trained batch 1859 batch loss 2.8161757 epoch total loss 3.13264132\n",
      "Trained batch 1860 batch loss 2.94146228 epoch total loss 3.13253832\n",
      "Trained batch 1861 batch loss 2.99970603 epoch total loss 3.13246703\n",
      "Trained batch 1862 batch loss 3.08677864 epoch total loss 3.13244247\n",
      "Trained batch 1863 batch loss 2.82266665 epoch total loss 3.1322763\n",
      "Trained batch 1864 batch loss 2.85358119 epoch total loss 3.13212681\n",
      "Trained batch 1865 batch loss 2.64092398 epoch total loss 3.13186336\n",
      "Trained batch 1866 batch loss 2.47141647 epoch total loss 3.1315093\n",
      "Trained batch 1867 batch loss 2.69027925 epoch total loss 3.13127303\n",
      "Trained batch 1868 batch loss 2.95134139 epoch total loss 3.13117671\n",
      "Trained batch 1869 batch loss 3.13858986 epoch total loss 3.13118076\n",
      "Trained batch 1870 batch loss 2.92619634 epoch total loss 3.13107109\n",
      "Trained batch 1871 batch loss 3.00620699 epoch total loss 3.13100457\n",
      "Trained batch 1872 batch loss 2.66976643 epoch total loss 3.13075829\n",
      "Trained batch 1873 batch loss 2.75822687 epoch total loss 3.13055921\n",
      "Trained batch 1874 batch loss 2.75658178 epoch total loss 3.13035965\n",
      "Trained batch 1875 batch loss 2.6747694 epoch total loss 3.1301167\n",
      "Trained batch 1876 batch loss 2.97678065 epoch total loss 3.13003492\n",
      "Trained batch 1877 batch loss 2.58653116 epoch total loss 3.12974524\n",
      "Trained batch 1878 batch loss 2.99000907 epoch total loss 3.12967086\n",
      "Trained batch 1879 batch loss 2.84110546 epoch total loss 3.12951756\n",
      "Trained batch 1880 batch loss 3.17409134 epoch total loss 3.12954116\n",
      "Trained batch 1881 batch loss 2.86951447 epoch total loss 3.12940311\n",
      "Trained batch 1882 batch loss 2.79780293 epoch total loss 3.12922692\n",
      "Trained batch 1883 batch loss 3.03106403 epoch total loss 3.12917495\n",
      "Trained batch 1884 batch loss 3.14912796 epoch total loss 3.12918544\n",
      "Trained batch 1885 batch loss 2.88190651 epoch total loss 3.12905407\n",
      "Trained batch 1886 batch loss 2.78538179 epoch total loss 3.12887192\n",
      "Trained batch 1887 batch loss 2.89912987 epoch total loss 3.12874985\n",
      "Trained batch 1888 batch loss 2.95618963 epoch total loss 3.12865853\n",
      "Trained batch 1889 batch loss 3.25243068 epoch total loss 3.1287241\n",
      "Trained batch 1890 batch loss 2.97937465 epoch total loss 3.12864494\n",
      "Trained batch 1891 batch loss 3.18598437 epoch total loss 3.12867546\n",
      "Trained batch 1892 batch loss 2.91899776 epoch total loss 3.1285646\n",
      "Trained batch 1893 batch loss 3.01367974 epoch total loss 3.1285038\n",
      "Trained batch 1894 batch loss 2.91074657 epoch total loss 3.12838888\n",
      "Trained batch 1895 batch loss 2.96098948 epoch total loss 3.12830043\n",
      "Trained batch 1896 batch loss 3.11547351 epoch total loss 3.12829351\n",
      "Trained batch 1897 batch loss 2.95640397 epoch total loss 3.12820315\n",
      "Trained batch 1898 batch loss 3.14170742 epoch total loss 3.12821\n",
      "Trained batch 1899 batch loss 3.0915606 epoch total loss 3.12819099\n",
      "Trained batch 1900 batch loss 3.07104015 epoch total loss 3.12816072\n",
      "Trained batch 1901 batch loss 3.09904695 epoch total loss 3.12814546\n",
      "Trained batch 1902 batch loss 2.79650831 epoch total loss 3.12797093\n",
      "Trained batch 1903 batch loss 2.96047592 epoch total loss 3.12788296\n",
      "Trained batch 1904 batch loss 3.03752899 epoch total loss 3.12783551\n",
      "Trained batch 1905 batch loss 3.3413229 epoch total loss 3.12794757\n",
      "Trained batch 1906 batch loss 3.13166642 epoch total loss 3.12794971\n",
      "Trained batch 1907 batch loss 3.18916154 epoch total loss 3.12798166\n",
      "Trained batch 1908 batch loss 3.32065296 epoch total loss 3.12808275\n",
      "Trained batch 1909 batch loss 3.14181757 epoch total loss 3.12809\n",
      "Trained batch 1910 batch loss 2.88320756 epoch total loss 3.12796164\n",
      "Trained batch 1911 batch loss 3.05474925 epoch total loss 3.12792325\n",
      "Trained batch 1912 batch loss 2.81004381 epoch total loss 3.12775707\n",
      "Trained batch 1913 batch loss 2.92678809 epoch total loss 3.12765193\n",
      "Trained batch 1914 batch loss 2.89590597 epoch total loss 3.12753105\n",
      "Trained batch 1915 batch loss 3.08308291 epoch total loss 3.12750769\n",
      "Trained batch 1916 batch loss 2.88874292 epoch total loss 3.12738299\n",
      "Trained batch 1917 batch loss 2.9310658 epoch total loss 3.12728071\n",
      "Trained batch 1918 batch loss 2.85149884 epoch total loss 3.12713695\n",
      "Trained batch 1919 batch loss 3.1582942 epoch total loss 3.12715316\n",
      "Trained batch 1920 batch loss 2.94010925 epoch total loss 3.12705564\n",
      "Trained batch 1921 batch loss 2.51277542 epoch total loss 3.12673569\n",
      "Trained batch 1922 batch loss 2.77503324 epoch total loss 3.12655282\n",
      "Trained batch 1923 batch loss 2.93089199 epoch total loss 3.12645078\n",
      "Trained batch 1924 batch loss 2.88567281 epoch total loss 3.12632585\n",
      "Trained batch 1925 batch loss 2.77116513 epoch total loss 3.12614107\n",
      "Trained batch 1926 batch loss 2.78354025 epoch total loss 3.12596345\n",
      "Trained batch 1927 batch loss 2.88854456 epoch total loss 3.12584019\n",
      "Trained batch 1928 batch loss 2.72367644 epoch total loss 3.12563157\n",
      "Trained batch 1929 batch loss 2.79840803 epoch total loss 3.12546206\n",
      "Trained batch 1930 batch loss 2.80092525 epoch total loss 3.12529373\n",
      "Trained batch 1931 batch loss 2.93755746 epoch total loss 3.12519646\n",
      "Trained batch 1932 batch loss 3.29651737 epoch total loss 3.12528515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1933 batch loss 2.78044105 epoch total loss 3.12510657\n",
      "Trained batch 1934 batch loss 2.81237721 epoch total loss 3.12494493\n",
      "Trained batch 1935 batch loss 3.15829659 epoch total loss 3.12496209\n",
      "Trained batch 1936 batch loss 3.03442192 epoch total loss 3.12491536\n",
      "Trained batch 1937 batch loss 3.05250025 epoch total loss 3.12487793\n",
      "Trained batch 1938 batch loss 3.11394095 epoch total loss 3.12487221\n",
      "Trained batch 1939 batch loss 2.82771492 epoch total loss 3.1247189\n",
      "Trained batch 1940 batch loss 2.93095636 epoch total loss 3.12461925\n",
      "Trained batch 1941 batch loss 2.99046636 epoch total loss 3.12454987\n",
      "Trained batch 1942 batch loss 2.95106459 epoch total loss 3.1244607\n",
      "Trained batch 1943 batch loss 2.88459897 epoch total loss 3.1243372\n",
      "Trained batch 1944 batch loss 3.14981604 epoch total loss 3.12435055\n",
      "Trained batch 1945 batch loss 3.16239572 epoch total loss 3.12437\n",
      "Trained batch 1946 batch loss 2.81821346 epoch total loss 3.12421298\n",
      "Trained batch 1947 batch loss 2.95201302 epoch total loss 3.12412453\n",
      "Trained batch 1948 batch loss 3.07279181 epoch total loss 3.12409806\n",
      "Trained batch 1949 batch loss 2.64724684 epoch total loss 3.12385368\n",
      "Trained batch 1950 batch loss 2.98654509 epoch total loss 3.12378311\n",
      "Trained batch 1951 batch loss 2.84424782 epoch total loss 3.12363982\n",
      "Trained batch 1952 batch loss 2.80808902 epoch total loss 3.12347817\n",
      "Trained batch 1953 batch loss 2.73762488 epoch total loss 3.12328053\n",
      "Trained batch 1954 batch loss 2.57146168 epoch total loss 3.12299824\n",
      "Trained batch 1955 batch loss 2.63684559 epoch total loss 3.12274933\n",
      "Trained batch 1956 batch loss 2.67968106 epoch total loss 3.12252283\n",
      "Trained batch 1957 batch loss 2.97009206 epoch total loss 3.12244511\n",
      "Trained batch 1958 batch loss 2.86246157 epoch total loss 3.12231231\n",
      "Trained batch 1959 batch loss 2.87638092 epoch total loss 3.12218666\n",
      "Trained batch 1960 batch loss 2.97290492 epoch total loss 3.12211061\n",
      "Trained batch 1961 batch loss 2.82281399 epoch total loss 3.12195802\n",
      "Trained batch 1962 batch loss 2.79796267 epoch total loss 3.12179279\n",
      "Trained batch 1963 batch loss 2.93266845 epoch total loss 3.12169647\n",
      "Trained batch 1964 batch loss 2.92732549 epoch total loss 3.12159753\n",
      "Trained batch 1965 batch loss 3.05647731 epoch total loss 3.12156439\n",
      "Trained batch 1966 batch loss 2.76138902 epoch total loss 3.12138104\n",
      "Trained batch 1967 batch loss 2.75714207 epoch total loss 3.12119603\n",
      "Trained batch 1968 batch loss 2.98317409 epoch total loss 3.12112594\n",
      "Trained batch 1969 batch loss 2.89771414 epoch total loss 3.12101269\n",
      "Trained batch 1970 batch loss 2.97208 epoch total loss 3.12093711\n",
      "Trained batch 1971 batch loss 2.85741258 epoch total loss 3.12080336\n",
      "Trained batch 1972 batch loss 2.89675689 epoch total loss 3.12068987\n",
      "Trained batch 1973 batch loss 2.76620555 epoch total loss 3.12051\n",
      "Trained batch 1974 batch loss 2.73065615 epoch total loss 3.12031269\n",
      "Trained batch 1975 batch loss 2.657547 epoch total loss 3.12007833\n",
      "Trained batch 1976 batch loss 2.73824906 epoch total loss 3.11988521\n",
      "Trained batch 1977 batch loss 2.75376129 epoch total loss 3.1197\n",
      "Trained batch 1978 batch loss 3.29731655 epoch total loss 3.11978984\n",
      "Trained batch 1979 batch loss 2.98096418 epoch total loss 3.11971974\n",
      "Trained batch 1980 batch loss 3.19285727 epoch total loss 3.1197567\n",
      "Trained batch 1981 batch loss 3.04532933 epoch total loss 3.11971903\n",
      "Trained batch 1982 batch loss 2.77285099 epoch total loss 3.11954427\n",
      "Trained batch 1983 batch loss 3.045259 epoch total loss 3.11950684\n",
      "Trained batch 1984 batch loss 2.84120107 epoch total loss 3.11936665\n",
      "Trained batch 1985 batch loss 3.07177186 epoch total loss 3.11934257\n",
      "Trained batch 1986 batch loss 2.91426826 epoch total loss 3.11923933\n",
      "Trained batch 1987 batch loss 3.17402506 epoch total loss 3.11926675\n",
      "Trained batch 1988 batch loss 2.74670601 epoch total loss 3.11907911\n",
      "Trained batch 1989 batch loss 3.10414 epoch total loss 3.11907172\n",
      "Trained batch 1990 batch loss 2.63908672 epoch total loss 3.11883044\n",
      "Trained batch 1991 batch loss 2.57531309 epoch total loss 3.11855745\n",
      "Trained batch 1992 batch loss 2.73059845 epoch total loss 3.11836267\n",
      "Trained batch 1993 batch loss 2.58551168 epoch total loss 3.11809516\n",
      "Trained batch 1994 batch loss 2.9709034 epoch total loss 3.11802125\n",
      "Trained batch 1995 batch loss 2.98715329 epoch total loss 3.11795568\n",
      "Trained batch 1996 batch loss 3.08652115 epoch total loss 3.11794\n",
      "Trained batch 1997 batch loss 3.06184053 epoch total loss 3.11791205\n",
      "Trained batch 1998 batch loss 3.0499692 epoch total loss 3.11787796\n",
      "Trained batch 1999 batch loss 3.02422094 epoch total loss 3.11783123\n",
      "Trained batch 2000 batch loss 2.97413635 epoch total loss 3.11775923\n",
      "Trained batch 2001 batch loss 2.95107484 epoch total loss 3.11767602\n",
      "Trained batch 2002 batch loss 2.9357605 epoch total loss 3.11758494\n",
      "Trained batch 2003 batch loss 2.65505123 epoch total loss 3.11735415\n",
      "Trained batch 2004 batch loss 2.78646111 epoch total loss 3.11718917\n",
      "Trained batch 2005 batch loss 3.03042316 epoch total loss 3.11714578\n",
      "Trained batch 2006 batch loss 2.65993047 epoch total loss 3.11691809\n",
      "Trained batch 2007 batch loss 3.18605375 epoch total loss 3.11695242\n",
      "Trained batch 2008 batch loss 3.08853674 epoch total loss 3.11693835\n",
      "Trained batch 2009 batch loss 2.97744656 epoch total loss 3.11686897\n",
      "Trained batch 2010 batch loss 2.89982796 epoch total loss 3.11676097\n",
      "Trained batch 2011 batch loss 2.28754115 epoch total loss 3.11634851\n",
      "Trained batch 2012 batch loss 2.64221644 epoch total loss 3.11611295\n",
      "Trained batch 2013 batch loss 2.68504238 epoch total loss 3.11589885\n",
      "Trained batch 2014 batch loss 2.59220099 epoch total loss 3.11563873\n",
      "Trained batch 2015 batch loss 2.70358396 epoch total loss 3.11543441\n",
      "Trained batch 2016 batch loss 2.54135013 epoch total loss 3.1151495\n",
      "Trained batch 2017 batch loss 2.52675462 epoch total loss 3.11485791\n",
      "Trained batch 2018 batch loss 2.65470147 epoch total loss 3.11463\n",
      "Trained batch 2019 batch loss 3.02076483 epoch total loss 3.11458349\n",
      "Trained batch 2020 batch loss 3.10663557 epoch total loss 3.11457944\n",
      "Trained batch 2021 batch loss 3.02136183 epoch total loss 3.11453342\n",
      "Trained batch 2022 batch loss 2.9637866 epoch total loss 3.11445904\n",
      "Trained batch 2023 batch loss 2.96862555 epoch total loss 3.11438704\n",
      "Trained batch 2024 batch loss 2.92196369 epoch total loss 3.11429191\n",
      "Trained batch 2025 batch loss 2.70966744 epoch total loss 3.11409187\n",
      "Trained batch 2026 batch loss 2.63168049 epoch total loss 3.11385393\n",
      "Trained batch 2027 batch loss 2.58680773 epoch total loss 3.11359382\n",
      "Trained batch 2028 batch loss 2.69973946 epoch total loss 3.11338973\n",
      "Trained batch 2029 batch loss 2.78319311 epoch total loss 3.11322713\n",
      "Trained batch 2030 batch loss 3.19402242 epoch total loss 3.11326671\n",
      "Trained batch 2031 batch loss 3.21109509 epoch total loss 3.11331487\n",
      "Trained batch 2032 batch loss 2.8995142 epoch total loss 3.11320972\n",
      "Trained batch 2033 batch loss 2.89875174 epoch total loss 3.11310434\n",
      "Trained batch 2034 batch loss 2.91067934 epoch total loss 3.11300468\n",
      "Trained batch 2035 batch loss 2.98179984 epoch total loss 3.11294031\n",
      "Trained batch 2036 batch loss 2.80404139 epoch total loss 3.11278868\n",
      "Trained batch 2037 batch loss 2.74990439 epoch total loss 3.11261058\n",
      "Trained batch 2038 batch loss 2.65707159 epoch total loss 3.11238718\n",
      "Trained batch 2039 batch loss 2.65330219 epoch total loss 3.11216187\n",
      "Trained batch 2040 batch loss 2.58277559 epoch total loss 3.11190248\n",
      "Trained batch 2041 batch loss 2.41394138 epoch total loss 3.11156058\n",
      "Trained batch 2042 batch loss 2.75288582 epoch total loss 3.11138511\n",
      "Trained batch 2043 batch loss 2.56614923 epoch total loss 3.11111808\n",
      "Trained batch 2044 batch loss 2.64375734 epoch total loss 3.1108892\n",
      "Trained batch 2045 batch loss 2.40505505 epoch total loss 3.1105442\n",
      "Trained batch 2046 batch loss 2.60135674 epoch total loss 3.11029553\n",
      "Trained batch 2047 batch loss 2.58335781 epoch total loss 3.11003804\n",
      "Trained batch 2048 batch loss 2.77600121 epoch total loss 3.10987496\n",
      "Trained batch 2049 batch loss 3.10441542 epoch total loss 3.10987234\n",
      "Trained batch 2050 batch loss 3.10063124 epoch total loss 3.10986781\n",
      "Trained batch 2051 batch loss 2.92682052 epoch total loss 3.10977864\n",
      "Trained batch 2052 batch loss 2.95919299 epoch total loss 3.10970497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2053 batch loss 2.9720397 epoch total loss 3.10963798\n",
      "Trained batch 2054 batch loss 3.01838636 epoch total loss 3.10959363\n",
      "Trained batch 2055 batch loss 2.75447702 epoch total loss 3.10942078\n",
      "Trained batch 2056 batch loss 2.91868186 epoch total loss 3.10932803\n",
      "Trained batch 2057 batch loss 2.79101324 epoch total loss 3.1091733\n",
      "Trained batch 2058 batch loss 2.47088146 epoch total loss 3.10886288\n",
      "Trained batch 2059 batch loss 2.58818913 epoch total loss 3.10861015\n",
      "Trained batch 2060 batch loss 2.7327044 epoch total loss 3.10842776\n",
      "Trained batch 2061 batch loss 2.99043465 epoch total loss 3.10837054\n",
      "Trained batch 2062 batch loss 2.93967056 epoch total loss 3.10828853\n",
      "Trained batch 2063 batch loss 3.09230089 epoch total loss 3.1082809\n",
      "Trained batch 2064 batch loss 2.90836835 epoch total loss 3.10818386\n",
      "Trained batch 2065 batch loss 2.89989877 epoch total loss 3.10808301\n",
      "Trained batch 2066 batch loss 2.65662861 epoch total loss 3.10786462\n",
      "Trained batch 2067 batch loss 2.80086589 epoch total loss 3.10771608\n",
      "Trained batch 2068 batch loss 2.76584339 epoch total loss 3.10755062\n",
      "Trained batch 2069 batch loss 2.7092607 epoch total loss 3.10735822\n",
      "Trained batch 2070 batch loss 2.59841156 epoch total loss 3.10711241\n",
      "Trained batch 2071 batch loss 2.94576812 epoch total loss 3.10703444\n",
      "Trained batch 2072 batch loss 3.14342594 epoch total loss 3.10705209\n",
      "Trained batch 2073 batch loss 2.81922126 epoch total loss 3.10691333\n",
      "Trained batch 2074 batch loss 3.21864438 epoch total loss 3.10696721\n",
      "Trained batch 2075 batch loss 3.07105398 epoch total loss 3.10695\n",
      "Trained batch 2076 batch loss 3.06399345 epoch total loss 3.1069293\n",
      "Trained batch 2077 batch loss 3.15229654 epoch total loss 3.10695124\n",
      "Trained batch 2078 batch loss 2.81697059 epoch total loss 3.10681152\n",
      "Trained batch 2079 batch loss 2.63987541 epoch total loss 3.10658693\n",
      "Trained batch 2080 batch loss 2.58489037 epoch total loss 3.10633612\n",
      "Trained batch 2081 batch loss 2.72595549 epoch total loss 3.10615349\n",
      "Trained batch 2082 batch loss 2.61165857 epoch total loss 3.10591602\n",
      "Trained batch 2083 batch loss 2.77736735 epoch total loss 3.10575819\n",
      "Trained batch 2084 batch loss 2.66576624 epoch total loss 3.10554695\n",
      "Trained batch 2085 batch loss 2.73350096 epoch total loss 3.10536861\n",
      "Trained batch 2086 batch loss 2.61666393 epoch total loss 3.10513425\n",
      "Trained batch 2087 batch loss 2.83374977 epoch total loss 3.10500431\n",
      "Trained batch 2088 batch loss 2.64941692 epoch total loss 3.10478616\n",
      "Trained batch 2089 batch loss 3.04092789 epoch total loss 3.10475564\n",
      "Trained batch 2090 batch loss 2.8596034 epoch total loss 3.1046381\n",
      "Trained batch 2091 batch loss 2.77389669 epoch total loss 3.10448\n",
      "Trained batch 2092 batch loss 2.86440563 epoch total loss 3.10436511\n",
      "Trained batch 2093 batch loss 2.64885092 epoch total loss 3.10414767\n",
      "Trained batch 2094 batch loss 2.70068502 epoch total loss 3.10395503\n",
      "Trained batch 2095 batch loss 2.73313 epoch total loss 3.10377789\n",
      "Trained batch 2096 batch loss 2.69147396 epoch total loss 3.10358119\n",
      "Trained batch 2097 batch loss 2.7841754 epoch total loss 3.10342884\n",
      "Trained batch 2098 batch loss 2.85560942 epoch total loss 3.10331059\n",
      "Trained batch 2099 batch loss 2.89971805 epoch total loss 3.10321379\n",
      "Trained batch 2100 batch loss 2.95456743 epoch total loss 3.10314298\n",
      "Trained batch 2101 batch loss 2.68775105 epoch total loss 3.10294533\n",
      "Trained batch 2102 batch loss 2.37938881 epoch total loss 3.10260105\n",
      "Trained batch 2103 batch loss 2.57170868 epoch total loss 3.10234857\n",
      "Trained batch 2104 batch loss 2.72443795 epoch total loss 3.10216904\n",
      "Trained batch 2105 batch loss 2.13015079 epoch total loss 3.10170746\n",
      "Trained batch 2106 batch loss 2.1744709 epoch total loss 3.1012671\n",
      "Trained batch 2107 batch loss 2.19508457 epoch total loss 3.10083723\n",
      "Trained batch 2108 batch loss 2.29728961 epoch total loss 3.100456\n",
      "Trained batch 2109 batch loss 2.95813465 epoch total loss 3.10038853\n",
      "Trained batch 2110 batch loss 3.06618547 epoch total loss 3.10037231\n",
      "Trained batch 2111 batch loss 3.29935312 epoch total loss 3.10046649\n",
      "Trained batch 2112 batch loss 3.13903546 epoch total loss 3.10048485\n",
      "Trained batch 2113 batch loss 3.09633493 epoch total loss 3.10048294\n",
      "Trained batch 2114 batch loss 2.94859552 epoch total loss 3.10041118\n",
      "Trained batch 2115 batch loss 2.7046566 epoch total loss 3.10022402\n",
      "Trained batch 2116 batch loss 3.00583601 epoch total loss 3.10017943\n",
      "Trained batch 2117 batch loss 2.98593187 epoch total loss 3.10012531\n",
      "Trained batch 2118 batch loss 2.67549968 epoch total loss 3.0999248\n",
      "Trained batch 2119 batch loss 2.93293977 epoch total loss 3.09984612\n",
      "Trained batch 2120 batch loss 2.88424349 epoch total loss 3.09974432\n",
      "Trained batch 2121 batch loss 2.58862925 epoch total loss 3.09950352\n",
      "Trained batch 2122 batch loss 2.66166878 epoch total loss 3.09929705\n",
      "Trained batch 2123 batch loss 2.69539833 epoch total loss 3.09910679\n",
      "Trained batch 2124 batch loss 2.64064 epoch total loss 3.09889102\n",
      "Trained batch 2125 batch loss 2.86450958 epoch total loss 3.09878087\n",
      "Trained batch 2126 batch loss 2.64446974 epoch total loss 3.09856701\n",
      "Trained batch 2127 batch loss 2.9965167 epoch total loss 3.09851909\n",
      "Trained batch 2128 batch loss 2.75162721 epoch total loss 3.09835601\n",
      "Trained batch 2129 batch loss 2.83885431 epoch total loss 3.09823418\n",
      "Trained batch 2130 batch loss 2.82454014 epoch total loss 3.09810591\n",
      "Trained batch 2131 batch loss 3.31886435 epoch total loss 3.09820938\n",
      "Trained batch 2132 batch loss 3.1544652 epoch total loss 3.09823561\n",
      "Trained batch 2133 batch loss 3.13718081 epoch total loss 3.09825397\n",
      "Trained batch 2134 batch loss 3.04284811 epoch total loss 3.09822798\n",
      "Trained batch 2135 batch loss 2.85466051 epoch total loss 3.09811378\n",
      "Trained batch 2136 batch loss 3.04758596 epoch total loss 3.09809017\n",
      "Trained batch 2137 batch loss 2.8889308 epoch total loss 3.09799242\n",
      "Trained batch 2138 batch loss 3.23840046 epoch total loss 3.09805799\n",
      "Trained batch 2139 batch loss 2.93302393 epoch total loss 3.09798098\n",
      "Trained batch 2140 batch loss 3.04722548 epoch total loss 3.09795713\n",
      "Trained batch 2141 batch loss 2.87018633 epoch total loss 3.0978508\n",
      "Trained batch 2142 batch loss 3.12180519 epoch total loss 3.09786177\n",
      "Trained batch 2143 batch loss 3.13259649 epoch total loss 3.09787822\n",
      "Trained batch 2144 batch loss 2.67282581 epoch total loss 3.09767985\n",
      "Trained batch 2145 batch loss 2.74865627 epoch total loss 3.09751725\n",
      "Trained batch 2146 batch loss 3.15384388 epoch total loss 3.09754348\n",
      "Trained batch 2147 batch loss 2.95680308 epoch total loss 3.09747791\n",
      "Trained batch 2148 batch loss 2.9019134 epoch total loss 3.09738684\n",
      "Trained batch 2149 batch loss 2.72115135 epoch total loss 3.09721184\n",
      "Trained batch 2150 batch loss 2.87300038 epoch total loss 3.09710765\n",
      "Trained batch 2151 batch loss 2.94546509 epoch total loss 3.09703708\n",
      "Trained batch 2152 batch loss 2.9679563 epoch total loss 3.096977\n",
      "Trained batch 2153 batch loss 2.89563727 epoch total loss 3.0968833\n",
      "Trained batch 2154 batch loss 2.80482531 epoch total loss 3.09674764\n",
      "Trained batch 2155 batch loss 2.99713159 epoch total loss 3.09670138\n",
      "Trained batch 2156 batch loss 2.92311692 epoch total loss 3.09662104\n",
      "Trained batch 2157 batch loss 2.7673564 epoch total loss 3.09646845\n",
      "Trained batch 2158 batch loss 2.74461365 epoch total loss 3.09630537\n",
      "Trained batch 2159 batch loss 2.66970301 epoch total loss 3.09610796\n",
      "Trained batch 2160 batch loss 2.80787706 epoch total loss 3.09597468\n",
      "Trained batch 2161 batch loss 2.76354146 epoch total loss 3.0958209\n",
      "Trained batch 2162 batch loss 2.7106123 epoch total loss 3.09564257\n",
      "Trained batch 2163 batch loss 2.61005116 epoch total loss 3.09541798\n",
      "Trained batch 2164 batch loss 2.49375153 epoch total loss 3.09514\n",
      "Trained batch 2165 batch loss 2.83848619 epoch total loss 3.09502125\n",
      "Trained batch 2166 batch loss 2.82259655 epoch total loss 3.0948956\n",
      "Trained batch 2167 batch loss 2.65749669 epoch total loss 3.0946939\n",
      "Trained batch 2168 batch loss 2.86876 epoch total loss 3.09458971\n",
      "Trained batch 2169 batch loss 2.77955055 epoch total loss 3.09444451\n",
      "Trained batch 2170 batch loss 2.54468632 epoch total loss 3.09419131\n",
      "Trained batch 2171 batch loss 2.48558521 epoch total loss 3.09391069\n",
      "Trained batch 2172 batch loss 2.72569513 epoch total loss 3.09374118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2173 batch loss 2.63704777 epoch total loss 3.09353113\n",
      "Trained batch 2174 batch loss 2.19299293 epoch total loss 3.09311676\n",
      "Trained batch 2175 batch loss 2.18266106 epoch total loss 3.09269834\n",
      "Trained batch 2176 batch loss 2.41997623 epoch total loss 3.09238911\n",
      "Trained batch 2177 batch loss 2.30639076 epoch total loss 3.0920279\n",
      "Trained batch 2178 batch loss 2.5100646 epoch total loss 3.09176087\n",
      "Trained batch 2179 batch loss 2.65950346 epoch total loss 3.09156251\n",
      "Trained batch 2180 batch loss 2.8295958 epoch total loss 3.09144235\n",
      "Trained batch 2181 batch loss 2.78749394 epoch total loss 3.09130311\n",
      "Trained batch 2182 batch loss 2.54521394 epoch total loss 3.09105277\n",
      "Trained batch 2183 batch loss 2.83303738 epoch total loss 3.09093451\n",
      "Trained batch 2184 batch loss 2.98537397 epoch total loss 3.09088635\n",
      "Trained batch 2185 batch loss 2.75534725 epoch total loss 3.09073281\n",
      "Trained batch 2186 batch loss 2.63883591 epoch total loss 3.09052587\n",
      "Trained batch 2187 batch loss 2.7525804 epoch total loss 3.09037137\n",
      "Trained batch 2188 batch loss 2.6911037 epoch total loss 3.09018874\n",
      "Trained batch 2189 batch loss 2.77047729 epoch total loss 3.09004283\n",
      "Trained batch 2190 batch loss 2.90916967 epoch total loss 3.08996\n",
      "Trained batch 2191 batch loss 2.97013903 epoch total loss 3.0899055\n",
      "Trained batch 2192 batch loss 2.60235786 epoch total loss 3.08968306\n",
      "Trained batch 2193 batch loss 2.77351809 epoch total loss 3.08953905\n",
      "Trained batch 2194 batch loss 3.00581908 epoch total loss 3.0895009\n",
      "Trained batch 2195 batch loss 3.21297646 epoch total loss 3.08955693\n",
      "Trained batch 2196 batch loss 2.49079657 epoch total loss 3.08928442\n",
      "Trained batch 2197 batch loss 2.62668324 epoch total loss 3.08907366\n",
      "Trained batch 2198 batch loss 2.38962483 epoch total loss 3.08875537\n",
      "Trained batch 2199 batch loss 2.6272769 epoch total loss 3.08854556\n",
      "Trained batch 2200 batch loss 2.76968765 epoch total loss 3.0884006\n",
      "Trained batch 2201 batch loss 2.73756766 epoch total loss 3.08824134\n",
      "Trained batch 2202 batch loss 2.79060411 epoch total loss 3.08810616\n",
      "Trained batch 2203 batch loss 2.82439208 epoch total loss 3.08798647\n",
      "Trained batch 2204 batch loss 3.02757525 epoch total loss 3.08795881\n",
      "Trained batch 2205 batch loss 2.89163733 epoch total loss 3.08787\n",
      "Trained batch 2206 batch loss 3.05351901 epoch total loss 3.08785439\n",
      "Trained batch 2207 batch loss 3.28974867 epoch total loss 3.0879457\n",
      "Trained batch 2208 batch loss 3.15498853 epoch total loss 3.08797598\n",
      "Trained batch 2209 batch loss 3.43156672 epoch total loss 3.08813167\n",
      "Trained batch 2210 batch loss 3.01051927 epoch total loss 3.08809662\n",
      "Trained batch 2211 batch loss 2.84551477 epoch total loss 3.08798695\n",
      "Trained batch 2212 batch loss 2.71139932 epoch total loss 3.08781672\n",
      "Trained batch 2213 batch loss 2.57980585 epoch total loss 3.08758712\n",
      "Trained batch 2214 batch loss 2.85057092 epoch total loss 3.08748\n",
      "Trained batch 2215 batch loss 2.51618338 epoch total loss 3.0872221\n",
      "Trained batch 2216 batch loss 2.80734086 epoch total loss 3.08709574\n",
      "Trained batch 2217 batch loss 2.77047682 epoch total loss 3.08695292\n",
      "Trained batch 2218 batch loss 2.56253862 epoch total loss 3.08671641\n",
      "Trained batch 2219 batch loss 2.76260185 epoch total loss 3.08657026\n",
      "Trained batch 2220 batch loss 2.82679033 epoch total loss 3.0864532\n",
      "Trained batch 2221 batch loss 2.81980753 epoch total loss 3.08633327\n",
      "Trained batch 2222 batch loss 2.88943648 epoch total loss 3.08624482\n",
      "Trained batch 2223 batch loss 2.78653479 epoch total loss 3.08610988\n",
      "Trained batch 2224 batch loss 2.9934895 epoch total loss 3.08606839\n",
      "Trained batch 2225 batch loss 2.77987099 epoch total loss 3.08593059\n",
      "Trained batch 2226 batch loss 2.627913 epoch total loss 3.08572483\n",
      "Trained batch 2227 batch loss 2.85499334 epoch total loss 3.08562136\n",
      "Trained batch 2228 batch loss 2.72112393 epoch total loss 3.0854578\n",
      "Trained batch 2229 batch loss 2.88128185 epoch total loss 3.08536625\n",
      "Trained batch 2230 batch loss 3.07216358 epoch total loss 3.08536029\n",
      "Trained batch 2231 batch loss 3.11794 epoch total loss 3.08537507\n",
      "Trained batch 2232 batch loss 2.75625634 epoch total loss 3.08522773\n",
      "Trained batch 2233 batch loss 2.98553705 epoch total loss 3.08518291\n",
      "Trained batch 2234 batch loss 3.01958418 epoch total loss 3.08515358\n",
      "Trained batch 2235 batch loss 3.06366801 epoch total loss 3.0851438\n",
      "Trained batch 2236 batch loss 2.90324497 epoch total loss 3.0850625\n",
      "Trained batch 2237 batch loss 2.91597152 epoch total loss 3.08498693\n",
      "Trained batch 2238 batch loss 2.90606856 epoch total loss 3.08490705\n",
      "Trained batch 2239 batch loss 2.68995142 epoch total loss 3.08473063\n",
      "Trained batch 2240 batch loss 2.81442285 epoch total loss 3.08461\n",
      "Trained batch 2241 batch loss 2.52120256 epoch total loss 3.08435845\n",
      "Trained batch 2242 batch loss 2.57926035 epoch total loss 3.08413315\n",
      "Trained batch 2243 batch loss 2.86595821 epoch total loss 3.08403563\n",
      "Trained batch 2244 batch loss 2.52213192 epoch total loss 3.0837853\n",
      "Trained batch 2245 batch loss 2.83561945 epoch total loss 3.08367467\n",
      "Trained batch 2246 batch loss 2.47187948 epoch total loss 3.08340216\n",
      "Trained batch 2247 batch loss 2.7685051 epoch total loss 3.08326197\n",
      "Trained batch 2248 batch loss 2.99706721 epoch total loss 3.08322382\n",
      "Trained batch 2249 batch loss 3.13533139 epoch total loss 3.08324695\n",
      "Trained batch 2250 batch loss 2.89680481 epoch total loss 3.08316398\n",
      "Trained batch 2251 batch loss 2.73671341 epoch total loss 3.0830102\n",
      "Trained batch 2252 batch loss 2.65721369 epoch total loss 3.08282113\n",
      "Trained batch 2253 batch loss 2.7340517 epoch total loss 3.08266616\n",
      "Trained batch 2254 batch loss 2.81413698 epoch total loss 3.08254695\n",
      "Trained batch 2255 batch loss 2.53961277 epoch total loss 3.08230615\n",
      "Trained batch 2256 batch loss 2.83058071 epoch total loss 3.08219457\n",
      "Trained batch 2257 batch loss 3.14718366 epoch total loss 3.08222342\n",
      "Trained batch 2258 batch loss 2.84213376 epoch total loss 3.08211708\n",
      "Trained batch 2259 batch loss 2.14470482 epoch total loss 3.08170199\n",
      "Trained batch 2260 batch loss 2.40490079 epoch total loss 3.08140254\n",
      "Trained batch 2261 batch loss 2.75351429 epoch total loss 3.08125758\n",
      "Trained batch 2262 batch loss 2.98668289 epoch total loss 3.08121562\n",
      "Trained batch 2263 batch loss 2.73198032 epoch total loss 3.08106136\n",
      "Trained batch 2264 batch loss 2.82408738 epoch total loss 3.08094788\n",
      "Trained batch 2265 batch loss 2.8619523 epoch total loss 3.08085108\n",
      "Trained batch 2266 batch loss 2.85526204 epoch total loss 3.08075166\n",
      "Trained batch 2267 batch loss 3.12796044 epoch total loss 3.08077264\n",
      "Trained batch 2268 batch loss 2.70539665 epoch total loss 3.08060718\n",
      "Trained batch 2269 batch loss 2.46646714 epoch total loss 3.08033633\n",
      "Trained batch 2270 batch loss 2.69819593 epoch total loss 3.08016801\n",
      "Trained batch 2271 batch loss 2.82254338 epoch total loss 3.08005476\n",
      "Trained batch 2272 batch loss 2.95204234 epoch total loss 3.07999849\n",
      "Trained batch 2273 batch loss 2.73104572 epoch total loss 3.07984495\n",
      "Trained batch 2274 batch loss 2.90763378 epoch total loss 3.07976913\n",
      "Trained batch 2275 batch loss 2.77896047 epoch total loss 3.07963681\n",
      "Trained batch 2276 batch loss 3.11609697 epoch total loss 3.07965302\n",
      "Trained batch 2277 batch loss 2.98097205 epoch total loss 3.07960963\n",
      "Trained batch 2278 batch loss 2.97235394 epoch total loss 3.07956243\n",
      "Trained batch 2279 batch loss 2.71155453 epoch total loss 3.07940078\n",
      "Trained batch 2280 batch loss 2.43076205 epoch total loss 3.07911634\n",
      "Trained batch 2281 batch loss 2.4374795 epoch total loss 3.07883501\n",
      "Trained batch 2282 batch loss 2.26511407 epoch total loss 3.07847857\n",
      "Trained batch 2283 batch loss 2.29742289 epoch total loss 3.07813644\n",
      "Trained batch 2284 batch loss 2.58207941 epoch total loss 3.07791924\n",
      "Trained batch 2285 batch loss 2.68032169 epoch total loss 3.07774496\n",
      "Trained batch 2286 batch loss 3.08862615 epoch total loss 3.07775\n",
      "Trained batch 2287 batch loss 2.84103584 epoch total loss 3.07764626\n",
      "Trained batch 2288 batch loss 2.65772581 epoch total loss 3.07746267\n",
      "Trained batch 2289 batch loss 2.64249182 epoch total loss 3.07727289\n",
      "Trained batch 2290 batch loss 2.65758848 epoch total loss 3.07708955\n",
      "Trained batch 2291 batch loss 3.15489173 epoch total loss 3.0771234\n",
      "Trained batch 2292 batch loss 2.63874674 epoch total loss 3.07693219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2293 batch loss 2.82552242 epoch total loss 3.07682252\n",
      "Trained batch 2294 batch loss 2.68763185 epoch total loss 3.076653\n",
      "Trained batch 2295 batch loss 3.24044895 epoch total loss 3.07672429\n",
      "Trained batch 2296 batch loss 3.06936598 epoch total loss 3.07672095\n",
      "Trained batch 2297 batch loss 2.90611148 epoch total loss 3.0766468\n",
      "Trained batch 2298 batch loss 2.8114152 epoch total loss 3.07653141\n",
      "Trained batch 2299 batch loss 3.1301589 epoch total loss 3.07655478\n",
      "Trained batch 2300 batch loss 3.43704915 epoch total loss 3.07671142\n",
      "Trained batch 2301 batch loss 3.45300388 epoch total loss 3.07687521\n",
      "Trained batch 2302 batch loss 3.2970922 epoch total loss 3.07697058\n",
      "Trained batch 2303 batch loss 3.04711223 epoch total loss 3.0769577\n",
      "Trained batch 2304 batch loss 3.11603212 epoch total loss 3.07697463\n",
      "Trained batch 2305 batch loss 2.98680115 epoch total loss 3.07693553\n",
      "Trained batch 2306 batch loss 2.86957526 epoch total loss 3.07684565\n",
      "Trained batch 2307 batch loss 2.80559587 epoch total loss 3.07672811\n",
      "Trained batch 2308 batch loss 2.74643493 epoch total loss 3.07658505\n",
      "Trained batch 2309 batch loss 2.4832654 epoch total loss 3.07632804\n",
      "Trained batch 2310 batch loss 2.30041814 epoch total loss 3.07599211\n",
      "Trained batch 2311 batch loss 2.41942573 epoch total loss 3.07570815\n",
      "Trained batch 2312 batch loss 2.50011897 epoch total loss 3.075459\n",
      "Trained batch 2313 batch loss 2.66653442 epoch total loss 3.07528234\n",
      "Trained batch 2314 batch loss 2.81682539 epoch total loss 3.07517052\n",
      "Trained batch 2315 batch loss 2.66229725 epoch total loss 3.07499218\n",
      "Trained batch 2316 batch loss 2.58150959 epoch total loss 3.07477903\n",
      "Trained batch 2317 batch loss 2.66571689 epoch total loss 3.0746026\n",
      "Trained batch 2318 batch loss 2.85775852 epoch total loss 3.07450891\n",
      "Trained batch 2319 batch loss 2.88927388 epoch total loss 3.07442904\n",
      "Trained batch 2320 batch loss 2.98651552 epoch total loss 3.07439113\n",
      "Trained batch 2321 batch loss 3.00114274 epoch total loss 3.07435942\n",
      "Trained batch 2322 batch loss 2.9074502 epoch total loss 3.07428741\n",
      "Trained batch 2323 batch loss 2.781214 epoch total loss 3.07416129\n",
      "Trained batch 2324 batch loss 2.68264961 epoch total loss 3.07399297\n",
      "Trained batch 2325 batch loss 2.74577236 epoch total loss 3.07385159\n",
      "Trained batch 2326 batch loss 2.80263448 epoch total loss 3.073735\n",
      "Trained batch 2327 batch loss 2.62906671 epoch total loss 3.07354403\n",
      "Trained batch 2328 batch loss 2.77265477 epoch total loss 3.07341456\n",
      "Trained batch 2329 batch loss 2.89108825 epoch total loss 3.07333636\n",
      "Trained batch 2330 batch loss 2.88315797 epoch total loss 3.07325482\n",
      "Trained batch 2331 batch loss 2.93198037 epoch total loss 3.07319427\n",
      "Trained batch 2332 batch loss 3.08518219 epoch total loss 3.07319927\n",
      "Trained batch 2333 batch loss 2.38155103 epoch total loss 3.07290268\n",
      "Trained batch 2334 batch loss 2.22346115 epoch total loss 3.07253885\n",
      "Trained batch 2335 batch loss 2.14856315 epoch total loss 3.07214308\n",
      "Trained batch 2336 batch loss 2.38176155 epoch total loss 3.07184768\n",
      "Trained batch 2337 batch loss 1.90519166 epoch total loss 3.07134843\n",
      "Trained batch 2338 batch loss 2.36839366 epoch total loss 3.07104754\n",
      "Trained batch 2339 batch loss 2.98058033 epoch total loss 3.07100892\n",
      "Trained batch 2340 batch loss 3.01168513 epoch total loss 3.07098365\n",
      "Trained batch 2341 batch loss 3.04616189 epoch total loss 3.07097316\n",
      "Trained batch 2342 batch loss 3.34029531 epoch total loss 3.07108808\n",
      "Trained batch 2343 batch loss 3.22065473 epoch total loss 3.07115197\n",
      "Trained batch 2344 batch loss 3.25654244 epoch total loss 3.07123089\n",
      "Trained batch 2345 batch loss 3.22730756 epoch total loss 3.07129765\n",
      "Trained batch 2346 batch loss 3.17260242 epoch total loss 3.07134056\n",
      "Trained batch 2347 batch loss 2.97450662 epoch total loss 3.07129955\n",
      "Trained batch 2348 batch loss 2.92661524 epoch total loss 3.0712378\n",
      "Trained batch 2349 batch loss 3.13402629 epoch total loss 3.07126451\n",
      "Trained batch 2350 batch loss 2.79465032 epoch total loss 3.07114673\n",
      "Trained batch 2351 batch loss 3.01436138 epoch total loss 3.07112241\n",
      "Trained batch 2352 batch loss 2.71513414 epoch total loss 3.07097125\n",
      "Trained batch 2353 batch loss 2.59096241 epoch total loss 3.07076716\n",
      "Trained batch 2354 batch loss 2.69369483 epoch total loss 3.07060695\n",
      "Trained batch 2355 batch loss 2.64833403 epoch total loss 3.07042789\n",
      "Trained batch 2356 batch loss 2.79137111 epoch total loss 3.0703094\n",
      "Trained batch 2357 batch loss 2.88443637 epoch total loss 3.07023048\n",
      "Trained batch 2358 batch loss 2.82749033 epoch total loss 3.07012749\n",
      "Trained batch 2359 batch loss 2.83316469 epoch total loss 3.07002711\n",
      "Trained batch 2360 batch loss 2.73247576 epoch total loss 3.06988406\n",
      "Trained batch 2361 batch loss 2.84304929 epoch total loss 3.06978798\n",
      "Trained batch 2362 batch loss 2.66981125 epoch total loss 3.0696187\n",
      "Trained batch 2363 batch loss 2.72464132 epoch total loss 3.06947279\n",
      "Trained batch 2364 batch loss 2.52470255 epoch total loss 3.06924248\n",
      "Trained batch 2365 batch loss 2.8237772 epoch total loss 3.06913853\n",
      "Trained batch 2366 batch loss 2.79116082 epoch total loss 3.06902099\n",
      "Trained batch 2367 batch loss 2.95178223 epoch total loss 3.0689714\n",
      "Trained batch 2368 batch loss 3.09446478 epoch total loss 3.06898212\n",
      "Trained batch 2369 batch loss 2.72987461 epoch total loss 3.06883907\n",
      "Trained batch 2370 batch loss 2.72222614 epoch total loss 3.06869268\n",
      "Trained batch 2371 batch loss 2.99673271 epoch total loss 3.06866241\n",
      "Trained batch 2372 batch loss 3.21372867 epoch total loss 3.06872344\n",
      "Trained batch 2373 batch loss 3.17401242 epoch total loss 3.06876779\n",
      "Trained batch 2374 batch loss 2.5227108 epoch total loss 3.06853795\n",
      "Trained batch 2375 batch loss 2.39184713 epoch total loss 3.06825304\n",
      "Trained batch 2376 batch loss 2.63039136 epoch total loss 3.06806874\n",
      "Trained batch 2377 batch loss 2.56869721 epoch total loss 3.0678587\n",
      "Trained batch 2378 batch loss 2.77904177 epoch total loss 3.0677371\n",
      "Trained batch 2379 batch loss 2.73658633 epoch total loss 3.0675981\n",
      "Trained batch 2380 batch loss 2.98702765 epoch total loss 3.06756425\n",
      "Trained batch 2381 batch loss 2.82466102 epoch total loss 3.06746221\n",
      "Trained batch 2382 batch loss 3.1047852 epoch total loss 3.06747794\n",
      "Trained batch 2383 batch loss 3.50384855 epoch total loss 3.06766105\n",
      "Trained batch 2384 batch loss 3.13559365 epoch total loss 3.06768966\n",
      "Trained batch 2385 batch loss 2.853755 epoch total loss 3.06759977\n",
      "Trained batch 2386 batch loss 2.94645429 epoch total loss 3.06754899\n",
      "Trained batch 2387 batch loss 2.88757277 epoch total loss 3.06747365\n",
      "Trained batch 2388 batch loss 2.50521588 epoch total loss 3.06723833\n",
      "Trained batch 2389 batch loss 2.57706022 epoch total loss 3.06703305\n",
      "Trained batch 2390 batch loss 2.76220632 epoch total loss 3.0669055\n",
      "Trained batch 2391 batch loss 2.85107064 epoch total loss 3.06681538\n",
      "Trained batch 2392 batch loss 2.79917955 epoch total loss 3.06670356\n",
      "Trained batch 2393 batch loss 3.00300312 epoch total loss 3.06667686\n",
      "Trained batch 2394 batch loss 2.64843607 epoch total loss 3.06650209\n",
      "Trained batch 2395 batch loss 2.67877126 epoch total loss 3.06634021\n",
      "Trained batch 2396 batch loss 2.71635818 epoch total loss 3.06619406\n",
      "Trained batch 2397 batch loss 2.63531232 epoch total loss 3.06601429\n",
      "Trained batch 2398 batch loss 2.49107194 epoch total loss 3.06577468\n",
      "Trained batch 2399 batch loss 2.28932548 epoch total loss 3.06545115\n",
      "Trained batch 2400 batch loss 2.38675833 epoch total loss 3.06516814\n",
      "Trained batch 2401 batch loss 2.50684166 epoch total loss 3.06493568\n",
      "Trained batch 2402 batch loss 2.96643353 epoch total loss 3.06489468\n",
      "Trained batch 2403 batch loss 3.38895464 epoch total loss 3.06502962\n",
      "Trained batch 2404 batch loss 3.74868464 epoch total loss 3.06531382\n",
      "Trained batch 2405 batch loss 2.81029391 epoch total loss 3.06520772\n",
      "Trained batch 2406 batch loss 3.02275562 epoch total loss 3.06519032\n",
      "Trained batch 2407 batch loss 2.94273639 epoch total loss 3.06513929\n",
      "Trained batch 2408 batch loss 2.8202529 epoch total loss 3.06503773\n",
      "Trained batch 2409 batch loss 2.73479033 epoch total loss 3.06490064\n",
      "Trained batch 2410 batch loss 2.82603908 epoch total loss 3.06480169\n",
      "Trained batch 2411 batch loss 3.02254272 epoch total loss 3.06478405\n",
      "Trained batch 2412 batch loss 3.04745936 epoch total loss 3.0647769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2413 batch loss 2.79837036 epoch total loss 3.06466651\n",
      "Trained batch 2414 batch loss 2.74125624 epoch total loss 3.06453252\n",
      "Trained batch 2415 batch loss 2.80377674 epoch total loss 3.06442451\n",
      "Trained batch 2416 batch loss 2.73727 epoch total loss 3.06428909\n",
      "Trained batch 2417 batch loss 2.7898531 epoch total loss 3.06417561\n",
      "Trained batch 2418 batch loss 2.83251143 epoch total loss 3.06407976\n",
      "Trained batch 2419 batch loss 2.85231686 epoch total loss 3.06399226\n",
      "Trained batch 2420 batch loss 2.84981823 epoch total loss 3.06390381\n",
      "Trained batch 2421 batch loss 2.87270784 epoch total loss 3.06382465\n",
      "Trained batch 2422 batch loss 2.91906714 epoch total loss 3.06376481\n",
      "Trained batch 2423 batch loss 2.82850409 epoch total loss 3.06366777\n",
      "Trained batch 2424 batch loss 2.77844095 epoch total loss 3.06355\n",
      "Trained batch 2425 batch loss 2.86215067 epoch total loss 3.06346703\n",
      "Trained batch 2426 batch loss 3.01690388 epoch total loss 3.06344795\n",
      "Trained batch 2427 batch loss 2.68616819 epoch total loss 3.0632925\n",
      "Trained batch 2428 batch loss 2.89608526 epoch total loss 3.0632236\n",
      "Trained batch 2429 batch loss 2.62041855 epoch total loss 3.06304145\n",
      "Trained batch 2430 batch loss 2.87551212 epoch total loss 3.0629642\n",
      "Trained batch 2431 batch loss 2.65596294 epoch total loss 3.06279659\n",
      "Trained batch 2432 batch loss 2.81919813 epoch total loss 3.06269646\n",
      "Trained batch 2433 batch loss 2.68057775 epoch total loss 3.06253958\n",
      "Trained batch 2434 batch loss 2.84579849 epoch total loss 3.06245041\n",
      "Trained batch 2435 batch loss 2.87538099 epoch total loss 3.06237364\n",
      "Trained batch 2436 batch loss 2.78708863 epoch total loss 3.06226063\n",
      "Trained batch 2437 batch loss 2.96418548 epoch total loss 3.06222057\n",
      "Trained batch 2438 batch loss 2.95306706 epoch total loss 3.06217575\n",
      "Trained batch 2439 batch loss 2.49295378 epoch total loss 3.06194234\n",
      "Trained batch 2440 batch loss 2.7237711 epoch total loss 3.06180382\n",
      "Trained batch 2441 batch loss 3.02194 epoch total loss 3.06178737\n",
      "Trained batch 2442 batch loss 2.67507172 epoch total loss 3.0616293\n",
      "Trained batch 2443 batch loss 2.72616315 epoch total loss 3.06149197\n",
      "Trained batch 2444 batch loss 2.58070254 epoch total loss 3.06129503\n",
      "Trained batch 2445 batch loss 2.37970209 epoch total loss 3.06101632\n",
      "Trained batch 2446 batch loss 2.45778036 epoch total loss 3.0607698\n",
      "Trained batch 2447 batch loss 2.57541 epoch total loss 3.06057143\n",
      "Trained batch 2448 batch loss 2.72495461 epoch total loss 3.06043434\n",
      "Trained batch 2449 batch loss 2.49093 epoch total loss 3.06020164\n",
      "Trained batch 2450 batch loss 2.63184214 epoch total loss 3.06002688\n",
      "Trained batch 2451 batch loss 2.51558733 epoch total loss 3.05980468\n",
      "Trained batch 2452 batch loss 2.70141625 epoch total loss 3.05965877\n",
      "Trained batch 2453 batch loss 2.81043196 epoch total loss 3.0595572\n",
      "Trained batch 2454 batch loss 2.75627661 epoch total loss 3.0594337\n",
      "Trained batch 2455 batch loss 2.53508782 epoch total loss 3.05922\n",
      "Trained batch 2456 batch loss 2.75647807 epoch total loss 3.05909681\n",
      "Trained batch 2457 batch loss 2.71225786 epoch total loss 3.05895567\n",
      "Trained batch 2458 batch loss 2.52675462 epoch total loss 3.05873919\n",
      "Trained batch 2459 batch loss 2.68187 epoch total loss 3.05858588\n",
      "Trained batch 2460 batch loss 2.96889424 epoch total loss 3.0585494\n",
      "Trained batch 2461 batch loss 2.83184242 epoch total loss 3.05845714\n",
      "Trained batch 2462 batch loss 3.0275991 epoch total loss 3.05844474\n",
      "Trained batch 2463 batch loss 3.24801517 epoch total loss 3.05852175\n",
      "Trained batch 2464 batch loss 2.55108762 epoch total loss 3.05831599\n",
      "Trained batch 2465 batch loss 2.53363323 epoch total loss 3.05810308\n",
      "Trained batch 2466 batch loss 2.57657 epoch total loss 3.05790782\n",
      "Trained batch 2467 batch loss 3.07278037 epoch total loss 3.05791378\n",
      "Trained batch 2468 batch loss 2.84844422 epoch total loss 3.05782914\n",
      "Trained batch 2469 batch loss 3.02258348 epoch total loss 3.05781484\n",
      "Trained batch 2470 batch loss 2.92276597 epoch total loss 3.05776\n",
      "Trained batch 2471 batch loss 2.66913414 epoch total loss 3.05760264\n",
      "Trained batch 2472 batch loss 2.65097976 epoch total loss 3.05743814\n",
      "Trained batch 2473 batch loss 2.75971055 epoch total loss 3.05731797\n",
      "Trained batch 2474 batch loss 2.78772593 epoch total loss 3.05720878\n",
      "Trained batch 2475 batch loss 3.37658978 epoch total loss 3.05733776\n",
      "Trained batch 2476 batch loss 3.18563294 epoch total loss 3.0573895\n",
      "Trained batch 2477 batch loss 2.67216015 epoch total loss 3.05723429\n",
      "Trained batch 2478 batch loss 3.26186252 epoch total loss 3.05731678\n",
      "Trained batch 2479 batch loss 2.61041498 epoch total loss 3.0571363\n",
      "Trained batch 2480 batch loss 3.0726862 epoch total loss 3.05714273\n",
      "Trained batch 2481 batch loss 2.89475465 epoch total loss 3.05707717\n",
      "Trained batch 2482 batch loss 3.24478722 epoch total loss 3.05715275\n",
      "Trained batch 2483 batch loss 3.0353322 epoch total loss 3.05714393\n",
      "Trained batch 2484 batch loss 2.87678909 epoch total loss 3.05707121\n",
      "Trained batch 2485 batch loss 2.90317202 epoch total loss 3.05700946\n",
      "Trained batch 2486 batch loss 3.0214653 epoch total loss 3.05699515\n",
      "Trained batch 2487 batch loss 2.8801322 epoch total loss 3.0569241\n",
      "Trained batch 2488 batch loss 2.91244268 epoch total loss 3.05686617\n",
      "Trained batch 2489 batch loss 3.23215532 epoch total loss 3.0569365\n",
      "Trained batch 2490 batch loss 2.9134922 epoch total loss 3.05687881\n",
      "Trained batch 2491 batch loss 2.79803348 epoch total loss 3.05677485\n",
      "Trained batch 2492 batch loss 2.74716592 epoch total loss 3.05665064\n",
      "Trained batch 2493 batch loss 2.65800023 epoch total loss 3.0564909\n",
      "Trained batch 2494 batch loss 2.83632 epoch total loss 3.05640244\n",
      "Trained batch 2495 batch loss 2.6893425 epoch total loss 3.05625558\n",
      "Trained batch 2496 batch loss 3.04659 epoch total loss 3.05625153\n",
      "Trained batch 2497 batch loss 3.11964893 epoch total loss 3.0562768\n",
      "Trained batch 2498 batch loss 3.28578854 epoch total loss 3.05636883\n",
      "Trained batch 2499 batch loss 3.13780642 epoch total loss 3.05640125\n",
      "Trained batch 2500 batch loss 2.97380972 epoch total loss 3.05636811\n",
      "Trained batch 2501 batch loss 2.8099947 epoch total loss 3.05626965\n",
      "Trained batch 2502 batch loss 3.0037756 epoch total loss 3.05624866\n",
      "Trained batch 2503 batch loss 2.80556655 epoch total loss 3.05614853\n",
      "Trained batch 2504 batch loss 2.62831926 epoch total loss 3.05597782\n",
      "Trained batch 2505 batch loss 3.02768445 epoch total loss 3.05596662\n",
      "Trained batch 2506 batch loss 3.07854605 epoch total loss 3.05597568\n",
      "Trained batch 2507 batch loss 3.09135103 epoch total loss 3.05598974\n",
      "Trained batch 2508 batch loss 2.73649216 epoch total loss 3.05586219\n",
      "Trained batch 2509 batch loss 2.91895103 epoch total loss 3.05580759\n",
      "Trained batch 2510 batch loss 2.88848591 epoch total loss 3.05574107\n",
      "Trained batch 2511 batch loss 2.98618698 epoch total loss 3.05571342\n",
      "Trained batch 2512 batch loss 3.0024538 epoch total loss 3.0556922\n",
      "Trained batch 2513 batch loss 3.31603622 epoch total loss 3.05579591\n",
      "Trained batch 2514 batch loss 2.86815763 epoch total loss 3.05572128\n",
      "Trained batch 2515 batch loss 2.95563579 epoch total loss 3.05568123\n",
      "Trained batch 2516 batch loss 2.9619627 epoch total loss 3.05564404\n",
      "Trained batch 2517 batch loss 2.46706152 epoch total loss 3.05541039\n",
      "Trained batch 2518 batch loss 2.50013709 epoch total loss 3.05518985\n",
      "Trained batch 2519 batch loss 2.50561619 epoch total loss 3.05497169\n",
      "Trained batch 2520 batch loss 2.29827452 epoch total loss 3.05467153\n",
      "Trained batch 2521 batch loss 2.41485453 epoch total loss 3.05441761\n",
      "Trained batch 2522 batch loss 2.16543126 epoch total loss 3.05406523\n",
      "Trained batch 2523 batch loss 2.4952724 epoch total loss 3.05384374\n",
      "Trained batch 2524 batch loss 2.83041883 epoch total loss 3.05375528\n",
      "Trained batch 2525 batch loss 3.47097445 epoch total loss 3.05392051\n",
      "Trained batch 2526 batch loss 3.24778867 epoch total loss 3.05399728\n",
      "Trained batch 2527 batch loss 3.08559012 epoch total loss 3.05400968\n",
      "Trained batch 2528 batch loss 3.18609691 epoch total loss 3.05406189\n",
      "Trained batch 2529 batch loss 3.01806712 epoch total loss 3.05404758\n",
      "Trained batch 2530 batch loss 2.92993212 epoch total loss 3.05399871\n",
      "Trained batch 2531 batch loss 2.88661909 epoch total loss 3.05393267\n",
      "Trained batch 2532 batch loss 3.03733706 epoch total loss 3.05392599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2533 batch loss 3.18783 epoch total loss 3.05397892\n",
      "Trained batch 2534 batch loss 3.0526967 epoch total loss 3.05397844\n",
      "Trained batch 2535 batch loss 2.88047171 epoch total loss 3.05391\n",
      "Trained batch 2536 batch loss 2.77614665 epoch total loss 3.05380058\n",
      "Trained batch 2537 batch loss 2.59236097 epoch total loss 3.05361867\n",
      "Trained batch 2538 batch loss 2.38361216 epoch total loss 3.05335474\n",
      "Trained batch 2539 batch loss 2.79575872 epoch total loss 3.05325317\n",
      "Trained batch 2540 batch loss 2.81816864 epoch total loss 3.05316067\n",
      "Trained batch 2541 batch loss 2.93957853 epoch total loss 3.05311608\n",
      "Trained batch 2542 batch loss 3.0212822 epoch total loss 3.05310345\n",
      "Trained batch 2543 batch loss 2.83530569 epoch total loss 3.05301785\n",
      "Trained batch 2544 batch loss 2.73015046 epoch total loss 3.05289102\n",
      "Trained batch 2545 batch loss 2.98443365 epoch total loss 3.05286407\n",
      "Trained batch 2546 batch loss 2.99027586 epoch total loss 3.05283952\n",
      "Trained batch 2547 batch loss 2.9483614 epoch total loss 3.05279851\n",
      "Trained batch 2548 batch loss 3.26113558 epoch total loss 3.05288029\n",
      "Trained batch 2549 batch loss 2.68976402 epoch total loss 3.05273771\n",
      "Trained batch 2550 batch loss 2.82379293 epoch total loss 3.05264807\n",
      "Trained batch 2551 batch loss 3.03248692 epoch total loss 3.0526402\n",
      "Trained batch 2552 batch loss 3.25218797 epoch total loss 3.0527184\n",
      "Trained batch 2553 batch loss 2.83064365 epoch total loss 3.05263138\n",
      "Trained batch 2554 batch loss 2.85611677 epoch total loss 3.05255437\n",
      "Trained batch 2555 batch loss 3.0793395 epoch total loss 3.05256462\n",
      "Trained batch 2556 batch loss 3.32776642 epoch total loss 3.05267239\n",
      "Trained batch 2557 batch loss 3.30543828 epoch total loss 3.05277133\n",
      "Trained batch 2558 batch loss 2.84273529 epoch total loss 3.05268908\n",
      "Trained batch 2559 batch loss 2.88488388 epoch total loss 3.05262351\n",
      "Trained batch 2560 batch loss 3.06798267 epoch total loss 3.05262947\n",
      "Trained batch 2561 batch loss 2.96506548 epoch total loss 3.05259514\n",
      "Trained batch 2562 batch loss 2.95959973 epoch total loss 3.0525589\n",
      "Trained batch 2563 batch loss 2.97816086 epoch total loss 3.05252981\n",
      "Trained batch 2564 batch loss 2.9911561 epoch total loss 3.05250573\n",
      "Trained batch 2565 batch loss 2.8968749 epoch total loss 3.05244517\n",
      "Trained batch 2566 batch loss 2.74261212 epoch total loss 3.05232453\n",
      "Trained batch 2567 batch loss 3.07790637 epoch total loss 3.05233455\n",
      "Trained batch 2568 batch loss 2.84768248 epoch total loss 3.05225492\n",
      "Trained batch 2569 batch loss 2.83750701 epoch total loss 3.05217123\n",
      "Trained batch 2570 batch loss 2.70697761 epoch total loss 3.052037\n",
      "Trained batch 2571 batch loss 2.72158194 epoch total loss 3.05190849\n",
      "Trained batch 2572 batch loss 2.73619294 epoch total loss 3.05178571\n",
      "Trained batch 2573 batch loss 2.76475215 epoch total loss 3.05167413\n",
      "Trained batch 2574 batch loss 2.65974569 epoch total loss 3.05152178\n",
      "Trained batch 2575 batch loss 2.73336482 epoch total loss 3.05139828\n",
      "Trained batch 2576 batch loss 3.04861569 epoch total loss 3.05139732\n",
      "Trained batch 2577 batch loss 2.95943952 epoch total loss 3.05136156\n",
      "Trained batch 2578 batch loss 2.81843615 epoch total loss 3.0512712\n",
      "Trained batch 2579 batch loss 2.83160067 epoch total loss 3.05118608\n",
      "Trained batch 2580 batch loss 2.8953867 epoch total loss 3.05112576\n",
      "Trained batch 2581 batch loss 2.84971476 epoch total loss 3.05104756\n",
      "Trained batch 2582 batch loss 2.74371839 epoch total loss 3.05092859\n",
      "Trained batch 2583 batch loss 2.74342179 epoch total loss 3.05080962\n",
      "Trained batch 2584 batch loss 2.94916868 epoch total loss 3.05077028\n",
      "Trained batch 2585 batch loss 2.79386234 epoch total loss 3.05067086\n",
      "Trained batch 2586 batch loss 3.12556911 epoch total loss 3.0507\n",
      "Trained batch 2587 batch loss 2.91970277 epoch total loss 3.0506494\n",
      "Trained batch 2588 batch loss 2.95858741 epoch total loss 3.05061364\n",
      "Trained batch 2589 batch loss 2.83300138 epoch total loss 3.05052972\n",
      "Trained batch 2590 batch loss 3.12601471 epoch total loss 3.05055881\n",
      "Trained batch 2591 batch loss 3.04007673 epoch total loss 3.05055475\n",
      "Trained batch 2592 batch loss 3.05945134 epoch total loss 3.05055809\n",
      "Trained batch 2593 batch loss 2.76452255 epoch total loss 3.05044794\n",
      "Trained batch 2594 batch loss 2.84372497 epoch total loss 3.05036831\n",
      "Trained batch 2595 batch loss 2.65108514 epoch total loss 3.05021429\n",
      "Trained batch 2596 batch loss 2.94908237 epoch total loss 3.05017543\n",
      "Trained batch 2597 batch loss 2.58893776 epoch total loss 3.04999781\n",
      "Trained batch 2598 batch loss 2.91741371 epoch total loss 3.04994678\n",
      "Trained batch 2599 batch loss 2.75249839 epoch total loss 3.04983234\n",
      "Trained batch 2600 batch loss 2.72591662 epoch total loss 3.04970789\n",
      "Trained batch 2601 batch loss 2.6990037 epoch total loss 3.04957294\n",
      "Trained batch 2602 batch loss 2.635499 epoch total loss 3.04941392\n",
      "Trained batch 2603 batch loss 2.47585106 epoch total loss 3.04919362\n",
      "Trained batch 2604 batch loss 2.57145071 epoch total loss 3.04901028\n",
      "Trained batch 2605 batch loss 2.75807595 epoch total loss 3.0488987\n",
      "Trained batch 2606 batch loss 2.65992808 epoch total loss 3.04874945\n",
      "Trained batch 2607 batch loss 3.05140352 epoch total loss 3.0487504\n",
      "Trained batch 2608 batch loss 2.70255804 epoch total loss 3.0486176\n",
      "Trained batch 2609 batch loss 2.94785643 epoch total loss 3.04857898\n",
      "Trained batch 2610 batch loss 2.90300155 epoch total loss 3.04852319\n",
      "Trained batch 2611 batch loss 2.55734777 epoch total loss 3.04833508\n",
      "Trained batch 2612 batch loss 2.75474024 epoch total loss 3.04822254\n",
      "Trained batch 2613 batch loss 2.74807525 epoch total loss 3.04810786\n",
      "Trained batch 2614 batch loss 2.72762585 epoch total loss 3.04798508\n",
      "Trained batch 2615 batch loss 2.69909263 epoch total loss 3.0478518\n",
      "Trained batch 2616 batch loss 2.95826244 epoch total loss 3.04781771\n",
      "Trained batch 2617 batch loss 2.57522798 epoch total loss 3.04763699\n",
      "Trained batch 2618 batch loss 2.45382261 epoch total loss 3.04741\n",
      "Trained batch 2619 batch loss 2.29895377 epoch total loss 3.04712439\n",
      "Trained batch 2620 batch loss 2.86984754 epoch total loss 3.04705644\n",
      "Trained batch 2621 batch loss 2.71576142 epoch total loss 3.04693\n",
      "Trained batch 2622 batch loss 3.30848074 epoch total loss 3.04703\n",
      "Trained batch 2623 batch loss 3.14591694 epoch total loss 3.04706764\n",
      "Trained batch 2624 batch loss 3.02990818 epoch total loss 3.04706097\n",
      "Trained batch 2625 batch loss 2.98352242 epoch total loss 3.04703689\n",
      "Trained batch 2626 batch loss 2.74225068 epoch total loss 3.04692078\n",
      "Trained batch 2627 batch loss 2.61074734 epoch total loss 3.04675484\n",
      "Trained batch 2628 batch loss 2.51389599 epoch total loss 3.04655194\n",
      "Trained batch 2629 batch loss 2.60777235 epoch total loss 3.04638505\n",
      "Trained batch 2630 batch loss 2.63087702 epoch total loss 3.04622698\n",
      "Trained batch 2631 batch loss 2.62138414 epoch total loss 3.04606557\n",
      "Trained batch 2632 batch loss 2.54539347 epoch total loss 3.04587555\n",
      "Trained batch 2633 batch loss 2.59403348 epoch total loss 3.04570389\n",
      "Trained batch 2634 batch loss 2.45694017 epoch total loss 3.04548049\n",
      "Trained batch 2635 batch loss 2.55047512 epoch total loss 3.04529238\n",
      "Trained batch 2636 batch loss 2.54767942 epoch total loss 3.04510379\n",
      "Trained batch 2637 batch loss 2.48367095 epoch total loss 3.04489088\n",
      "Trained batch 2638 batch loss 2.37343884 epoch total loss 3.04463649\n",
      "Trained batch 2639 batch loss 2.59904718 epoch total loss 3.04446769\n",
      "Trained batch 2640 batch loss 2.77252889 epoch total loss 3.04436469\n",
      "Trained batch 2641 batch loss 3.03050709 epoch total loss 3.04435921\n",
      "Trained batch 2642 batch loss 2.74321961 epoch total loss 3.04424524\n",
      "Trained batch 2643 batch loss 2.7117517 epoch total loss 3.0441196\n",
      "Trained batch 2644 batch loss 2.77334452 epoch total loss 3.04401708\n",
      "Trained batch 2645 batch loss 2.95482063 epoch total loss 3.04398346\n",
      "Trained batch 2646 batch loss 2.9377923 epoch total loss 3.04394317\n",
      "Trained batch 2647 batch loss 2.76148796 epoch total loss 3.04383659\n",
      "Trained batch 2648 batch loss 2.77465 epoch total loss 3.04373503\n",
      "Trained batch 2649 batch loss 2.84931207 epoch total loss 3.04366136\n",
      "Trained batch 2650 batch loss 2.66610217 epoch total loss 3.04351902\n",
      "Trained batch 2651 batch loss 2.77517366 epoch total loss 3.04341769\n",
      "Trained batch 2652 batch loss 2.88122368 epoch total loss 3.04335666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2653 batch loss 2.82947922 epoch total loss 3.04327607\n",
      "Trained batch 2654 batch loss 3.04703236 epoch total loss 3.0432775\n",
      "Trained batch 2655 batch loss 2.88166428 epoch total loss 3.04321671\n",
      "Trained batch 2656 batch loss 2.7746613 epoch total loss 3.04311562\n",
      "Trained batch 2657 batch loss 2.65577769 epoch total loss 3.04297\n",
      "Trained batch 2658 batch loss 2.73132944 epoch total loss 3.04285264\n",
      "Trained batch 2659 batch loss 2.90451169 epoch total loss 3.04280043\n",
      "Trained batch 2660 batch loss 3.1045568 epoch total loss 3.04282379\n",
      "Trained batch 2661 batch loss 3.07062364 epoch total loss 3.04283428\n",
      "Trained batch 2662 batch loss 2.95573902 epoch total loss 3.04280138\n",
      "Trained batch 2663 batch loss 2.50959206 epoch total loss 3.04260135\n",
      "Trained batch 2664 batch loss 2.90269089 epoch total loss 3.04254889\n",
      "Trained batch 2665 batch loss 2.96047735 epoch total loss 3.04251814\n",
      "Trained batch 2666 batch loss 2.69022179 epoch total loss 3.04238605\n",
      "Trained batch 2667 batch loss 2.66941619 epoch total loss 3.0422461\n",
      "Trained batch 2668 batch loss 2.73378682 epoch total loss 3.04213047\n",
      "Trained batch 2669 batch loss 2.92411399 epoch total loss 3.04208636\n",
      "Trained batch 2670 batch loss 2.66528559 epoch total loss 3.04194546\n",
      "Trained batch 2671 batch loss 2.49168015 epoch total loss 3.04173946\n",
      "Trained batch 2672 batch loss 2.74688697 epoch total loss 3.04162908\n",
      "Trained batch 2673 batch loss 2.6612308 epoch total loss 3.04148674\n",
      "Trained batch 2674 batch loss 2.81941175 epoch total loss 3.04140377\n",
      "Trained batch 2675 batch loss 2.79308176 epoch total loss 3.04131079\n",
      "Trained batch 2676 batch loss 2.97263288 epoch total loss 3.04128504\n",
      "Trained batch 2677 batch loss 3.18744063 epoch total loss 3.04133964\n",
      "Trained batch 2678 batch loss 3.12014627 epoch total loss 3.0413692\n",
      "Trained batch 2679 batch loss 2.96557379 epoch total loss 3.04134083\n",
      "Trained batch 2680 batch loss 3.24070597 epoch total loss 3.04141521\n",
      "Trained batch 2681 batch loss 3.37529707 epoch total loss 3.04153967\n",
      "Trained batch 2682 batch loss 3.26415873 epoch total loss 3.04162288\n",
      "Trained batch 2683 batch loss 2.87362552 epoch total loss 3.04156017\n",
      "Trained batch 2684 batch loss 3.31852961 epoch total loss 3.04166317\n",
      "Trained batch 2685 batch loss 2.70829535 epoch total loss 3.04153919\n",
      "Trained batch 2686 batch loss 2.73722386 epoch total loss 3.04142594\n",
      "Trained batch 2687 batch loss 2.55425239 epoch total loss 3.04124451\n",
      "Trained batch 2688 batch loss 2.62807775 epoch total loss 3.04109073\n",
      "Trained batch 2689 batch loss 2.44872427 epoch total loss 3.04087043\n",
      "Trained batch 2690 batch loss 2.3015914 epoch total loss 3.04059577\n",
      "Trained batch 2691 batch loss 2.12377357 epoch total loss 3.04025507\n",
      "Trained batch 2692 batch loss 2.3141861 epoch total loss 3.03998518\n",
      "Trained batch 2693 batch loss 2.38230181 epoch total loss 3.03974104\n",
      "Trained batch 2694 batch loss 2.66015196 epoch total loss 3.03960013\n",
      "Trained batch 2695 batch loss 2.53638363 epoch total loss 3.03941345\n",
      "Trained batch 2696 batch loss 2.65803766 epoch total loss 3.03927183\n",
      "Trained batch 2697 batch loss 2.87462044 epoch total loss 3.03921103\n",
      "Trained batch 2698 batch loss 2.71135926 epoch total loss 3.0390892\n",
      "Trained batch 2699 batch loss 2.40123701 epoch total loss 3.03885293\n",
      "Trained batch 2700 batch loss 2.66292834 epoch total loss 3.03871393\n",
      "Trained batch 2701 batch loss 3.00352955 epoch total loss 3.03870106\n",
      "Trained batch 2702 batch loss 2.83119 epoch total loss 3.03862405\n",
      "Trained batch 2703 batch loss 2.55505061 epoch total loss 3.038445\n",
      "Trained batch 2704 batch loss 2.66402602 epoch total loss 3.03830671\n",
      "Trained batch 2705 batch loss 2.70195866 epoch total loss 3.03818226\n",
      "Trained batch 2706 batch loss 2.67583203 epoch total loss 3.03804851\n",
      "Trained batch 2707 batch loss 2.43403912 epoch total loss 3.03782511\n",
      "Trained batch 2708 batch loss 2.50235271 epoch total loss 3.03762722\n",
      "Trained batch 2709 batch loss 2.56526804 epoch total loss 3.03745294\n",
      "Trained batch 2710 batch loss 2.83156824 epoch total loss 3.03737712\n",
      "Trained batch 2711 batch loss 2.75899386 epoch total loss 3.03727436\n",
      "Trained batch 2712 batch loss 2.73497486 epoch total loss 3.03716302\n",
      "Trained batch 2713 batch loss 2.77346969 epoch total loss 3.03706574\n",
      "Trained batch 2714 batch loss 3.08988786 epoch total loss 3.03708529\n",
      "Trained batch 2715 batch loss 2.87122846 epoch total loss 3.03702402\n",
      "Trained batch 2716 batch loss 2.57821727 epoch total loss 3.03685522\n",
      "Trained batch 2717 batch loss 2.80689478 epoch total loss 3.03677034\n",
      "Trained batch 2718 batch loss 2.530375 epoch total loss 3.03658414\n",
      "Trained batch 2719 batch loss 2.52020574 epoch total loss 3.03639436\n",
      "Trained batch 2720 batch loss 2.54479909 epoch total loss 3.03621364\n",
      "Trained batch 2721 batch loss 3.02445126 epoch total loss 3.03620934\n",
      "Trained batch 2722 batch loss 2.77183175 epoch total loss 3.03611207\n",
      "Trained batch 2723 batch loss 2.94737267 epoch total loss 3.03607941\n",
      "Trained batch 2724 batch loss 2.92825747 epoch total loss 3.03604\n",
      "Trained batch 2725 batch loss 2.75736547 epoch total loss 3.03593779\n",
      "Trained batch 2726 batch loss 2.85737658 epoch total loss 3.03587246\n",
      "Trained batch 2727 batch loss 2.87681818 epoch total loss 3.03581405\n",
      "Trained batch 2728 batch loss 2.85748315 epoch total loss 3.03574872\n",
      "Trained batch 2729 batch loss 2.84308314 epoch total loss 3.03567791\n",
      "Trained batch 2730 batch loss 2.46593714 epoch total loss 3.03546929\n",
      "Trained batch 2731 batch loss 2.53081608 epoch total loss 3.03528452\n",
      "Trained batch 2732 batch loss 2.76439404 epoch total loss 3.03518558\n",
      "Trained batch 2733 batch loss 2.5623157 epoch total loss 3.03501272\n",
      "Trained batch 2734 batch loss 2.9236083 epoch total loss 3.03497195\n",
      "Trained batch 2735 batch loss 2.74367523 epoch total loss 3.03486562\n",
      "Trained batch 2736 batch loss 2.51221132 epoch total loss 3.03467464\n",
      "Trained batch 2737 batch loss 2.66705966 epoch total loss 3.03454041\n",
      "Trained batch 2738 batch loss 2.89684248 epoch total loss 3.03449\n",
      "Trained batch 2739 batch loss 2.84717083 epoch total loss 3.03442168\n",
      "Trained batch 2740 batch loss 3.14247656 epoch total loss 3.03446126\n",
      "Trained batch 2741 batch loss 3.20293808 epoch total loss 3.03452277\n",
      "Trained batch 2742 batch loss 3.05387259 epoch total loss 3.03452969\n",
      "Trained batch 2743 batch loss 2.94381189 epoch total loss 3.03449655\n",
      "Trained batch 2744 batch loss 2.84042788 epoch total loss 3.03442597\n",
      "Trained batch 2745 batch loss 2.85852766 epoch total loss 3.03436184\n",
      "Trained batch 2746 batch loss 2.44647121 epoch total loss 3.03414774\n",
      "Trained batch 2747 batch loss 2.92220664 epoch total loss 3.03410673\n",
      "Trained batch 2748 batch loss 2.71314859 epoch total loss 3.03399\n",
      "Trained batch 2749 batch loss 2.77674055 epoch total loss 3.03389621\n",
      "Trained batch 2750 batch loss 2.83864737 epoch total loss 3.0338254\n",
      "Trained batch 2751 batch loss 2.84245801 epoch total loss 3.03375578\n",
      "Trained batch 2752 batch loss 2.65014982 epoch total loss 3.03361654\n",
      "Trained batch 2753 batch loss 2.62268543 epoch total loss 3.03346729\n",
      "Trained batch 2754 batch loss 2.78571081 epoch total loss 3.03337765\n",
      "Trained batch 2755 batch loss 2.65739131 epoch total loss 3.03324103\n",
      "Trained batch 2756 batch loss 3.14436388 epoch total loss 3.03328133\n",
      "Trained batch 2757 batch loss 3.06922674 epoch total loss 3.03329444\n",
      "Trained batch 2758 batch loss 3.00954223 epoch total loss 3.03328609\n",
      "Trained batch 2759 batch loss 2.74062896 epoch total loss 3.03317976\n",
      "Trained batch 2760 batch loss 2.58980107 epoch total loss 3.03301907\n",
      "Trained batch 2761 batch loss 2.67807674 epoch total loss 3.03289056\n",
      "Trained batch 2762 batch loss 2.89414573 epoch total loss 3.03284025\n",
      "Trained batch 2763 batch loss 3.00912642 epoch total loss 3.03283167\n",
      "Trained batch 2764 batch loss 2.87919116 epoch total loss 3.03277588\n",
      "Trained batch 2765 batch loss 2.65508056 epoch total loss 3.0326395\n",
      "Trained batch 2766 batch loss 2.45232749 epoch total loss 3.03242946\n",
      "Trained batch 2767 batch loss 2.80401754 epoch total loss 3.03234696\n",
      "Trained batch 2768 batch loss 2.78798509 epoch total loss 3.03225875\n",
      "Trained batch 2769 batch loss 2.80679512 epoch total loss 3.03217721\n",
      "Trained batch 2770 batch loss 2.82769728 epoch total loss 3.03210354\n",
      "Trained batch 2771 batch loss 2.68141866 epoch total loss 3.03197694\n",
      "Trained batch 2772 batch loss 2.75899529 epoch total loss 3.03187847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2773 batch loss 2.62767076 epoch total loss 3.0317328\n",
      "Trained batch 2774 batch loss 2.4261539 epoch total loss 3.03151441\n",
      "Trained batch 2775 batch loss 2.79421186 epoch total loss 3.03142881\n",
      "Trained batch 2776 batch loss 2.58395314 epoch total loss 3.03126764\n",
      "Trained batch 2777 batch loss 2.70122385 epoch total loss 3.03114867\n",
      "Trained batch 2778 batch loss 2.5563736 epoch total loss 3.03097796\n",
      "Trained batch 2779 batch loss 2.94666624 epoch total loss 3.03094745\n",
      "Trained batch 2780 batch loss 2.95742941 epoch total loss 3.03092074\n",
      "Trained batch 2781 batch loss 2.87594151 epoch total loss 3.03086519\n",
      "Epoch 1 train loss 3.03086519241333\n",
      "Validated batch 1 batch loss 3.09293604\n",
      "Validated batch 2 batch loss 2.89222503\n",
      "Validated batch 3 batch loss 2.56747866\n",
      "Validated batch 4 batch loss 2.98709846\n",
      "Validated batch 5 batch loss 2.67098141\n",
      "Validated batch 6 batch loss 2.68826628\n",
      "Validated batch 7 batch loss 2.56337357\n",
      "Validated batch 8 batch loss 2.72763824\n",
      "Validated batch 9 batch loss 2.43733025\n",
      "Validated batch 10 batch loss 2.79129457\n",
      "Validated batch 11 batch loss 2.97306252\n",
      "Validated batch 12 batch loss 2.58424735\n",
      "Validated batch 13 batch loss 2.88274503\n",
      "Validated batch 14 batch loss 2.84494114\n",
      "Validated batch 15 batch loss 2.60260487\n",
      "Validated batch 16 batch loss 2.60107541\n",
      "Validated batch 17 batch loss 2.56278419\n",
      "Validated batch 18 batch loss 2.98608446\n",
      "Validated batch 19 batch loss 2.58631635\n",
      "Validated batch 20 batch loss 2.73001695\n",
      "Validated batch 21 batch loss 2.63370585\n",
      "Validated batch 22 batch loss 2.9420743\n",
      "Validated batch 23 batch loss 2.78260946\n",
      "Validated batch 24 batch loss 2.68804884\n",
      "Validated batch 25 batch loss 2.95759\n",
      "Validated batch 26 batch loss 2.64344907\n",
      "Validated batch 27 batch loss 2.58566332\n",
      "Validated batch 28 batch loss 2.87209249\n",
      "Validated batch 29 batch loss 2.76404524\n",
      "Validated batch 30 batch loss 2.59221363\n",
      "Validated batch 31 batch loss 2.84752154\n",
      "Validated batch 32 batch loss 2.79779339\n",
      "Validated batch 33 batch loss 2.78575492\n",
      "Validated batch 34 batch loss 2.75757217\n",
      "Validated batch 35 batch loss 2.97965717\n",
      "Validated batch 36 batch loss 2.84385848\n",
      "Validated batch 37 batch loss 2.90395808\n",
      "Validated batch 38 batch loss 3.10380483\n",
      "Validated batch 39 batch loss 3.23944902\n",
      "Validated batch 40 batch loss 3.38768768\n",
      "Validated batch 41 batch loss 2.97312355\n",
      "Validated batch 42 batch loss 2.77396297\n",
      "Validated batch 43 batch loss 2.78195333\n",
      "Validated batch 44 batch loss 2.91390419\n",
      "Validated batch 45 batch loss 2.46324\n",
      "Validated batch 46 batch loss 2.63373804\n",
      "Validated batch 47 batch loss 2.56510782\n",
      "Validated batch 48 batch loss 2.79246759\n",
      "Validated batch 49 batch loss 2.44016457\n",
      "Validated batch 50 batch loss 2.63079333\n",
      "Validated batch 51 batch loss 2.80692863\n",
      "Validated batch 52 batch loss 2.81611204\n",
      "Validated batch 53 batch loss 2.572294\n",
      "Validated batch 54 batch loss 2.66174054\n",
      "Validated batch 55 batch loss 2.74920225\n",
      "Validated batch 56 batch loss 2.76981163\n",
      "Validated batch 57 batch loss 2.87091613\n",
      "Validated batch 58 batch loss 2.90189338\n",
      "Validated batch 59 batch loss 2.68629336\n",
      "Validated batch 60 batch loss 2.82287335\n",
      "Validated batch 61 batch loss 2.84163\n",
      "Validated batch 62 batch loss 3.326967\n",
      "Validated batch 63 batch loss 2.82633877\n",
      "Validated batch 64 batch loss 3.02375054\n",
      "Validated batch 65 batch loss 2.86447906\n",
      "Validated batch 66 batch loss 2.74894619\n",
      "Validated batch 67 batch loss 2.9182682\n",
      "Validated batch 68 batch loss 2.62355304\n",
      "Validated batch 69 batch loss 2.9498229\n",
      "Validated batch 70 batch loss 2.81912684\n",
      "Validated batch 71 batch loss 3.07179022\n",
      "Validated batch 72 batch loss 2.96813703\n",
      "Validated batch 73 batch loss 2.92372203\n",
      "Validated batch 74 batch loss 2.96558332\n",
      "Validated batch 75 batch loss 2.95437455\n",
      "Validated batch 76 batch loss 2.96128249\n",
      "Validated batch 77 batch loss 3.13143396\n",
      "Validated batch 78 batch loss 2.77540064\n",
      "Validated batch 79 batch loss 3.07612562\n",
      "Validated batch 80 batch loss 2.8792789\n",
      "Validated batch 81 batch loss 2.54342246\n",
      "Validated batch 82 batch loss 2.60572886\n",
      "Validated batch 83 batch loss 2.72636795\n",
      "Validated batch 84 batch loss 2.90271974\n",
      "Validated batch 85 batch loss 2.76288223\n",
      "Validated batch 86 batch loss 2.96896362\n",
      "Validated batch 87 batch loss 3.01356411\n",
      "Validated batch 88 batch loss 2.63934612\n",
      "Validated batch 89 batch loss 2.75583553\n",
      "Validated batch 90 batch loss 2.79169655\n",
      "Validated batch 91 batch loss 2.85958433\n",
      "Validated batch 92 batch loss 2.54525518\n",
      "Validated batch 93 batch loss 2.66115427\n",
      "Validated batch 94 batch loss 2.86310863\n",
      "Validated batch 95 batch loss 2.6365242\n",
      "Validated batch 96 batch loss 2.55916071\n",
      "Validated batch 97 batch loss 2.82305145\n",
      "Validated batch 98 batch loss 2.61942339\n",
      "Validated batch 99 batch loss 2.78552341\n",
      "Validated batch 100 batch loss 2.5501945\n",
      "Validated batch 101 batch loss 2.84120345\n",
      "Validated batch 102 batch loss 2.68219423\n",
      "Validated batch 103 batch loss 2.6506958\n",
      "Validated batch 104 batch loss 2.77843356\n",
      "Validated batch 105 batch loss 2.80033612\n",
      "Validated batch 106 batch loss 2.86798167\n",
      "Validated batch 107 batch loss 2.64878631\n",
      "Validated batch 108 batch loss 2.69217038\n",
      "Validated batch 109 batch loss 3.04465\n",
      "Validated batch 110 batch loss 2.63739252\n",
      "Validated batch 111 batch loss 2.63942432\n",
      "Validated batch 112 batch loss 2.90702486\n",
      "Validated batch 113 batch loss 3.07942963\n",
      "Validated batch 114 batch loss 2.733639\n",
      "Validated batch 115 batch loss 3.14099455\n",
      "Validated batch 116 batch loss 2.62497568\n",
      "Validated batch 117 batch loss 2.44623327\n",
      "Validated batch 118 batch loss 2.88763809\n",
      "Validated batch 119 batch loss 2.66113091\n",
      "Validated batch 120 batch loss 2.68733716\n",
      "Validated batch 121 batch loss 2.82257295\n",
      "Validated batch 122 batch loss 2.83080506\n",
      "Validated batch 123 batch loss 2.65176296\n",
      "Validated batch 124 batch loss 2.90239286\n",
      "Validated batch 125 batch loss 2.96226549\n",
      "Validated batch 126 batch loss 3.13838029\n",
      "Validated batch 127 batch loss 2.52205181\n",
      "Validated batch 128 batch loss 3.12102747\n",
      "Validated batch 129 batch loss 2.76842737\n",
      "Validated batch 130 batch loss 2.49338531\n",
      "Validated batch 131 batch loss 2.95066118\n",
      "Validated batch 132 batch loss 2.93203402\n",
      "Validated batch 133 batch loss 2.74239516\n",
      "Validated batch 134 batch loss 2.80935931\n",
      "Validated batch 135 batch loss 2.9688158\n",
      "Validated batch 136 batch loss 2.4645381\n",
      "Validated batch 137 batch loss 2.77883768\n",
      "Validated batch 138 batch loss 2.82720351\n",
      "Validated batch 139 batch loss 2.73990726\n",
      "Validated batch 140 batch loss 3.01257014\n",
      "Validated batch 141 batch loss 2.6305685\n",
      "Validated batch 142 batch loss 2.78062439\n",
      "Validated batch 143 batch loss 2.79231668\n",
      "Validated batch 144 batch loss 2.65875459\n",
      "Validated batch 145 batch loss 2.4898057\n",
      "Validated batch 146 batch loss 2.71535516\n",
      "Validated batch 147 batch loss 2.90484715\n",
      "Validated batch 148 batch loss 2.66131878\n",
      "Validated batch 149 batch loss 3.27026534\n",
      "Validated batch 150 batch loss 2.78283119\n",
      "Validated batch 151 batch loss 2.53073597\n",
      "Validated batch 152 batch loss 2.65091395\n",
      "Validated batch 153 batch loss 2.71606874\n",
      "Validated batch 154 batch loss 2.88290882\n",
      "Validated batch 155 batch loss 2.6498251\n",
      "Validated batch 156 batch loss 2.86889601\n",
      "Validated batch 157 batch loss 2.84532666\n",
      "Validated batch 158 batch loss 2.73211432\n",
      "Validated batch 159 batch loss 2.72027636\n",
      "Validated batch 160 batch loss 2.8472383\n",
      "Validated batch 161 batch loss 2.82329798\n",
      "Validated batch 162 batch loss 2.31000042\n",
      "Validated batch 163 batch loss 2.76492405\n",
      "Validated batch 164 batch loss 3.30946445\n",
      "Validated batch 165 batch loss 2.96912217\n",
      "Validated batch 166 batch loss 2.57528901\n",
      "Validated batch 167 batch loss 2.75436664\n",
      "Validated batch 168 batch loss 2.85637069\n",
      "Validated batch 169 batch loss 2.53541708\n",
      "Validated batch 170 batch loss 2.97975802\n",
      "Validated batch 171 batch loss 2.78160739\n",
      "Validated batch 172 batch loss 2.85738516\n",
      "Validated batch 173 batch loss 2.87998295\n",
      "Validated batch 174 batch loss 2.25489855\n",
      "Validated batch 175 batch loss 2.6722064\n",
      "Validated batch 176 batch loss 2.75165153\n",
      "Validated batch 177 batch loss 2.7328577\n",
      "Validated batch 178 batch loss 2.58774281\n",
      "Validated batch 179 batch loss 2.73147917\n",
      "Validated batch 180 batch loss 2.59331536\n",
      "Validated batch 181 batch loss 2.80148196\n",
      "Validated batch 182 batch loss 2.76216149\n",
      "Validated batch 183 batch loss 2.7775259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 184 batch loss 2.7848196\n",
      "Validated batch 185 batch loss 2.96185327\n",
      "Validated batch 186 batch loss 2.89553308\n",
      "Validated batch 187 batch loss 3.07734966\n",
      "Validated batch 188 batch loss 2.8008039\n",
      "Validated batch 189 batch loss 2.32334757\n",
      "Validated batch 190 batch loss 2.84593344\n",
      "Validated batch 191 batch loss 2.8745389\n",
      "Validated batch 192 batch loss 2.6333971\n",
      "Validated batch 193 batch loss 2.7011838\n",
      "Validated batch 194 batch loss 2.95599461\n",
      "Validated batch 195 batch loss 2.57541418\n",
      "Validated batch 196 batch loss 2.88368368\n",
      "Validated batch 197 batch loss 2.77352262\n",
      "Validated batch 198 batch loss 2.6773603\n",
      "Validated batch 199 batch loss 2.67159557\n",
      "Validated batch 200 batch loss 2.91262937\n",
      "Validated batch 201 batch loss 2.15789294\n",
      "Validated batch 202 batch loss 3.14663601\n",
      "Validated batch 203 batch loss 3.17329693\n",
      "Validated batch 204 batch loss 2.76728702\n",
      "Validated batch 205 batch loss 2.69797444\n",
      "Validated batch 206 batch loss 2.65719295\n",
      "Validated batch 207 batch loss 2.55067444\n",
      "Validated batch 208 batch loss 2.66804385\n",
      "Validated batch 209 batch loss 2.7500124\n",
      "Validated batch 210 batch loss 2.75302672\n",
      "Validated batch 211 batch loss 3.08269644\n",
      "Validated batch 212 batch loss 3.11310935\n",
      "Validated batch 213 batch loss 2.7558012\n",
      "Validated batch 214 batch loss 2.99337602\n",
      "Validated batch 215 batch loss 2.96893311\n",
      "Validated batch 216 batch loss 3.06611848\n",
      "Validated batch 217 batch loss 2.58185291\n",
      "Validated batch 218 batch loss 2.78819799\n",
      "Validated batch 219 batch loss 3.15190983\n",
      "Validated batch 220 batch loss 2.99327946\n",
      "Validated batch 221 batch loss 2.68131781\n",
      "Validated batch 222 batch loss 2.76335382\n",
      "Validated batch 223 batch loss 3.28030705\n",
      "Validated batch 224 batch loss 2.57910228\n",
      "Validated batch 225 batch loss 2.82014751\n",
      "Validated batch 226 batch loss 3.04088902\n",
      "Validated batch 227 batch loss 2.36552906\n",
      "Validated batch 228 batch loss 2.35940218\n",
      "Validated batch 229 batch loss 2.71060562\n",
      "Validated batch 230 batch loss 2.95998907\n",
      "Validated batch 231 batch loss 2.77665138\n",
      "Validated batch 232 batch loss 2.75566769\n",
      "Validated batch 233 batch loss 2.97919607\n",
      "Validated batch 234 batch loss 2.83005357\n",
      "Validated batch 235 batch loss 2.50597596\n",
      "Validated batch 236 batch loss 2.58087277\n",
      "Validated batch 237 batch loss 2.51563478\n",
      "Validated batch 238 batch loss 3.00363\n",
      "Validated batch 239 batch loss 2.94733429\n",
      "Validated batch 240 batch loss 2.65296\n",
      "Validated batch 241 batch loss 2.83012915\n",
      "Validated batch 242 batch loss 2.7545011\n",
      "Validated batch 243 batch loss 2.86411262\n",
      "Validated batch 244 batch loss 2.93432283\n",
      "Validated batch 245 batch loss 2.8630693\n",
      "Validated batch 246 batch loss 3.0169487\n",
      "Validated batch 247 batch loss 2.90625405\n",
      "Validated batch 248 batch loss 2.98239088\n",
      "Validated batch 249 batch loss 2.84737587\n",
      "Validated batch 250 batch loss 2.76472902\n",
      "Validated batch 251 batch loss 2.84641838\n",
      "Validated batch 252 batch loss 3.02538919\n",
      "Validated batch 253 batch loss 2.96235609\n",
      "Validated batch 254 batch loss 2.80487061\n",
      "Validated batch 255 batch loss 2.42492867\n",
      "Validated batch 256 batch loss 2.97445488\n",
      "Validated batch 257 batch loss 2.95787144\n",
      "Validated batch 258 batch loss 2.95882154\n",
      "Validated batch 259 batch loss 3.05136728\n",
      "Validated batch 260 batch loss 2.68647313\n",
      "Validated batch 261 batch loss 2.88132\n",
      "Validated batch 262 batch loss 2.87854934\n",
      "Validated batch 263 batch loss 2.76369619\n",
      "Validated batch 264 batch loss 3.2589519\n",
      "Validated batch 265 batch loss 2.64142275\n",
      "Validated batch 266 batch loss 2.47859\n",
      "Validated batch 267 batch loss 2.69643378\n",
      "Validated batch 268 batch loss 2.62036014\n",
      "Validated batch 269 batch loss 2.90655375\n",
      "Validated batch 270 batch loss 2.44110346\n",
      "Validated batch 271 batch loss 2.46896124\n",
      "Validated batch 272 batch loss 2.80532956\n",
      "Validated batch 273 batch loss 3.16673899\n",
      "Validated batch 274 batch loss 2.91361856\n",
      "Validated batch 275 batch loss 2.79862809\n",
      "Validated batch 276 batch loss 2.54385471\n",
      "Validated batch 277 batch loss 2.83717465\n",
      "Validated batch 278 batch loss 2.60865974\n",
      "Validated batch 279 batch loss 2.79164886\n",
      "Validated batch 280 batch loss 2.83359957\n",
      "Validated batch 281 batch loss 2.91069078\n",
      "Validated batch 282 batch loss 2.86919689\n",
      "Validated batch 283 batch loss 2.79419947\n",
      "Validated batch 284 batch loss 2.56792259\n",
      "Validated batch 285 batch loss 2.74977779\n",
      "Validated batch 286 batch loss 2.97134161\n",
      "Validated batch 287 batch loss 3.05499387\n",
      "Validated batch 288 batch loss 3.02454162\n",
      "Validated batch 289 batch loss 2.62849545\n",
      "Validated batch 290 batch loss 2.3996377\n",
      "Validated batch 291 batch loss 2.8253088\n",
      "Validated batch 292 batch loss 2.80386019\n",
      "Validated batch 293 batch loss 2.82913876\n",
      "Validated batch 294 batch loss 2.60253954\n",
      "Validated batch 295 batch loss 2.7989397\n",
      "Validated batch 296 batch loss 2.81376386\n",
      "Validated batch 297 batch loss 2.90382147\n",
      "Validated batch 298 batch loss 2.92549109\n",
      "Validated batch 299 batch loss 2.62304974\n",
      "Validated batch 300 batch loss 2.80651808\n",
      "Validated batch 301 batch loss 2.19855165\n",
      "Validated batch 302 batch loss 2.46671438\n",
      "Validated batch 303 batch loss 2.90427256\n",
      "Validated batch 304 batch loss 2.69140744\n",
      "Validated batch 305 batch loss 2.8102479\n",
      "Validated batch 306 batch loss 2.7170372\n",
      "Validated batch 307 batch loss 2.7840662\n",
      "Validated batch 308 batch loss 2.74585962\n",
      "Validated batch 309 batch loss 2.88753772\n",
      "Validated batch 310 batch loss 2.8264432\n",
      "Validated batch 311 batch loss 2.57887697\n",
      "Validated batch 312 batch loss 2.654212\n",
      "Validated batch 313 batch loss 3.02695\n",
      "Validated batch 314 batch loss 2.70842314\n",
      "Validated batch 315 batch loss 2.84446883\n",
      "Validated batch 316 batch loss 2.93481946\n",
      "Validated batch 317 batch loss 2.7493341\n",
      "Validated batch 318 batch loss 2.77224159\n",
      "Validated batch 319 batch loss 2.83314896\n",
      "Validated batch 320 batch loss 2.7328968\n",
      "Validated batch 321 batch loss 2.84686422\n",
      "Validated batch 322 batch loss 2.51033568\n",
      "Validated batch 323 batch loss 2.71215725\n",
      "Validated batch 324 batch loss 2.60118103\n",
      "Validated batch 325 batch loss 2.90401626\n",
      "Validated batch 326 batch loss 2.97206831\n",
      "Validated batch 327 batch loss 2.70999908\n",
      "Validated batch 328 batch loss 2.80672956\n",
      "Validated batch 329 batch loss 2.27339745\n",
      "Validated batch 330 batch loss 2.67343426\n",
      "Validated batch 331 batch loss 2.5999229\n",
      "Validated batch 332 batch loss 2.66881347\n",
      "Validated batch 333 batch loss 2.68700504\n",
      "Validated batch 334 batch loss 3.165869\n",
      "Validated batch 335 batch loss 2.52908063\n",
      "Validated batch 336 batch loss 2.66515565\n",
      "Validated batch 337 batch loss 2.82695246\n",
      "Validated batch 338 batch loss 2.34404182\n",
      "Validated batch 339 batch loss 2.63912773\n",
      "Validated batch 340 batch loss 2.74441481\n",
      "Validated batch 341 batch loss 2.6579175\n",
      "Validated batch 342 batch loss 2.88344193\n",
      "Validated batch 343 batch loss 2.656986\n",
      "Validated batch 344 batch loss 2.63771749\n",
      "Validated batch 345 batch loss 2.62564898\n",
      "Validated batch 346 batch loss 2.8955524\n",
      "Validated batch 347 batch loss 2.58301449\n",
      "Validated batch 348 batch loss 2.6703639\n",
      "Validated batch 349 batch loss 3.06369376\n",
      "Validated batch 350 batch loss 2.5814\n",
      "Validated batch 351 batch loss 2.83463502\n",
      "Validated batch 352 batch loss 2.92620659\n",
      "Validated batch 353 batch loss 2.82572126\n",
      "Validated batch 354 batch loss 3.07022071\n",
      "Validated batch 355 batch loss 2.61028266\n",
      "Validated batch 356 batch loss 2.79638386\n",
      "Validated batch 357 batch loss 2.81444788\n",
      "Validated batch 358 batch loss 3.06550336\n",
      "Validated batch 359 batch loss 2.66424155\n",
      "Validated batch 360 batch loss 2.56239367\n",
      "Validated batch 361 batch loss 2.38117361\n",
      "Validated batch 362 batch loss 2.56388855\n",
      "Validated batch 363 batch loss 2.99239302\n",
      "Validated batch 364 batch loss 2.64264107\n",
      "Validated batch 365 batch loss 2.45933294\n",
      "Validated batch 366 batch loss 2.81431675\n",
      "Validated batch 367 batch loss 3.03723216\n",
      "Validated batch 368 batch loss 2.96304083\n",
      "Validated batch 369 batch loss 3.07792854\n",
      "Validated batch 370 batch loss 2.84756589\n",
      "Epoch 1 val loss 2.7831857204437256\n",
      "Model ./models/model-v0.0.1-epoch-1-loss-2.7832.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 2.82861805 epoch total loss 2.82861805\n",
      "Trained batch 2 batch loss 2.68736458 epoch total loss 2.75799131\n",
      "Trained batch 3 batch loss 2.72144938 epoch total loss 2.74581075\n",
      "Trained batch 4 batch loss 2.61212659 epoch total loss 2.71238971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 5 batch loss 2.8187561 epoch total loss 2.73366308\n",
      "Trained batch 6 batch loss 2.61777115 epoch total loss 2.71434784\n",
      "Trained batch 7 batch loss 2.66100621 epoch total loss 2.70672774\n",
      "Trained batch 8 batch loss 2.55071688 epoch total loss 2.6872263\n",
      "Trained batch 9 batch loss 2.58898067 epoch total loss 2.6763103\n",
      "Trained batch 10 batch loss 2.55601978 epoch total loss 2.66428113\n",
      "Trained batch 11 batch loss 2.41157579 epoch total loss 2.64130783\n",
      "Trained batch 12 batch loss 3.01319 epoch total loss 2.67229819\n",
      "Trained batch 13 batch loss 2.92795444 epoch total loss 2.69196415\n",
      "Trained batch 14 batch loss 2.50475645 epoch total loss 2.67859221\n",
      "Trained batch 15 batch loss 2.50335288 epoch total loss 2.66690946\n",
      "Trained batch 16 batch loss 2.56388044 epoch total loss 2.66047025\n",
      "Trained batch 17 batch loss 2.72091269 epoch total loss 2.66402578\n",
      "Trained batch 18 batch loss 2.7130127 epoch total loss 2.66674709\n",
      "Trained batch 19 batch loss 2.42993355 epoch total loss 2.65428329\n",
      "Trained batch 20 batch loss 2.49638247 epoch total loss 2.64638829\n",
      "Trained batch 21 batch loss 2.60672474 epoch total loss 2.64449954\n",
      "Trained batch 22 batch loss 2.47008038 epoch total loss 2.63657141\n",
      "Trained batch 23 batch loss 2.37101769 epoch total loss 2.62502551\n",
      "Trained batch 24 batch loss 2.4242816 epoch total loss 2.61666131\n",
      "Trained batch 25 batch loss 2.4318378 epoch total loss 2.60926819\n",
      "Trained batch 26 batch loss 2.40123463 epoch total loss 2.6012671\n",
      "Trained batch 27 batch loss 2.67856145 epoch total loss 2.60412955\n",
      "Trained batch 28 batch loss 2.8499949 epoch total loss 2.61291075\n",
      "Trained batch 29 batch loss 2.82354593 epoch total loss 2.62017393\n",
      "Trained batch 30 batch loss 2.64854956 epoch total loss 2.62112\n",
      "Trained batch 31 batch loss 2.61880279 epoch total loss 2.62104535\n",
      "Trained batch 32 batch loss 2.70005631 epoch total loss 2.62351441\n",
      "Trained batch 33 batch loss 2.70777702 epoch total loss 2.62606788\n",
      "Trained batch 34 batch loss 2.80939865 epoch total loss 2.63146\n",
      "Trained batch 35 batch loss 2.87993956 epoch total loss 2.6385591\n",
      "Trained batch 36 batch loss 2.85589433 epoch total loss 2.64459634\n",
      "Trained batch 37 batch loss 2.80583096 epoch total loss 2.64895391\n",
      "Trained batch 38 batch loss 2.8393321 epoch total loss 2.65396404\n",
      "Trained batch 39 batch loss 2.91951346 epoch total loss 2.6607728\n",
      "Trained batch 40 batch loss 2.79522347 epoch total loss 2.66413426\n",
      "Trained batch 41 batch loss 2.8391428 epoch total loss 2.66840267\n",
      "Trained batch 42 batch loss 2.81153131 epoch total loss 2.67181063\n",
      "Trained batch 43 batch loss 2.74157524 epoch total loss 2.67343307\n",
      "Trained batch 44 batch loss 2.94318056 epoch total loss 2.67956376\n",
      "Trained batch 45 batch loss 2.6332674 epoch total loss 2.67853498\n",
      "Trained batch 46 batch loss 3.22358274 epoch total loss 2.69038367\n",
      "Trained batch 47 batch loss 2.75099301 epoch total loss 2.69167328\n",
      "Trained batch 48 batch loss 2.63416052 epoch total loss 2.69047523\n",
      "Trained batch 49 batch loss 2.6599896 epoch total loss 2.68985295\n",
      "Trained batch 50 batch loss 2.49591303 epoch total loss 2.68597412\n",
      "Trained batch 51 batch loss 2.20838642 epoch total loss 2.67660975\n",
      "Trained batch 52 batch loss 2.63278484 epoch total loss 2.67576694\n",
      "Trained batch 53 batch loss 2.52027 epoch total loss 2.67283297\n",
      "Trained batch 54 batch loss 2.61087775 epoch total loss 2.67168546\n",
      "Trained batch 55 batch loss 2.76329803 epoch total loss 2.67335105\n",
      "Trained batch 56 batch loss 2.69455338 epoch total loss 2.67372942\n",
      "Trained batch 57 batch loss 2.7141 epoch total loss 2.67443776\n",
      "Trained batch 58 batch loss 2.48796368 epoch total loss 2.67122245\n",
      "Trained batch 59 batch loss 2.92553568 epoch total loss 2.67553306\n",
      "Trained batch 60 batch loss 2.69291115 epoch total loss 2.67582273\n",
      "Trained batch 61 batch loss 2.27585244 epoch total loss 2.66926575\n",
      "Trained batch 62 batch loss 2.67174625 epoch total loss 2.6693058\n",
      "Trained batch 63 batch loss 2.61948061 epoch total loss 2.66851497\n",
      "Trained batch 64 batch loss 2.88225675 epoch total loss 2.67185473\n",
      "Trained batch 65 batch loss 2.83596373 epoch total loss 2.67437959\n",
      "Trained batch 66 batch loss 2.64911985 epoch total loss 2.67399693\n",
      "Trained batch 67 batch loss 2.68144822 epoch total loss 2.67410803\n",
      "Trained batch 68 batch loss 3.01963139 epoch total loss 2.67918944\n",
      "Trained batch 69 batch loss 3.13172293 epoch total loss 2.68574786\n",
      "Trained batch 70 batch loss 2.69600248 epoch total loss 2.68589425\n",
      "Trained batch 71 batch loss 2.4318378 epoch total loss 2.68231606\n",
      "Trained batch 72 batch loss 2.26098347 epoch total loss 2.67646432\n",
      "Trained batch 73 batch loss 2.70086813 epoch total loss 2.67679858\n",
      "Trained batch 74 batch loss 2.61876392 epoch total loss 2.67601418\n",
      "Trained batch 75 batch loss 2.6427784 epoch total loss 2.6755712\n",
      "Trained batch 76 batch loss 2.84290504 epoch total loss 2.677773\n",
      "Trained batch 77 batch loss 2.86926913 epoch total loss 2.6802597\n",
      "Trained batch 78 batch loss 2.99423027 epoch total loss 2.68428516\n",
      "Trained batch 79 batch loss 2.93158698 epoch total loss 2.68741536\n",
      "Trained batch 80 batch loss 3.12273383 epoch total loss 2.69285679\n",
      "Trained batch 81 batch loss 2.91828203 epoch total loss 2.69563985\n",
      "Trained batch 82 batch loss 2.7802887 epoch total loss 2.6966722\n",
      "Trained batch 83 batch loss 2.9603343 epoch total loss 2.69984889\n",
      "Trained batch 84 batch loss 2.55455756 epoch total loss 2.69811893\n",
      "Trained batch 85 batch loss 2.45893216 epoch total loss 2.69530511\n",
      "Trained batch 86 batch loss 2.43685699 epoch total loss 2.69229984\n",
      "Trained batch 87 batch loss 2.6453867 epoch total loss 2.69176078\n",
      "Trained batch 88 batch loss 2.77017832 epoch total loss 2.69265175\n",
      "Trained batch 89 batch loss 2.76838708 epoch total loss 2.69350266\n",
      "Trained batch 90 batch loss 2.7478137 epoch total loss 2.6941061\n",
      "Trained batch 91 batch loss 2.63347077 epoch total loss 2.69344\n",
      "Trained batch 92 batch loss 2.72395849 epoch total loss 2.6937716\n",
      "Trained batch 93 batch loss 2.68613815 epoch total loss 2.69368958\n",
      "Trained batch 94 batch loss 2.83256888 epoch total loss 2.69516683\n",
      "Trained batch 95 batch loss 2.48798037 epoch total loss 2.69298601\n",
      "Trained batch 96 batch loss 2.77240372 epoch total loss 2.69381332\n",
      "Trained batch 97 batch loss 2.79139423 epoch total loss 2.69481921\n",
      "Trained batch 98 batch loss 2.72724414 epoch total loss 2.69515\n",
      "Trained batch 99 batch loss 3.18016839 epoch total loss 2.70004916\n",
      "Trained batch 100 batch loss 3.01118 epoch total loss 2.70316029\n",
      "Trained batch 101 batch loss 2.90219426 epoch total loss 2.70513105\n",
      "Trained batch 102 batch loss 3.08963585 epoch total loss 2.70890069\n",
      "Trained batch 103 batch loss 2.37634706 epoch total loss 2.70567179\n",
      "Trained batch 104 batch loss 2.46458697 epoch total loss 2.70335388\n",
      "Trained batch 105 batch loss 2.83021116 epoch total loss 2.70456195\n",
      "Trained batch 106 batch loss 2.70436049 epoch total loss 2.7045598\n",
      "Trained batch 107 batch loss 2.600945 epoch total loss 2.70359159\n",
      "Trained batch 108 batch loss 2.57556605 epoch total loss 2.70240617\n",
      "Trained batch 109 batch loss 2.62104511 epoch total loss 2.70165968\n",
      "Trained batch 110 batch loss 2.74519873 epoch total loss 2.70205545\n",
      "Trained batch 111 batch loss 2.91151595 epoch total loss 2.70394254\n",
      "Trained batch 112 batch loss 2.60780287 epoch total loss 2.70308423\n",
      "Trained batch 113 batch loss 2.72269583 epoch total loss 2.70325756\n",
      "Trained batch 114 batch loss 2.85338902 epoch total loss 2.70457458\n",
      "Trained batch 115 batch loss 2.48677206 epoch total loss 2.70268083\n",
      "Trained batch 116 batch loss 2.69173193 epoch total loss 2.70258641\n",
      "Trained batch 117 batch loss 2.75417042 epoch total loss 2.70302749\n",
      "Trained batch 118 batch loss 3.10033178 epoch total loss 2.70639443\n",
      "Trained batch 119 batch loss 2.52797961 epoch total loss 2.70489526\n",
      "Trained batch 120 batch loss 3.06170511 epoch total loss 2.70786881\n",
      "Trained batch 121 batch loss 2.85554409 epoch total loss 2.70908904\n",
      "Trained batch 122 batch loss 2.82925224 epoch total loss 2.71007395\n",
      "Trained batch 123 batch loss 2.80225182 epoch total loss 2.7108233\n",
      "Trained batch 124 batch loss 3.04544163 epoch total loss 2.71352196\n",
      "Trained batch 125 batch loss 2.77625394 epoch total loss 2.71402359\n",
      "Trained batch 126 batch loss 2.50895548 epoch total loss 2.71239614\n",
      "Trained batch 127 batch loss 3.10232902 epoch total loss 2.71546626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 128 batch loss 2.92381501 epoch total loss 2.71709418\n",
      "Trained batch 129 batch loss 2.74777603 epoch total loss 2.71733189\n",
      "Trained batch 130 batch loss 2.38298392 epoch total loss 2.71476\n",
      "Trained batch 131 batch loss 2.71680307 epoch total loss 2.7147758\n",
      "Trained batch 132 batch loss 2.68849587 epoch total loss 2.71457672\n",
      "Trained batch 133 batch loss 2.51882339 epoch total loss 2.71310496\n",
      "Trained batch 134 batch loss 2.80567145 epoch total loss 2.71379566\n",
      "Trained batch 135 batch loss 3.18033171 epoch total loss 2.71725154\n",
      "Trained batch 136 batch loss 3.29997611 epoch total loss 2.7215364\n",
      "Trained batch 137 batch loss 2.72285128 epoch total loss 2.7215457\n",
      "Trained batch 138 batch loss 2.92032814 epoch total loss 2.72298622\n",
      "Trained batch 139 batch loss 2.68352842 epoch total loss 2.72270226\n",
      "Trained batch 140 batch loss 2.67752552 epoch total loss 2.72237968\n",
      "Trained batch 141 batch loss 2.38609433 epoch total loss 2.71999478\n",
      "Trained batch 142 batch loss 2.57975602 epoch total loss 2.71900702\n",
      "Trained batch 143 batch loss 2.56994867 epoch total loss 2.71796465\n",
      "Trained batch 144 batch loss 2.74433088 epoch total loss 2.71814775\n",
      "Trained batch 145 batch loss 2.6320014 epoch total loss 2.71755362\n",
      "Trained batch 146 batch loss 2.83578777 epoch total loss 2.71836329\n",
      "Trained batch 147 batch loss 3.13766527 epoch total loss 2.72121572\n",
      "Trained batch 148 batch loss 3.2614212 epoch total loss 2.72486567\n",
      "Trained batch 149 batch loss 2.55297518 epoch total loss 2.72371197\n",
      "Trained batch 150 batch loss 3.00680637 epoch total loss 2.72559929\n",
      "Trained batch 151 batch loss 2.77835488 epoch total loss 2.72594881\n",
      "Trained batch 152 batch loss 2.73141861 epoch total loss 2.72598457\n",
      "Trained batch 153 batch loss 2.7636764 epoch total loss 2.7262311\n",
      "Trained batch 154 batch loss 2.66594 epoch total loss 2.72583961\n",
      "Trained batch 155 batch loss 2.56669164 epoch total loss 2.72481275\n",
      "Trained batch 156 batch loss 2.74377751 epoch total loss 2.72493434\n",
      "Trained batch 157 batch loss 2.42177248 epoch total loss 2.72300339\n",
      "Trained batch 158 batch loss 2.81630492 epoch total loss 2.72359395\n",
      "Trained batch 159 batch loss 2.80985069 epoch total loss 2.72413635\n",
      "Trained batch 160 batch loss 2.81661367 epoch total loss 2.72471452\n",
      "Trained batch 161 batch loss 2.90672493 epoch total loss 2.7258451\n",
      "Trained batch 162 batch loss 3.03904295 epoch total loss 2.7277782\n",
      "Trained batch 163 batch loss 3.20499468 epoch total loss 2.73070598\n",
      "Trained batch 164 batch loss 3.16451502 epoch total loss 2.73335123\n",
      "Trained batch 165 batch loss 2.80616736 epoch total loss 2.7337923\n",
      "Trained batch 166 batch loss 2.83772326 epoch total loss 2.73441863\n",
      "Trained batch 167 batch loss 2.65588188 epoch total loss 2.73394823\n",
      "Trained batch 168 batch loss 2.52067757 epoch total loss 2.73267889\n",
      "Trained batch 169 batch loss 2.54600024 epoch total loss 2.7315743\n",
      "Trained batch 170 batch loss 2.48579979 epoch total loss 2.73012853\n",
      "Trained batch 171 batch loss 2.70064092 epoch total loss 2.72995615\n",
      "Trained batch 172 batch loss 2.5277319 epoch total loss 2.72878051\n",
      "Trained batch 173 batch loss 2.98553228 epoch total loss 2.73026466\n",
      "Trained batch 174 batch loss 3.16683912 epoch total loss 2.73277378\n",
      "Trained batch 175 batch loss 3.00531244 epoch total loss 2.73433113\n",
      "Trained batch 176 batch loss 2.83859015 epoch total loss 2.73492336\n",
      "Trained batch 177 batch loss 2.70095158 epoch total loss 2.73473167\n",
      "Trained batch 178 batch loss 2.8677597 epoch total loss 2.73547888\n",
      "Trained batch 179 batch loss 2.87935233 epoch total loss 2.73628283\n",
      "Trained batch 180 batch loss 2.66290474 epoch total loss 2.73587513\n",
      "Trained batch 181 batch loss 2.99242115 epoch total loss 2.73729253\n",
      "Trained batch 182 batch loss 3.10336876 epoch total loss 2.73930383\n",
      "Trained batch 183 batch loss 2.63690472 epoch total loss 2.73874426\n",
      "Trained batch 184 batch loss 2.85310721 epoch total loss 2.73936605\n",
      "Trained batch 185 batch loss 2.86052895 epoch total loss 2.74002099\n",
      "Trained batch 186 batch loss 2.65607262 epoch total loss 2.73956966\n",
      "Trained batch 187 batch loss 2.77627945 epoch total loss 2.73976612\n",
      "Trained batch 188 batch loss 2.6798892 epoch total loss 2.73944736\n",
      "Trained batch 189 batch loss 2.71762419 epoch total loss 2.7393322\n",
      "Trained batch 190 batch loss 2.72591519 epoch total loss 2.73926139\n",
      "Trained batch 191 batch loss 2.65965772 epoch total loss 2.73884463\n",
      "Trained batch 192 batch loss 2.67696667 epoch total loss 2.73852229\n",
      "Trained batch 193 batch loss 2.57369828 epoch total loss 2.73766804\n",
      "Trained batch 194 batch loss 2.73957729 epoch total loss 2.73767781\n",
      "Trained batch 195 batch loss 2.88044286 epoch total loss 2.73841\n",
      "Trained batch 196 batch loss 2.99574947 epoch total loss 2.73972273\n",
      "Trained batch 197 batch loss 3.07947636 epoch total loss 2.74144745\n",
      "Trained batch 198 batch loss 3.08219957 epoch total loss 2.74316835\n",
      "Trained batch 199 batch loss 2.74701285 epoch total loss 2.74318767\n",
      "Trained batch 200 batch loss 2.86728454 epoch total loss 2.74380827\n",
      "Trained batch 201 batch loss 2.98689914 epoch total loss 2.74501753\n",
      "Trained batch 202 batch loss 2.84610128 epoch total loss 2.74551821\n",
      "Trained batch 203 batch loss 2.98711824 epoch total loss 2.74670839\n",
      "Trained batch 204 batch loss 2.70546174 epoch total loss 2.74650598\n",
      "Trained batch 205 batch loss 2.76112413 epoch total loss 2.74657726\n",
      "Trained batch 206 batch loss 2.93292046 epoch total loss 2.74748182\n",
      "Trained batch 207 batch loss 2.99471116 epoch total loss 2.74867606\n",
      "Trained batch 208 batch loss 2.65155792 epoch total loss 2.74820924\n",
      "Trained batch 209 batch loss 2.70557046 epoch total loss 2.74800515\n",
      "Trained batch 210 batch loss 2.56826806 epoch total loss 2.74714947\n",
      "Trained batch 211 batch loss 2.72965 epoch total loss 2.7470665\n",
      "Trained batch 212 batch loss 2.73834372 epoch total loss 2.74702549\n",
      "Trained batch 213 batch loss 2.85866785 epoch total loss 2.74754953\n",
      "Trained batch 214 batch loss 2.64324403 epoch total loss 2.74706197\n",
      "Trained batch 215 batch loss 2.44955826 epoch total loss 2.74567842\n",
      "Trained batch 216 batch loss 3.01277542 epoch total loss 2.74691486\n",
      "Trained batch 217 batch loss 2.95165348 epoch total loss 2.74785852\n",
      "Trained batch 218 batch loss 2.76600218 epoch total loss 2.74794149\n",
      "Trained batch 219 batch loss 2.7767334 epoch total loss 2.7480731\n",
      "Trained batch 220 batch loss 2.83726215 epoch total loss 2.74847865\n",
      "Trained batch 221 batch loss 2.68910527 epoch total loss 2.74820971\n",
      "Trained batch 222 batch loss 2.72192168 epoch total loss 2.74809146\n",
      "Trained batch 223 batch loss 2.65961266 epoch total loss 2.74769473\n",
      "Trained batch 224 batch loss 2.68751574 epoch total loss 2.74742579\n",
      "Trained batch 225 batch loss 2.90510368 epoch total loss 2.74812675\n",
      "Trained batch 226 batch loss 2.92378306 epoch total loss 2.74890375\n",
      "Trained batch 227 batch loss 2.77758574 epoch total loss 2.74903\n",
      "Trained batch 228 batch loss 2.82745671 epoch total loss 2.74937415\n",
      "Trained batch 229 batch loss 3.09463835 epoch total loss 2.75088191\n",
      "Trained batch 230 batch loss 2.73424435 epoch total loss 2.75080967\n",
      "Trained batch 231 batch loss 2.77348614 epoch total loss 2.7509079\n",
      "Trained batch 232 batch loss 2.76630044 epoch total loss 2.75097418\n",
      "Trained batch 233 batch loss 2.64870739 epoch total loss 2.75053525\n",
      "Trained batch 234 batch loss 2.28860283 epoch total loss 2.74856091\n",
      "Trained batch 235 batch loss 2.49719095 epoch total loss 2.74749136\n",
      "Trained batch 236 batch loss 2.59035635 epoch total loss 2.74682546\n",
      "Trained batch 237 batch loss 2.60788393 epoch total loss 2.74623919\n",
      "Trained batch 238 batch loss 2.68662906 epoch total loss 2.74598885\n",
      "Trained batch 239 batch loss 2.62298393 epoch total loss 2.7454741\n",
      "Trained batch 240 batch loss 2.64060307 epoch total loss 2.74503732\n",
      "Trained batch 241 batch loss 2.60943699 epoch total loss 2.74447465\n",
      "Trained batch 242 batch loss 2.95006704 epoch total loss 2.74532413\n",
      "Trained batch 243 batch loss 2.95609808 epoch total loss 2.74619174\n",
      "Trained batch 244 batch loss 2.91509128 epoch total loss 2.74688387\n",
      "Trained batch 245 batch loss 2.71700478 epoch total loss 2.7467618\n",
      "Trained batch 246 batch loss 2.86844015 epoch total loss 2.74725676\n",
      "Trained batch 247 batch loss 2.7562623 epoch total loss 2.74729323\n",
      "Trained batch 248 batch loss 2.79182673 epoch total loss 2.74747276\n",
      "Trained batch 249 batch loss 2.6458292 epoch total loss 2.74706435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 250 batch loss 2.71655178 epoch total loss 2.74694228\n",
      "Trained batch 251 batch loss 2.68556285 epoch total loss 2.74669766\n",
      "Trained batch 252 batch loss 2.66611 epoch total loss 2.74637818\n",
      "Trained batch 253 batch loss 2.8045435 epoch total loss 2.74660802\n",
      "Trained batch 254 batch loss 2.7693634 epoch total loss 2.74669766\n",
      "Trained batch 255 batch loss 2.68275785 epoch total loss 2.74644685\n",
      "Trained batch 256 batch loss 2.62572145 epoch total loss 2.74597526\n",
      "Trained batch 257 batch loss 2.60971689 epoch total loss 2.74544525\n",
      "Trained batch 258 batch loss 2.56666851 epoch total loss 2.74475217\n",
      "Trained batch 259 batch loss 2.99005342 epoch total loss 2.74569917\n",
      "Trained batch 260 batch loss 3.02300215 epoch total loss 2.74676585\n",
      "Trained batch 261 batch loss 2.8982513 epoch total loss 2.74734616\n",
      "Trained batch 262 batch loss 2.89437819 epoch total loss 2.7479074\n",
      "Trained batch 263 batch loss 3.20205951 epoch total loss 2.74963427\n",
      "Trained batch 264 batch loss 2.71599054 epoch total loss 2.74950695\n",
      "Trained batch 265 batch loss 2.51549029 epoch total loss 2.74862385\n",
      "Trained batch 266 batch loss 2.30710793 epoch total loss 2.74696398\n",
      "Trained batch 267 batch loss 2.32672882 epoch total loss 2.74539018\n",
      "Trained batch 268 batch loss 2.26973844 epoch total loss 2.74361515\n",
      "Trained batch 269 batch loss 2.36025238 epoch total loss 2.74219\n",
      "Trained batch 270 batch loss 2.34734 epoch total loss 2.74072766\n",
      "Trained batch 271 batch loss 2.44576693 epoch total loss 2.73963904\n",
      "Trained batch 272 batch loss 2.85428643 epoch total loss 2.74006081\n",
      "Trained batch 273 batch loss 3.14664698 epoch total loss 2.74155021\n",
      "Trained batch 274 batch loss 2.68259287 epoch total loss 2.74133492\n",
      "Trained batch 275 batch loss 2.90527987 epoch total loss 2.7419312\n",
      "Trained batch 276 batch loss 3.11467957 epoch total loss 2.7432816\n",
      "Trained batch 277 batch loss 3.35904694 epoch total loss 2.74550486\n",
      "Trained batch 278 batch loss 3.04940081 epoch total loss 2.74659777\n",
      "Trained batch 279 batch loss 2.96402 epoch total loss 2.74737716\n",
      "Trained batch 280 batch loss 2.63927 epoch total loss 2.74699116\n",
      "Trained batch 281 batch loss 2.78134823 epoch total loss 2.74711347\n",
      "Trained batch 282 batch loss 2.58901 epoch total loss 2.74655271\n",
      "Trained batch 283 batch loss 2.81747723 epoch total loss 2.74680352\n",
      "Trained batch 284 batch loss 3.05142927 epoch total loss 2.74787617\n",
      "Trained batch 285 batch loss 3.07714415 epoch total loss 2.74903154\n",
      "Trained batch 286 batch loss 2.73890567 epoch total loss 2.74899602\n",
      "Trained batch 287 batch loss 2.82118225 epoch total loss 2.74924755\n",
      "Trained batch 288 batch loss 2.91803455 epoch total loss 2.74983358\n",
      "Trained batch 289 batch loss 3.19862366 epoch total loss 2.7513864\n",
      "Trained batch 290 batch loss 2.91109848 epoch total loss 2.75193715\n",
      "Trained batch 291 batch loss 2.95691729 epoch total loss 2.75264144\n",
      "Trained batch 292 batch loss 2.9883523 epoch total loss 2.75344872\n",
      "Trained batch 293 batch loss 3.22733307 epoch total loss 2.75506616\n",
      "Trained batch 294 batch loss 3.14967036 epoch total loss 2.75640821\n",
      "Trained batch 295 batch loss 3.11553049 epoch total loss 2.75762558\n",
      "Trained batch 296 batch loss 2.71517444 epoch total loss 2.75748205\n",
      "Trained batch 297 batch loss 2.90019846 epoch total loss 2.7579627\n",
      "Trained batch 298 batch loss 2.70999146 epoch total loss 2.75780177\n",
      "Trained batch 299 batch loss 2.79920363 epoch total loss 2.75794029\n",
      "Trained batch 300 batch loss 2.78177953 epoch total loss 2.75802\n",
      "Trained batch 301 batch loss 2.53425622 epoch total loss 2.7572763\n",
      "Trained batch 302 batch loss 2.52385545 epoch total loss 2.75650358\n",
      "Trained batch 303 batch loss 2.50754666 epoch total loss 2.75568199\n",
      "Trained batch 304 batch loss 2.59465241 epoch total loss 2.75515223\n",
      "Trained batch 305 batch loss 3.16275239 epoch total loss 2.7564888\n",
      "Trained batch 306 batch loss 2.88701415 epoch total loss 2.75691533\n",
      "Trained batch 307 batch loss 2.69300437 epoch total loss 2.75670719\n",
      "Trained batch 308 batch loss 2.54703259 epoch total loss 2.75602651\n",
      "Trained batch 309 batch loss 2.67223072 epoch total loss 2.75575519\n",
      "Trained batch 310 batch loss 2.72768688 epoch total loss 2.75566459\n",
      "Trained batch 311 batch loss 2.8257494 epoch total loss 2.75589\n",
      "Trained batch 312 batch loss 2.72148943 epoch total loss 2.75577974\n",
      "Trained batch 313 batch loss 2.99295044 epoch total loss 2.75653768\n",
      "Trained batch 314 batch loss 2.66937184 epoch total loss 2.75626\n",
      "Trained batch 315 batch loss 2.65244961 epoch total loss 2.75593042\n",
      "Trained batch 316 batch loss 2.90166545 epoch total loss 2.75639176\n",
      "Trained batch 317 batch loss 2.78042078 epoch total loss 2.75646734\n",
      "Trained batch 318 batch loss 2.91615844 epoch total loss 2.75696945\n",
      "Trained batch 319 batch loss 2.62137675 epoch total loss 2.75654459\n",
      "Trained batch 320 batch loss 2.70143223 epoch total loss 2.75637221\n",
      "Trained batch 321 batch loss 3.10162592 epoch total loss 2.75744772\n",
      "Trained batch 322 batch loss 2.85712147 epoch total loss 2.75775743\n",
      "Trained batch 323 batch loss 2.86937046 epoch total loss 2.75810289\n",
      "Trained batch 324 batch loss 3.17057252 epoch total loss 2.75937605\n",
      "Trained batch 325 batch loss 2.7172935 epoch total loss 2.75924659\n",
      "Trained batch 326 batch loss 2.90216351 epoch total loss 2.75968504\n",
      "Trained batch 327 batch loss 3.06476498 epoch total loss 2.76061797\n",
      "Trained batch 328 batch loss 2.71741104 epoch total loss 2.76048613\n",
      "Trained batch 329 batch loss 2.78990889 epoch total loss 2.76057553\n",
      "Trained batch 330 batch loss 2.75525045 epoch total loss 2.76055956\n",
      "Trained batch 331 batch loss 2.5101378 epoch total loss 2.75980282\n",
      "Trained batch 332 batch loss 2.69301796 epoch total loss 2.75960159\n",
      "Trained batch 333 batch loss 2.79798365 epoch total loss 2.75971699\n",
      "Trained batch 334 batch loss 2.53247333 epoch total loss 2.75903654\n",
      "Trained batch 335 batch loss 2.6469 epoch total loss 2.7587018\n",
      "Trained batch 336 batch loss 2.81361198 epoch total loss 2.75886512\n",
      "Trained batch 337 batch loss 2.87365198 epoch total loss 2.75920582\n",
      "Trained batch 338 batch loss 2.60863876 epoch total loss 2.75876045\n",
      "Trained batch 339 batch loss 2.90928411 epoch total loss 2.75920439\n",
      "Trained batch 340 batch loss 2.68427253 epoch total loss 2.75898409\n",
      "Trained batch 341 batch loss 2.87369823 epoch total loss 2.7593205\n",
      "Trained batch 342 batch loss 2.85604692 epoch total loss 2.75960326\n",
      "Trained batch 343 batch loss 2.43056917 epoch total loss 2.75864387\n",
      "Trained batch 344 batch loss 2.89923549 epoch total loss 2.75905252\n",
      "Trained batch 345 batch loss 2.77427673 epoch total loss 2.75909662\n",
      "Trained batch 346 batch loss 2.62201047 epoch total loss 2.75870061\n",
      "Trained batch 347 batch loss 2.8041091 epoch total loss 2.7588315\n",
      "Trained batch 348 batch loss 2.80988526 epoch total loss 2.75897813\n",
      "Trained batch 349 batch loss 2.71706367 epoch total loss 2.75885797\n",
      "Trained batch 350 batch loss 2.59516335 epoch total loss 2.75839019\n",
      "Trained batch 351 batch loss 2.62873173 epoch total loss 2.75802088\n",
      "Trained batch 352 batch loss 2.72644567 epoch total loss 2.75793099\n",
      "Trained batch 353 batch loss 2.78625846 epoch total loss 2.75801134\n",
      "Trained batch 354 batch loss 3.18849635 epoch total loss 2.75922728\n",
      "Trained batch 355 batch loss 2.84126449 epoch total loss 2.7594583\n",
      "Trained batch 356 batch loss 2.78238845 epoch total loss 2.75952291\n",
      "Trained batch 357 batch loss 2.58587 epoch total loss 2.75903654\n",
      "Trained batch 358 batch loss 2.38475037 epoch total loss 2.75799108\n",
      "Trained batch 359 batch loss 2.48578429 epoch total loss 2.75723267\n",
      "Trained batch 360 batch loss 2.51306248 epoch total loss 2.7565546\n",
      "Trained batch 361 batch loss 2.41438818 epoch total loss 2.75560665\n",
      "Trained batch 362 batch loss 2.34548664 epoch total loss 2.75447369\n",
      "Trained batch 363 batch loss 2.45214295 epoch total loss 2.75364065\n",
      "Trained batch 364 batch loss 2.33941889 epoch total loss 2.75250268\n",
      "Trained batch 365 batch loss 2.71900177 epoch total loss 2.75241089\n",
      "Trained batch 366 batch loss 2.80572128 epoch total loss 2.75255656\n",
      "Trained batch 367 batch loss 2.76247883 epoch total loss 2.7525835\n",
      "Trained batch 368 batch loss 2.92966652 epoch total loss 2.75306487\n",
      "Trained batch 369 batch loss 2.84683633 epoch total loss 2.75331903\n",
      "Trained batch 370 batch loss 2.74786806 epoch total loss 2.75330424\n",
      "Trained batch 371 batch loss 2.65469575 epoch total loss 2.75303864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 372 batch loss 2.44511914 epoch total loss 2.75221086\n",
      "Trained batch 373 batch loss 2.61088657 epoch total loss 2.75183201\n",
      "Trained batch 374 batch loss 2.4286046 epoch total loss 2.75096774\n",
      "Trained batch 375 batch loss 2.56819224 epoch total loss 2.75048041\n",
      "Trained batch 376 batch loss 2.70763373 epoch total loss 2.75036645\n",
      "Trained batch 377 batch loss 2.86348152 epoch total loss 2.75066662\n",
      "Trained batch 378 batch loss 3.20161724 epoch total loss 2.75186\n",
      "Trained batch 379 batch loss 2.98093867 epoch total loss 2.75246429\n",
      "Trained batch 380 batch loss 2.69721079 epoch total loss 2.7523191\n",
      "Trained batch 381 batch loss 2.64999175 epoch total loss 2.75205064\n",
      "Trained batch 382 batch loss 2.73342061 epoch total loss 2.75200176\n",
      "Trained batch 383 batch loss 2.56292629 epoch total loss 2.75150776\n",
      "Trained batch 384 batch loss 2.55911016 epoch total loss 2.75100684\n",
      "Trained batch 385 batch loss 2.32842255 epoch total loss 2.74990892\n",
      "Trained batch 386 batch loss 2.3564198 epoch total loss 2.74888968\n",
      "Trained batch 387 batch loss 2.09377527 epoch total loss 2.74719691\n",
      "Trained batch 388 batch loss 2.19039488 epoch total loss 2.74576187\n",
      "Trained batch 389 batch loss 2.39790058 epoch total loss 2.7448678\n",
      "Trained batch 390 batch loss 2.52499557 epoch total loss 2.74430394\n",
      "Trained batch 391 batch loss 2.67326164 epoch total loss 2.74412227\n",
      "Trained batch 392 batch loss 2.87637663 epoch total loss 2.74445939\n",
      "Trained batch 393 batch loss 2.39110708 epoch total loss 2.74356031\n",
      "Trained batch 394 batch loss 2.63663101 epoch total loss 2.74328899\n",
      "Trained batch 395 batch loss 2.58957934 epoch total loss 2.7429\n",
      "Trained batch 396 batch loss 2.8437078 epoch total loss 2.74315453\n",
      "Trained batch 397 batch loss 3.18713546 epoch total loss 2.74427295\n",
      "Trained batch 398 batch loss 2.6763196 epoch total loss 2.744102\n",
      "Trained batch 399 batch loss 2.29229927 epoch total loss 2.74296975\n",
      "Trained batch 400 batch loss 2.79096222 epoch total loss 2.74309\n",
      "Trained batch 401 batch loss 2.62104177 epoch total loss 2.74278569\n",
      "Trained batch 402 batch loss 2.59885335 epoch total loss 2.74242759\n",
      "Trained batch 403 batch loss 2.59989548 epoch total loss 2.74207401\n",
      "Trained batch 404 batch loss 2.78096795 epoch total loss 2.74217033\n",
      "Trained batch 405 batch loss 2.77880645 epoch total loss 2.74226069\n",
      "Trained batch 406 batch loss 3.0206666 epoch total loss 2.74294639\n",
      "Trained batch 407 batch loss 3.05929637 epoch total loss 2.74372363\n",
      "Trained batch 408 batch loss 2.94810104 epoch total loss 2.74422479\n",
      "Trained batch 409 batch loss 2.46066165 epoch total loss 2.74353147\n",
      "Trained batch 410 batch loss 2.22879386 epoch total loss 2.74227595\n",
      "Trained batch 411 batch loss 2.29208493 epoch total loss 2.74118066\n",
      "Trained batch 412 batch loss 2.15111923 epoch total loss 2.73974848\n",
      "Trained batch 413 batch loss 2.49883842 epoch total loss 2.73916507\n",
      "Trained batch 414 batch loss 2.4531281 epoch total loss 2.73847413\n",
      "Trained batch 415 batch loss 2.46512175 epoch total loss 2.73781538\n",
      "Trained batch 416 batch loss 2.79589701 epoch total loss 2.73795485\n",
      "Trained batch 417 batch loss 2.81271935 epoch total loss 2.73813438\n",
      "Trained batch 418 batch loss 2.56213403 epoch total loss 2.73771334\n",
      "Trained batch 419 batch loss 2.32815742 epoch total loss 2.73673582\n",
      "Trained batch 420 batch loss 2.62303257 epoch total loss 2.73646498\n",
      "Trained batch 421 batch loss 2.90150499 epoch total loss 2.73685694\n",
      "Trained batch 422 batch loss 2.661623 epoch total loss 2.7366786\n",
      "Trained batch 423 batch loss 2.7493124 epoch total loss 2.7367084\n",
      "Trained batch 424 batch loss 2.74835443 epoch total loss 2.73673606\n",
      "Trained batch 425 batch loss 2.98854685 epoch total loss 2.73732853\n",
      "Trained batch 426 batch loss 2.66928363 epoch total loss 2.73716879\n",
      "Trained batch 427 batch loss 2.70624256 epoch total loss 2.73709655\n",
      "Trained batch 428 batch loss 2.6563611 epoch total loss 2.73690796\n",
      "Trained batch 429 batch loss 2.8501296 epoch total loss 2.73717189\n",
      "Trained batch 430 batch loss 3.38342404 epoch total loss 2.73867464\n",
      "Trained batch 431 batch loss 3.16120768 epoch total loss 2.73965526\n",
      "Trained batch 432 batch loss 3.28250122 epoch total loss 2.74091172\n",
      "Trained batch 433 batch loss 3.10230803 epoch total loss 2.74174619\n",
      "Trained batch 434 batch loss 3.27826405 epoch total loss 2.74298263\n",
      "Trained batch 435 batch loss 2.90360022 epoch total loss 2.7433517\n",
      "Trained batch 436 batch loss 2.47861 epoch total loss 2.74274468\n",
      "Trained batch 437 batch loss 2.79701424 epoch total loss 2.7428689\n",
      "Trained batch 438 batch loss 2.46788621 epoch total loss 2.74224091\n",
      "Trained batch 439 batch loss 3.11163306 epoch total loss 2.74308228\n",
      "Trained batch 440 batch loss 2.77119255 epoch total loss 2.74314642\n",
      "Trained batch 441 batch loss 2.77821779 epoch total loss 2.74322581\n",
      "Trained batch 442 batch loss 2.94128561 epoch total loss 2.7436738\n",
      "Trained batch 443 batch loss 3.04091024 epoch total loss 2.74434471\n",
      "Trained batch 444 batch loss 3.04130101 epoch total loss 2.74501348\n",
      "Trained batch 445 batch loss 3.19131732 epoch total loss 2.7460165\n",
      "Trained batch 446 batch loss 2.96446538 epoch total loss 2.74650621\n",
      "Trained batch 447 batch loss 2.9965663 epoch total loss 2.74706578\n",
      "Trained batch 448 batch loss 2.61179733 epoch total loss 2.74676371\n",
      "Trained batch 449 batch loss 2.5108881 epoch total loss 2.74623847\n",
      "Trained batch 450 batch loss 2.73385549 epoch total loss 2.74621105\n",
      "Trained batch 451 batch loss 2.83615017 epoch total loss 2.74641037\n",
      "Trained batch 452 batch loss 2.65173054 epoch total loss 2.74620104\n",
      "Trained batch 453 batch loss 2.53859138 epoch total loss 2.74574256\n",
      "Trained batch 454 batch loss 2.86691523 epoch total loss 2.74600959\n",
      "Trained batch 455 batch loss 2.94892955 epoch total loss 2.74645567\n",
      "Trained batch 456 batch loss 2.74146271 epoch total loss 2.7464447\n",
      "Trained batch 457 batch loss 2.86968589 epoch total loss 2.74671435\n",
      "Trained batch 458 batch loss 3.09634733 epoch total loss 2.74747753\n",
      "Trained batch 459 batch loss 2.97530103 epoch total loss 2.74797392\n",
      "Trained batch 460 batch loss 2.70791817 epoch total loss 2.7478869\n",
      "Trained batch 461 batch loss 3.00730109 epoch total loss 2.74844956\n",
      "Trained batch 462 batch loss 3.16299748 epoch total loss 2.74934673\n",
      "Trained batch 463 batch loss 3.09493923 epoch total loss 2.75009322\n",
      "Trained batch 464 batch loss 3.01401591 epoch total loss 2.75066209\n",
      "Trained batch 465 batch loss 3.15287876 epoch total loss 2.75152707\n",
      "Trained batch 466 batch loss 2.99541593 epoch total loss 2.7520504\n",
      "Trained batch 467 batch loss 3.13229537 epoch total loss 2.7528646\n",
      "Trained batch 468 batch loss 3.05148792 epoch total loss 2.75350285\n",
      "Trained batch 469 batch loss 2.92486882 epoch total loss 2.75386834\n",
      "Trained batch 470 batch loss 3.0273 epoch total loss 2.75445\n",
      "Trained batch 471 batch loss 3.05223 epoch total loss 2.75508237\n",
      "Trained batch 472 batch loss 2.88161755 epoch total loss 2.75535035\n",
      "Trained batch 473 batch loss 2.92242646 epoch total loss 2.75570369\n",
      "Trained batch 474 batch loss 3.03772426 epoch total loss 2.75629878\n",
      "Trained batch 475 batch loss 2.83700848 epoch total loss 2.75646877\n",
      "Trained batch 476 batch loss 2.89847779 epoch total loss 2.75676703\n",
      "Trained batch 477 batch loss 3.01513433 epoch total loss 2.75730872\n",
      "Trained batch 478 batch loss 2.64218354 epoch total loss 2.75706792\n",
      "Trained batch 479 batch loss 2.80269742 epoch total loss 2.75716305\n",
      "Trained batch 480 batch loss 2.7657814 epoch total loss 2.75718093\n",
      "Trained batch 481 batch loss 2.74590206 epoch total loss 2.75715756\n",
      "Trained batch 482 batch loss 2.46891737 epoch total loss 2.75655937\n",
      "Trained batch 483 batch loss 2.32717896 epoch total loss 2.75567031\n",
      "Trained batch 484 batch loss 2.40130711 epoch total loss 2.75493836\n",
      "Trained batch 485 batch loss 2.27596188 epoch total loss 2.75395083\n",
      "Trained batch 486 batch loss 2.37622452 epoch total loss 2.75317359\n",
      "Trained batch 487 batch loss 2.90453219 epoch total loss 2.75348449\n",
      "Trained batch 488 batch loss 2.67852187 epoch total loss 2.75333071\n",
      "Trained batch 489 batch loss 2.38534021 epoch total loss 2.75257826\n",
      "Trained batch 490 batch loss 2.54306626 epoch total loss 2.75215077\n",
      "Trained batch 491 batch loss 2.6746285 epoch total loss 2.75199294\n",
      "Trained batch 492 batch loss 2.7420187 epoch total loss 2.75197268\n",
      "Trained batch 493 batch loss 2.63599229 epoch total loss 2.75173736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 494 batch loss 2.74517608 epoch total loss 2.751724\n",
      "Trained batch 495 batch loss 2.92470789 epoch total loss 2.75207353\n",
      "Trained batch 496 batch loss 2.96374416 epoch total loss 2.7525003\n",
      "Trained batch 497 batch loss 2.58532333 epoch total loss 2.75216389\n",
      "Trained batch 498 batch loss 2.68095136 epoch total loss 2.75202084\n",
      "Trained batch 499 batch loss 2.65850925 epoch total loss 2.75183344\n",
      "Trained batch 500 batch loss 2.4773736 epoch total loss 2.7512846\n",
      "Trained batch 501 batch loss 2.60164785 epoch total loss 2.7509861\n",
      "Trained batch 502 batch loss 2.72831249 epoch total loss 2.7509408\n",
      "Trained batch 503 batch loss 2.75157928 epoch total loss 2.75094199\n",
      "Trained batch 504 batch loss 2.88805461 epoch total loss 2.75121427\n",
      "Trained batch 505 batch loss 2.6582191 epoch total loss 2.75103\n",
      "Trained batch 506 batch loss 2.67158103 epoch total loss 2.75087309\n",
      "Trained batch 507 batch loss 2.19783783 epoch total loss 2.74978232\n",
      "Trained batch 508 batch loss 2.26078248 epoch total loss 2.74881959\n",
      "Trained batch 509 batch loss 1.98626113 epoch total loss 2.74732137\n",
      "Trained batch 510 batch loss 2.19439411 epoch total loss 2.74623704\n",
      "Trained batch 511 batch loss 2.12905741 epoch total loss 2.74502921\n",
      "Trained batch 512 batch loss 2.40431237 epoch total loss 2.74436378\n",
      "Trained batch 513 batch loss 2.93058515 epoch total loss 2.74472666\n",
      "Trained batch 514 batch loss 2.82594514 epoch total loss 2.74488473\n",
      "Trained batch 515 batch loss 3.0515306 epoch total loss 2.74548\n",
      "Trained batch 516 batch loss 3.05720496 epoch total loss 2.74608421\n",
      "Trained batch 517 batch loss 3.29467273 epoch total loss 2.74714541\n",
      "Trained batch 518 batch loss 3.11583591 epoch total loss 2.74785709\n",
      "Trained batch 519 batch loss 2.94352 epoch total loss 2.74823403\n",
      "Trained batch 520 batch loss 2.8440907 epoch total loss 2.74841857\n",
      "Trained batch 521 batch loss 2.99191666 epoch total loss 2.74888587\n",
      "Trained batch 522 batch loss 3.09763145 epoch total loss 2.74955416\n",
      "Trained batch 523 batch loss 3.06332326 epoch total loss 2.75015402\n",
      "Trained batch 524 batch loss 2.92317677 epoch total loss 2.75048423\n",
      "Trained batch 525 batch loss 2.86338186 epoch total loss 2.75069952\n",
      "Trained batch 526 batch loss 2.68416739 epoch total loss 2.75057292\n",
      "Trained batch 527 batch loss 2.64890695 epoch total loss 2.75038\n",
      "Trained batch 528 batch loss 2.17042971 epoch total loss 2.74928164\n",
      "Trained batch 529 batch loss 2.60606337 epoch total loss 2.74901104\n",
      "Trained batch 530 batch loss 2.70513129 epoch total loss 2.74892807\n",
      "Trained batch 531 batch loss 2.73131132 epoch total loss 2.74889493\n",
      "Trained batch 532 batch loss 2.6502769 epoch total loss 2.74870944\n",
      "Trained batch 533 batch loss 2.6987431 epoch total loss 2.74861574\n",
      "Trained batch 534 batch loss 2.68429756 epoch total loss 2.74849534\n",
      "Trained batch 535 batch loss 2.53495502 epoch total loss 2.74809623\n",
      "Trained batch 536 batch loss 2.72371054 epoch total loss 2.74805069\n",
      "Trained batch 537 batch loss 2.79872656 epoch total loss 2.7481451\n",
      "Trained batch 538 batch loss 3.07971978 epoch total loss 2.74876142\n",
      "Trained batch 539 batch loss 2.82245302 epoch total loss 2.74889827\n",
      "Trained batch 540 batch loss 2.62331343 epoch total loss 2.74866557\n",
      "Trained batch 541 batch loss 2.21707821 epoch total loss 2.74768281\n",
      "Trained batch 542 batch loss 2.95708609 epoch total loss 2.74806929\n",
      "Trained batch 543 batch loss 2.99206114 epoch total loss 2.74851847\n",
      "Trained batch 544 batch loss 2.73933744 epoch total loss 2.74850178\n",
      "Trained batch 545 batch loss 2.79006362 epoch total loss 2.74857783\n",
      "Trained batch 546 batch loss 3.08420944 epoch total loss 2.74919271\n",
      "Trained batch 547 batch loss 2.98611116 epoch total loss 2.74962568\n",
      "Trained batch 548 batch loss 3.00791979 epoch total loss 2.75009704\n",
      "Trained batch 549 batch loss 3.23110175 epoch total loss 2.75097322\n",
      "Trained batch 550 batch loss 2.96911669 epoch total loss 2.75137\n",
      "Trained batch 551 batch loss 3.21083617 epoch total loss 2.7522037\n",
      "Trained batch 552 batch loss 3.07841635 epoch total loss 2.7527945\n",
      "Trained batch 553 batch loss 3.06851554 epoch total loss 2.75336552\n",
      "Trained batch 554 batch loss 2.39672375 epoch total loss 2.75272179\n",
      "Trained batch 555 batch loss 2.46315098 epoch total loss 2.7522\n",
      "Trained batch 556 batch loss 2.22098207 epoch total loss 2.75124431\n",
      "Trained batch 557 batch loss 2.49534321 epoch total loss 2.75078511\n",
      "Trained batch 558 batch loss 3.11177397 epoch total loss 2.75143194\n",
      "Trained batch 559 batch loss 3.12086391 epoch total loss 2.75209284\n",
      "Trained batch 560 batch loss 3.27220821 epoch total loss 2.75302172\n",
      "Trained batch 561 batch loss 3.21554184 epoch total loss 2.75384617\n",
      "Trained batch 562 batch loss 2.82090068 epoch total loss 2.75396562\n",
      "Trained batch 563 batch loss 2.83563185 epoch total loss 2.75411057\n",
      "Trained batch 564 batch loss 2.80737519 epoch total loss 2.75420499\n",
      "Trained batch 565 batch loss 2.8602562 epoch total loss 2.75439262\n",
      "Trained batch 566 batch loss 2.90124369 epoch total loss 2.75465202\n",
      "Trained batch 567 batch loss 2.67790222 epoch total loss 2.7545166\n",
      "Trained batch 568 batch loss 2.71747684 epoch total loss 2.75445151\n",
      "Trained batch 569 batch loss 2.8120203 epoch total loss 2.7545526\n",
      "Trained batch 570 batch loss 2.75314426 epoch total loss 2.75455022\n",
      "Trained batch 571 batch loss 2.86476707 epoch total loss 2.7547431\n",
      "Trained batch 572 batch loss 2.63520455 epoch total loss 2.75453424\n",
      "Trained batch 573 batch loss 2.76809788 epoch total loss 2.75455785\n",
      "Trained batch 574 batch loss 2.834728 epoch total loss 2.75469756\n",
      "Trained batch 575 batch loss 2.97612381 epoch total loss 2.75508261\n",
      "Trained batch 576 batch loss 2.95264482 epoch total loss 2.75542545\n",
      "Trained batch 577 batch loss 2.80700207 epoch total loss 2.75551486\n",
      "Trained batch 578 batch loss 2.49021864 epoch total loss 2.7550559\n",
      "Trained batch 579 batch loss 2.76065969 epoch total loss 2.75506568\n",
      "Trained batch 580 batch loss 2.56674838 epoch total loss 2.75474095\n",
      "Trained batch 581 batch loss 2.748 epoch total loss 2.75472951\n",
      "Trained batch 582 batch loss 2.92790747 epoch total loss 2.75502682\n",
      "Trained batch 583 batch loss 2.57003 epoch total loss 2.75470972\n",
      "Trained batch 584 batch loss 2.62096691 epoch total loss 2.7544806\n",
      "Trained batch 585 batch loss 2.95004272 epoch total loss 2.7548151\n",
      "Trained batch 586 batch loss 2.55791044 epoch total loss 2.75447893\n",
      "Trained batch 587 batch loss 2.82408667 epoch total loss 2.75459743\n",
      "Trained batch 588 batch loss 2.75189781 epoch total loss 2.7545929\n",
      "Trained batch 589 batch loss 2.29723644 epoch total loss 2.7538166\n",
      "Trained batch 590 batch loss 2.7140274 epoch total loss 2.75374889\n",
      "Trained batch 591 batch loss 2.65537047 epoch total loss 2.75358248\n",
      "Trained batch 592 batch loss 3.07215381 epoch total loss 2.75412059\n",
      "Trained batch 593 batch loss 2.8322711 epoch total loss 2.75425243\n",
      "Trained batch 594 batch loss 3.09584856 epoch total loss 2.7548275\n",
      "Trained batch 595 batch loss 2.78946257 epoch total loss 2.75488567\n",
      "Trained batch 596 batch loss 2.42886305 epoch total loss 2.7543385\n",
      "Trained batch 597 batch loss 2.943578 epoch total loss 2.7546556\n",
      "Trained batch 598 batch loss 2.71712756 epoch total loss 2.7545929\n",
      "Trained batch 599 batch loss 2.76007175 epoch total loss 2.75460219\n",
      "Trained batch 600 batch loss 2.29318142 epoch total loss 2.75383329\n",
      "Trained batch 601 batch loss 2.48886132 epoch total loss 2.75339246\n",
      "Trained batch 602 batch loss 2.30503082 epoch total loss 2.75264764\n",
      "Trained batch 603 batch loss 2.39444232 epoch total loss 2.7520535\n",
      "Trained batch 604 batch loss 2.55956745 epoch total loss 2.75173497\n",
      "Trained batch 605 batch loss 2.65224981 epoch total loss 2.75157046\n",
      "Trained batch 606 batch loss 2.64567733 epoch total loss 2.75139546\n",
      "Trained batch 607 batch loss 2.53462195 epoch total loss 2.75103855\n",
      "Trained batch 608 batch loss 2.91179657 epoch total loss 2.75130272\n",
      "Trained batch 609 batch loss 2.92411923 epoch total loss 2.75158644\n",
      "Trained batch 610 batch loss 2.85445833 epoch total loss 2.75175524\n",
      "Trained batch 611 batch loss 3.11185884 epoch total loss 2.75234461\n",
      "Trained batch 612 batch loss 2.80437708 epoch total loss 2.75242949\n",
      "Trained batch 613 batch loss 2.7385745 epoch total loss 2.75240684\n",
      "Trained batch 614 batch loss 2.96845984 epoch total loss 2.75275874\n",
      "Trained batch 615 batch loss 2.99933314 epoch total loss 2.75315976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 616 batch loss 2.55134964 epoch total loss 2.75283217\n",
      "Trained batch 617 batch loss 2.76484919 epoch total loss 2.75285172\n",
      "Trained batch 618 batch loss 2.83860683 epoch total loss 2.75299048\n",
      "Trained batch 619 batch loss 2.88187075 epoch total loss 2.75319862\n",
      "Trained batch 620 batch loss 2.37139559 epoch total loss 2.75258279\n",
      "Trained batch 621 batch loss 2.40072417 epoch total loss 2.75201631\n",
      "Trained batch 622 batch loss 2.77019477 epoch total loss 2.75204539\n",
      "Trained batch 623 batch loss 2.34488583 epoch total loss 2.75139165\n",
      "Trained batch 624 batch loss 2.46728659 epoch total loss 2.75093651\n",
      "Trained batch 625 batch loss 2.79129601 epoch total loss 2.75100088\n",
      "Trained batch 626 batch loss 2.50721192 epoch total loss 2.75061154\n",
      "Trained batch 627 batch loss 2.77053 epoch total loss 2.75064325\n",
      "Trained batch 628 batch loss 2.86571169 epoch total loss 2.7508266\n",
      "Trained batch 629 batch loss 2.93608069 epoch total loss 2.75112104\n",
      "Trained batch 630 batch loss 2.82531929 epoch total loss 2.75123882\n",
      "Trained batch 631 batch loss 2.74842644 epoch total loss 2.75123429\n",
      "Trained batch 632 batch loss 2.73093557 epoch total loss 2.75120211\n",
      "Trained batch 633 batch loss 2.75688457 epoch total loss 2.75121117\n",
      "Trained batch 634 batch loss 2.72247362 epoch total loss 2.75116587\n",
      "Trained batch 635 batch loss 2.44054341 epoch total loss 2.75067663\n",
      "Trained batch 636 batch loss 2.62160063 epoch total loss 2.75047374\n",
      "Trained batch 637 batch loss 2.63335919 epoch total loss 2.75028968\n",
      "Trained batch 638 batch loss 2.64878559 epoch total loss 2.75013065\n",
      "Trained batch 639 batch loss 2.70495534 epoch total loss 2.75006\n",
      "Trained batch 640 batch loss 2.66516209 epoch total loss 2.74992728\n",
      "Trained batch 641 batch loss 2.89292121 epoch total loss 2.75015044\n",
      "Trained batch 642 batch loss 2.39855146 epoch total loss 2.74960279\n",
      "Trained batch 643 batch loss 2.4343853 epoch total loss 2.74911237\n",
      "Trained batch 644 batch loss 2.92330027 epoch total loss 2.74938297\n",
      "Trained batch 645 batch loss 2.81058073 epoch total loss 2.74947786\n",
      "Trained batch 646 batch loss 2.74968505 epoch total loss 2.7494781\n",
      "Trained batch 647 batch loss 2.64946699 epoch total loss 2.74932337\n",
      "Trained batch 648 batch loss 2.8030827 epoch total loss 2.74940634\n",
      "Trained batch 649 batch loss 2.5659678 epoch total loss 2.74912357\n",
      "Trained batch 650 batch loss 2.62067485 epoch total loss 2.74892616\n",
      "Trained batch 651 batch loss 2.67142749 epoch total loss 2.74880695\n",
      "Trained batch 652 batch loss 2.5079062 epoch total loss 2.74843764\n",
      "Trained batch 653 batch loss 2.67245436 epoch total loss 2.74832129\n",
      "Trained batch 654 batch loss 2.45924902 epoch total loss 2.74787927\n",
      "Trained batch 655 batch loss 2.57524681 epoch total loss 2.74761558\n",
      "Trained batch 656 batch loss 2.77185082 epoch total loss 2.74765253\n",
      "Trained batch 657 batch loss 3.01773834 epoch total loss 2.74806356\n",
      "Trained batch 658 batch loss 3.13353825 epoch total loss 2.74864936\n",
      "Trained batch 659 batch loss 2.59161878 epoch total loss 2.74841118\n",
      "Trained batch 660 batch loss 2.69172 epoch total loss 2.74832535\n",
      "Trained batch 661 batch loss 2.89635229 epoch total loss 2.74854946\n",
      "Trained batch 662 batch loss 2.62726068 epoch total loss 2.74836636\n",
      "Trained batch 663 batch loss 2.47061539 epoch total loss 2.74794722\n",
      "Trained batch 664 batch loss 2.41640759 epoch total loss 2.74744797\n",
      "Trained batch 665 batch loss 2.61626244 epoch total loss 2.74725056\n",
      "Trained batch 666 batch loss 2.58041668 epoch total loss 2.74700022\n",
      "Trained batch 667 batch loss 2.91261196 epoch total loss 2.74724841\n",
      "Trained batch 668 batch loss 2.68282652 epoch total loss 2.74715209\n",
      "Trained batch 669 batch loss 2.60177469 epoch total loss 2.74693465\n",
      "Trained batch 670 batch loss 2.35452938 epoch total loss 2.7463491\n",
      "Trained batch 671 batch loss 2.3279295 epoch total loss 2.74572539\n",
      "Trained batch 672 batch loss 2.28685379 epoch total loss 2.74504256\n",
      "Trained batch 673 batch loss 2.22034311 epoch total loss 2.74426293\n",
      "Trained batch 674 batch loss 2.35829306 epoch total loss 2.74369025\n",
      "Trained batch 675 batch loss 2.31933165 epoch total loss 2.74306154\n",
      "Trained batch 676 batch loss 2.46749377 epoch total loss 2.74265385\n",
      "Trained batch 677 batch loss 2.68134165 epoch total loss 2.74256349\n",
      "Trained batch 678 batch loss 2.58708215 epoch total loss 2.74233413\n",
      "Trained batch 679 batch loss 2.36182117 epoch total loss 2.74177361\n",
      "Trained batch 680 batch loss 2.44796824 epoch total loss 2.74134159\n",
      "Trained batch 681 batch loss 2.33781672 epoch total loss 2.74074912\n",
      "Trained batch 682 batch loss 2.69132113 epoch total loss 2.7406764\n",
      "Trained batch 683 batch loss 2.64644623 epoch total loss 2.7405386\n",
      "Trained batch 684 batch loss 2.61402273 epoch total loss 2.74035358\n",
      "Trained batch 685 batch loss 2.63996744 epoch total loss 2.7402072\n",
      "Trained batch 686 batch loss 2.72289872 epoch total loss 2.74018192\n",
      "Trained batch 687 batch loss 2.70744324 epoch total loss 2.74013424\n",
      "Trained batch 688 batch loss 2.49583864 epoch total loss 2.739779\n",
      "Trained batch 689 batch loss 2.27416658 epoch total loss 2.73910332\n",
      "Trained batch 690 batch loss 2.30935645 epoch total loss 2.73848057\n",
      "Trained batch 691 batch loss 2.60763764 epoch total loss 2.73829103\n",
      "Trained batch 692 batch loss 2.65741444 epoch total loss 2.73817444\n",
      "Trained batch 693 batch loss 2.70439053 epoch total loss 2.73812556\n",
      "Trained batch 694 batch loss 2.98118615 epoch total loss 2.7384758\n",
      "Trained batch 695 batch loss 3.13559437 epoch total loss 2.73904729\n",
      "Trained batch 696 batch loss 2.79532957 epoch total loss 2.73912811\n",
      "Trained batch 697 batch loss 2.78823185 epoch total loss 2.73919845\n",
      "Trained batch 698 batch loss 2.73140574 epoch total loss 2.73918724\n",
      "Trained batch 699 batch loss 2.65272713 epoch total loss 2.7390635\n",
      "Trained batch 700 batch loss 2.58951521 epoch total loss 2.73884988\n",
      "Trained batch 701 batch loss 2.67091942 epoch total loss 2.73875308\n",
      "Trained batch 702 batch loss 2.77949524 epoch total loss 2.73881102\n",
      "Trained batch 703 batch loss 2.52474594 epoch total loss 2.73850656\n",
      "Trained batch 704 batch loss 2.31343508 epoch total loss 2.73790288\n",
      "Trained batch 705 batch loss 2.83600283 epoch total loss 2.73804212\n",
      "Trained batch 706 batch loss 2.61817598 epoch total loss 2.73787236\n",
      "Trained batch 707 batch loss 2.50705433 epoch total loss 2.73754597\n",
      "Trained batch 708 batch loss 2.78868437 epoch total loss 2.73761821\n",
      "Trained batch 709 batch loss 2.60255241 epoch total loss 2.73742771\n",
      "Trained batch 710 batch loss 2.52376485 epoch total loss 2.73712683\n",
      "Trained batch 711 batch loss 2.49756145 epoch total loss 2.7367897\n",
      "Trained batch 712 batch loss 2.24433661 epoch total loss 2.73609829\n",
      "Trained batch 713 batch loss 2.63870215 epoch total loss 2.73596168\n",
      "Trained batch 714 batch loss 2.62321234 epoch total loss 2.7358036\n",
      "Trained batch 715 batch loss 2.69476414 epoch total loss 2.73574638\n",
      "Trained batch 716 batch loss 2.5727489 epoch total loss 2.73551869\n",
      "Trained batch 717 batch loss 2.32703972 epoch total loss 2.73494887\n",
      "Trained batch 718 batch loss 2.63942647 epoch total loss 2.73481584\n",
      "Trained batch 719 batch loss 3.00874066 epoch total loss 2.73519683\n",
      "Trained batch 720 batch loss 2.62202191 epoch total loss 2.73503971\n",
      "Trained batch 721 batch loss 2.70657229 epoch total loss 2.73500013\n",
      "Trained batch 722 batch loss 2.74840426 epoch total loss 2.73501873\n",
      "Trained batch 723 batch loss 2.34538317 epoch total loss 2.73448\n",
      "Trained batch 724 batch loss 2.31848383 epoch total loss 2.73390532\n",
      "Trained batch 725 batch loss 2.8514607 epoch total loss 2.73406744\n",
      "Trained batch 726 batch loss 2.62253332 epoch total loss 2.7339139\n",
      "Trained batch 727 batch loss 2.63547111 epoch total loss 2.73377848\n",
      "Trained batch 728 batch loss 2.87709188 epoch total loss 2.73397517\n",
      "Trained batch 729 batch loss 2.74700785 epoch total loss 2.73399305\n",
      "Trained batch 730 batch loss 2.59486151 epoch total loss 2.73380256\n",
      "Trained batch 731 batch loss 2.65157127 epoch total loss 2.73369\n",
      "Trained batch 732 batch loss 2.83674788 epoch total loss 2.73383093\n",
      "Trained batch 733 batch loss 2.71159601 epoch total loss 2.73380041\n",
      "Trained batch 734 batch loss 2.50330806 epoch total loss 2.73348641\n",
      "Trained batch 735 batch loss 2.38888454 epoch total loss 2.73301768\n",
      "Trained batch 736 batch loss 2.55752277 epoch total loss 2.73277903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 737 batch loss 2.58018446 epoch total loss 2.73257208\n",
      "Trained batch 738 batch loss 2.46163034 epoch total loss 2.73220491\n",
      "Trained batch 739 batch loss 2.67886305 epoch total loss 2.73213291\n",
      "Trained batch 740 batch loss 2.54272223 epoch total loss 2.73187685\n",
      "Trained batch 741 batch loss 2.5007081 epoch total loss 2.731565\n",
      "Trained batch 742 batch loss 2.43141627 epoch total loss 2.7311604\n",
      "Trained batch 743 batch loss 2.59843731 epoch total loss 2.73098159\n",
      "Trained batch 744 batch loss 2.80993462 epoch total loss 2.73108768\n",
      "Trained batch 745 batch loss 2.84853578 epoch total loss 2.73124552\n",
      "Trained batch 746 batch loss 2.50025392 epoch total loss 2.73093581\n",
      "Trained batch 747 batch loss 2.63304543 epoch total loss 2.73080468\n",
      "Trained batch 748 batch loss 2.34863544 epoch total loss 2.73029375\n",
      "Trained batch 749 batch loss 2.27681684 epoch total loss 2.72968841\n",
      "Trained batch 750 batch loss 2.38312578 epoch total loss 2.72922635\n",
      "Trained batch 751 batch loss 2.16328788 epoch total loss 2.72847271\n",
      "Trained batch 752 batch loss 2.2190423 epoch total loss 2.72779512\n",
      "Trained batch 753 batch loss 2.38840675 epoch total loss 2.72734451\n",
      "Trained batch 754 batch loss 2.92287755 epoch total loss 2.72760391\n",
      "Trained batch 755 batch loss 3.8824389 epoch total loss 2.72913337\n",
      "Trained batch 756 batch loss 3.3908844 epoch total loss 2.7300086\n",
      "Trained batch 757 batch loss 3.04091835 epoch total loss 2.7304194\n",
      "Trained batch 758 batch loss 2.80098081 epoch total loss 2.73051262\n",
      "Trained batch 759 batch loss 2.60816383 epoch total loss 2.73035145\n",
      "Trained batch 760 batch loss 2.54825664 epoch total loss 2.73011184\n",
      "Trained batch 761 batch loss 2.44400692 epoch total loss 2.72973609\n",
      "Trained batch 762 batch loss 2.72750902 epoch total loss 2.72973323\n",
      "Trained batch 763 batch loss 2.79274273 epoch total loss 2.72981572\n",
      "Trained batch 764 batch loss 2.92625856 epoch total loss 2.73007274\n",
      "Trained batch 765 batch loss 2.51823616 epoch total loss 2.72979593\n",
      "Trained batch 766 batch loss 2.79659557 epoch total loss 2.72988319\n",
      "Trained batch 767 batch loss 2.44472742 epoch total loss 2.7295115\n",
      "Trained batch 768 batch loss 2.58784819 epoch total loss 2.7293272\n",
      "Trained batch 769 batch loss 2.69500518 epoch total loss 2.72928262\n",
      "Trained batch 770 batch loss 2.87475157 epoch total loss 2.72947168\n",
      "Trained batch 771 batch loss 2.90481091 epoch total loss 2.7296989\n",
      "Trained batch 772 batch loss 2.76700497 epoch total loss 2.7297473\n",
      "Trained batch 773 batch loss 2.49000835 epoch total loss 2.72943735\n",
      "Trained batch 774 batch loss 2.87475944 epoch total loss 2.72962499\n",
      "Trained batch 775 batch loss 2.7083106 epoch total loss 2.72959733\n",
      "Trained batch 776 batch loss 2.7155087 epoch total loss 2.72957945\n",
      "Trained batch 777 batch loss 2.7763145 epoch total loss 2.72963953\n",
      "Trained batch 778 batch loss 2.72475791 epoch total loss 2.72963333\n",
      "Trained batch 779 batch loss 2.78491592 epoch total loss 2.72970438\n",
      "Trained batch 780 batch loss 2.28771162 epoch total loss 2.72913766\n",
      "Trained batch 781 batch loss 2.70681977 epoch total loss 2.72910905\n",
      "Trained batch 782 batch loss 2.60059571 epoch total loss 2.72894454\n",
      "Trained batch 783 batch loss 2.54768705 epoch total loss 2.72871304\n",
      "Trained batch 784 batch loss 2.47662401 epoch total loss 2.72839141\n",
      "Trained batch 785 batch loss 2.7768836 epoch total loss 2.72845316\n",
      "Trained batch 786 batch loss 3.02535057 epoch total loss 2.72883081\n",
      "Trained batch 787 batch loss 3.06259203 epoch total loss 2.72925496\n",
      "Trained batch 788 batch loss 2.72861123 epoch total loss 2.72925401\n",
      "Trained batch 789 batch loss 2.65743494 epoch total loss 2.72916293\n",
      "Trained batch 790 batch loss 2.61148572 epoch total loss 2.72901416\n",
      "Trained batch 791 batch loss 2.7026341 epoch total loss 2.72898078\n",
      "Trained batch 792 batch loss 2.72003365 epoch total loss 2.72896934\n",
      "Trained batch 793 batch loss 2.80909777 epoch total loss 2.72907043\n",
      "Trained batch 794 batch loss 3.006423 epoch total loss 2.72941971\n",
      "Trained batch 795 batch loss 2.88446164 epoch total loss 2.72961473\n",
      "Trained batch 796 batch loss 2.71082282 epoch total loss 2.72959137\n",
      "Trained batch 797 batch loss 2.76009846 epoch total loss 2.72962952\n",
      "Trained batch 798 batch loss 2.8392849 epoch total loss 2.72976685\n",
      "Trained batch 799 batch loss 2.80568027 epoch total loss 2.72986197\n",
      "Trained batch 800 batch loss 2.50247908 epoch total loss 2.72957754\n",
      "Trained batch 801 batch loss 2.98401546 epoch total loss 2.72989535\n",
      "Trained batch 802 batch loss 3.06529951 epoch total loss 2.73031354\n",
      "Trained batch 803 batch loss 2.85444522 epoch total loss 2.73046803\n",
      "Trained batch 804 batch loss 2.73316598 epoch total loss 2.73047137\n",
      "Trained batch 805 batch loss 2.41579 epoch total loss 2.7300806\n",
      "Trained batch 806 batch loss 2.81276751 epoch total loss 2.73018312\n",
      "Trained batch 807 batch loss 2.7231698 epoch total loss 2.7301743\n",
      "Trained batch 808 batch loss 2.79921913 epoch total loss 2.73026\n",
      "Trained batch 809 batch loss 2.71191883 epoch total loss 2.73023725\n",
      "Trained batch 810 batch loss 2.62767291 epoch total loss 2.73011065\n",
      "Trained batch 811 batch loss 3.01571298 epoch total loss 2.73046279\n",
      "Trained batch 812 batch loss 2.75288248 epoch total loss 2.73049045\n",
      "Trained batch 813 batch loss 2.58874559 epoch total loss 2.73031616\n",
      "Trained batch 814 batch loss 2.57517147 epoch total loss 2.73012567\n",
      "Trained batch 815 batch loss 2.57748938 epoch total loss 2.72993827\n",
      "Trained batch 816 batch loss 2.56495094 epoch total loss 2.72973609\n",
      "Trained batch 817 batch loss 2.50629091 epoch total loss 2.72946262\n",
      "Trained batch 818 batch loss 2.34849739 epoch total loss 2.72899675\n",
      "Trained batch 819 batch loss 2.44571733 epoch total loss 2.72865105\n",
      "Trained batch 820 batch loss 2.50950432 epoch total loss 2.72838378\n",
      "Trained batch 821 batch loss 2.45022726 epoch total loss 2.72804499\n",
      "Trained batch 822 batch loss 2.65594316 epoch total loss 2.72795725\n",
      "Trained batch 823 batch loss 2.63305759 epoch total loss 2.72784185\n",
      "Trained batch 824 batch loss 2.60221791 epoch total loss 2.7276895\n",
      "Trained batch 825 batch loss 2.57012892 epoch total loss 2.72749853\n",
      "Trained batch 826 batch loss 2.23508215 epoch total loss 2.72690248\n",
      "Trained batch 827 batch loss 2.30860519 epoch total loss 2.72639656\n",
      "Trained batch 828 batch loss 2.55965686 epoch total loss 2.7261951\n",
      "Trained batch 829 batch loss 2.33871412 epoch total loss 2.72572756\n",
      "Trained batch 830 batch loss 2.86821 epoch total loss 2.72589922\n",
      "Trained batch 831 batch loss 2.45791507 epoch total loss 2.72557688\n",
      "Trained batch 832 batch loss 2.79845762 epoch total loss 2.72566438\n",
      "Trained batch 833 batch loss 2.38919282 epoch total loss 2.72526026\n",
      "Trained batch 834 batch loss 2.35405016 epoch total loss 2.72481513\n",
      "Trained batch 835 batch loss 2.40122128 epoch total loss 2.72442746\n",
      "Trained batch 836 batch loss 2.38757944 epoch total loss 2.72402477\n",
      "Trained batch 837 batch loss 2.83849573 epoch total loss 2.72416139\n",
      "Trained batch 838 batch loss 2.88144946 epoch total loss 2.72434902\n",
      "Trained batch 839 batch loss 2.82328677 epoch total loss 2.7244668\n",
      "Trained batch 840 batch loss 2.94758677 epoch total loss 2.7247324\n",
      "Trained batch 841 batch loss 2.71454859 epoch total loss 2.72472024\n",
      "Trained batch 842 batch loss 2.82826495 epoch total loss 2.72484326\n",
      "Trained batch 843 batch loss 2.85814834 epoch total loss 2.72500157\n",
      "Trained batch 844 batch loss 2.5765636 epoch total loss 2.72482586\n",
      "Trained batch 845 batch loss 2.69198728 epoch total loss 2.72478676\n",
      "Trained batch 846 batch loss 2.6674962 epoch total loss 2.72471905\n",
      "Trained batch 847 batch loss 2.68220091 epoch total loss 2.72466874\n",
      "Trained batch 848 batch loss 2.77707338 epoch total loss 2.72473049\n",
      "Trained batch 849 batch loss 3.06568122 epoch total loss 2.72513223\n",
      "Trained batch 850 batch loss 2.94320345 epoch total loss 2.72538853\n",
      "Trained batch 851 batch loss 2.81388259 epoch total loss 2.72549272\n",
      "Trained batch 852 batch loss 2.63199878 epoch total loss 2.72538304\n",
      "Trained batch 853 batch loss 2.6526289 epoch total loss 2.72529769\n",
      "Trained batch 854 batch loss 2.67734051 epoch total loss 2.72524142\n",
      "Trained batch 855 batch loss 2.74607301 epoch total loss 2.72526574\n",
      "Trained batch 856 batch loss 2.75448108 epoch total loss 2.72529984\n",
      "Trained batch 857 batch loss 2.58793688 epoch total loss 2.72513962\n",
      "Trained batch 858 batch loss 2.74684167 epoch total loss 2.72516489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 859 batch loss 2.4491303 epoch total loss 2.7248435\n",
      "Trained batch 860 batch loss 2.39201212 epoch total loss 2.72445655\n",
      "Trained batch 861 batch loss 2.35159302 epoch total loss 2.72402358\n",
      "Trained batch 862 batch loss 2.72477198 epoch total loss 2.72402453\n",
      "Trained batch 863 batch loss 2.86197472 epoch total loss 2.72418451\n",
      "Trained batch 864 batch loss 2.83048701 epoch total loss 2.72430754\n",
      "Trained batch 865 batch loss 2.58965373 epoch total loss 2.72415185\n",
      "Trained batch 866 batch loss 2.67724204 epoch total loss 2.72409773\n",
      "Trained batch 867 batch loss 2.75214386 epoch total loss 2.72413015\n",
      "Trained batch 868 batch loss 2.60433388 epoch total loss 2.72399211\n",
      "Trained batch 869 batch loss 2.31612134 epoch total loss 2.72352266\n",
      "Trained batch 870 batch loss 2.39727712 epoch total loss 2.72314763\n",
      "Trained batch 871 batch loss 2.5009923 epoch total loss 2.72289252\n",
      "Trained batch 872 batch loss 2.84874415 epoch total loss 2.72303677\n",
      "Trained batch 873 batch loss 3.02752924 epoch total loss 2.72338557\n",
      "Trained batch 874 batch loss 2.83856916 epoch total loss 2.72351742\n",
      "Trained batch 875 batch loss 3.21270061 epoch total loss 2.72407651\n",
      "Trained batch 876 batch loss 2.64339495 epoch total loss 2.72398424\n",
      "Trained batch 877 batch loss 3.09412384 epoch total loss 2.72440648\n",
      "Trained batch 878 batch loss 2.58657551 epoch total loss 2.7242496\n",
      "Trained batch 879 batch loss 2.93750763 epoch total loss 2.72449207\n",
      "Trained batch 880 batch loss 2.78341031 epoch total loss 2.72455907\n",
      "Trained batch 881 batch loss 2.91986752 epoch total loss 2.7247808\n",
      "Trained batch 882 batch loss 2.88126254 epoch total loss 2.72495842\n",
      "Trained batch 883 batch loss 2.99898338 epoch total loss 2.72526884\n",
      "Trained batch 884 batch loss 2.84562111 epoch total loss 2.72540498\n",
      "Trained batch 885 batch loss 2.66721034 epoch total loss 2.72533941\n",
      "Trained batch 886 batch loss 2.56673956 epoch total loss 2.72516012\n",
      "Trained batch 887 batch loss 2.70377088 epoch total loss 2.72513628\n",
      "Trained batch 888 batch loss 2.73161483 epoch total loss 2.72514367\n",
      "Trained batch 889 batch loss 2.35245585 epoch total loss 2.72472453\n",
      "Trained batch 890 batch loss 2.68536639 epoch total loss 2.72468019\n",
      "Trained batch 891 batch loss 2.27976751 epoch total loss 2.72418094\n",
      "Trained batch 892 batch loss 2.41523242 epoch total loss 2.72383451\n",
      "Trained batch 893 batch loss 2.45491648 epoch total loss 2.72353339\n",
      "Trained batch 894 batch loss 2.32101583 epoch total loss 2.72308302\n",
      "Trained batch 895 batch loss 2.58315945 epoch total loss 2.72292686\n",
      "Trained batch 896 batch loss 2.47963715 epoch total loss 2.72265553\n",
      "Trained batch 897 batch loss 2.44473863 epoch total loss 2.72234559\n",
      "Trained batch 898 batch loss 2.29386067 epoch total loss 2.72186875\n",
      "Trained batch 899 batch loss 2.18080878 epoch total loss 2.72126698\n",
      "Trained batch 900 batch loss 2.12391567 epoch total loss 2.72060323\n",
      "Trained batch 901 batch loss 1.99906504 epoch total loss 2.71980238\n",
      "Trained batch 902 batch loss 2.08377647 epoch total loss 2.71909738\n",
      "Trained batch 903 batch loss 2.26283956 epoch total loss 2.71859217\n",
      "Trained batch 904 batch loss 1.87305546 epoch total loss 2.71765685\n",
      "Trained batch 905 batch loss 2.31104231 epoch total loss 2.71720743\n",
      "Trained batch 906 batch loss 2.70890832 epoch total loss 2.71719837\n",
      "Trained batch 907 batch loss 2.98550081 epoch total loss 2.71749425\n",
      "Trained batch 908 batch loss 2.95281959 epoch total loss 2.71775365\n",
      "Trained batch 909 batch loss 2.74767494 epoch total loss 2.71778631\n",
      "Trained batch 910 batch loss 2.67098975 epoch total loss 2.71773481\n",
      "Trained batch 911 batch loss 2.612113 epoch total loss 2.7176187\n",
      "Trained batch 912 batch loss 2.81714439 epoch total loss 2.7177279\n",
      "Trained batch 913 batch loss 2.44188738 epoch total loss 2.71742582\n",
      "Trained batch 914 batch loss 2.61317778 epoch total loss 2.71731186\n",
      "Trained batch 915 batch loss 2.91810536 epoch total loss 2.71753144\n",
      "Trained batch 916 batch loss 2.67678809 epoch total loss 2.71748686\n",
      "Trained batch 917 batch loss 2.76235247 epoch total loss 2.71753597\n",
      "Trained batch 918 batch loss 2.62589788 epoch total loss 2.71743631\n",
      "Trained batch 919 batch loss 2.82500935 epoch total loss 2.71755314\n",
      "Trained batch 920 batch loss 2.79825068 epoch total loss 2.71764112\n",
      "Trained batch 921 batch loss 2.81741548 epoch total loss 2.71774936\n",
      "Trained batch 922 batch loss 2.86989069 epoch total loss 2.71791434\n",
      "Trained batch 923 batch loss 2.65568328 epoch total loss 2.71784687\n",
      "Trained batch 924 batch loss 2.53587246 epoch total loss 2.71765\n",
      "Trained batch 925 batch loss 2.56358862 epoch total loss 2.71748328\n",
      "Trained batch 926 batch loss 2.83794641 epoch total loss 2.71761346\n",
      "Trained batch 927 batch loss 2.62178135 epoch total loss 2.71751\n",
      "Trained batch 928 batch loss 2.71160698 epoch total loss 2.71750379\n",
      "Trained batch 929 batch loss 2.57880545 epoch total loss 2.71735454\n",
      "Trained batch 930 batch loss 2.60473347 epoch total loss 2.71723342\n",
      "Trained batch 931 batch loss 2.4290309 epoch total loss 2.71692371\n",
      "Trained batch 932 batch loss 2.72251987 epoch total loss 2.71692967\n",
      "Trained batch 933 batch loss 2.85543 epoch total loss 2.71707821\n",
      "Trained batch 934 batch loss 2.69092655 epoch total loss 2.71705\n",
      "Trained batch 935 batch loss 2.61949348 epoch total loss 2.71694565\n",
      "Trained batch 936 batch loss 2.54099154 epoch total loss 2.71675777\n",
      "Trained batch 937 batch loss 2.39393759 epoch total loss 2.71641326\n",
      "Trained batch 938 batch loss 2.87482786 epoch total loss 2.71658206\n",
      "Trained batch 939 batch loss 2.77506018 epoch total loss 2.71664453\n",
      "Trained batch 940 batch loss 2.53918505 epoch total loss 2.71645594\n",
      "Trained batch 941 batch loss 2.45921731 epoch total loss 2.71618247\n",
      "Trained batch 942 batch loss 2.51018119 epoch total loss 2.71596384\n",
      "Trained batch 943 batch loss 2.31031466 epoch total loss 2.71553373\n",
      "Trained batch 944 batch loss 2.844944 epoch total loss 2.71567082\n",
      "Trained batch 945 batch loss 2.5965457 epoch total loss 2.7155447\n",
      "Trained batch 946 batch loss 2.73347569 epoch total loss 2.71556354\n",
      "Trained batch 947 batch loss 2.58672571 epoch total loss 2.7154274\n",
      "Trained batch 948 batch loss 2.60056806 epoch total loss 2.71530628\n",
      "Trained batch 949 batch loss 2.73867702 epoch total loss 2.71533108\n",
      "Trained batch 950 batch loss 2.3472569 epoch total loss 2.71494341\n",
      "Trained batch 951 batch loss 2.5344677 epoch total loss 2.71475363\n",
      "Trained batch 952 batch loss 2.59978771 epoch total loss 2.71463299\n",
      "Trained batch 953 batch loss 2.72555947 epoch total loss 2.71464443\n",
      "Trained batch 954 batch loss 2.63651466 epoch total loss 2.71456242\n",
      "Trained batch 955 batch loss 2.44854641 epoch total loss 2.71428394\n",
      "Trained batch 956 batch loss 2.55240154 epoch total loss 2.71411467\n",
      "Trained batch 957 batch loss 2.94740915 epoch total loss 2.71435857\n",
      "Trained batch 958 batch loss 2.7508781 epoch total loss 2.71439672\n",
      "Trained batch 959 batch loss 2.68497849 epoch total loss 2.7143662\n",
      "Trained batch 960 batch loss 2.12250304 epoch total loss 2.71374965\n",
      "Trained batch 961 batch loss 2.34243584 epoch total loss 2.71336341\n",
      "Trained batch 962 batch loss 2.3646419 epoch total loss 2.71300101\n",
      "Trained batch 963 batch loss 2.92065024 epoch total loss 2.71321654\n",
      "Trained batch 964 batch loss 3.20004487 epoch total loss 2.71372151\n",
      "Trained batch 965 batch loss 2.985888 epoch total loss 2.71400356\n",
      "Trained batch 966 batch loss 2.75264645 epoch total loss 2.71404362\n",
      "Trained batch 967 batch loss 3.05804658 epoch total loss 2.71439934\n",
      "Trained batch 968 batch loss 2.78364182 epoch total loss 2.71447086\n",
      "Trained batch 969 batch loss 2.83917522 epoch total loss 2.71459961\n",
      "Trained batch 970 batch loss 2.98786092 epoch total loss 2.71488118\n",
      "Trained batch 971 batch loss 2.94384313 epoch total loss 2.71511698\n",
      "Trained batch 972 batch loss 2.96084499 epoch total loss 2.71537\n",
      "Trained batch 973 batch loss 2.78575754 epoch total loss 2.71544218\n",
      "Trained batch 974 batch loss 2.76772618 epoch total loss 2.71549606\n",
      "Trained batch 975 batch loss 2.3802557 epoch total loss 2.71515226\n",
      "Trained batch 976 batch loss 2.45669866 epoch total loss 2.71488762\n",
      "Trained batch 977 batch loss 2.35560679 epoch total loss 2.71452\n",
      "Trained batch 978 batch loss 2.60973597 epoch total loss 2.71441269\n",
      "Trained batch 979 batch loss 2.75145268 epoch total loss 2.7144506\n",
      "Trained batch 980 batch loss 2.84626031 epoch total loss 2.71458483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 981 batch loss 2.87168694 epoch total loss 2.71474504\n",
      "Trained batch 982 batch loss 2.62655067 epoch total loss 2.71465516\n",
      "Trained batch 983 batch loss 2.62717032 epoch total loss 2.71456599\n",
      "Trained batch 984 batch loss 2.67975354 epoch total loss 2.71453071\n",
      "Trained batch 985 batch loss 3.03311157 epoch total loss 2.71485424\n",
      "Trained batch 986 batch loss 2.94658184 epoch total loss 2.71508908\n",
      "Trained batch 987 batch loss 2.87004709 epoch total loss 2.7152462\n",
      "Trained batch 988 batch loss 2.65977573 epoch total loss 2.71519\n",
      "Trained batch 989 batch loss 2.76148653 epoch total loss 2.71523666\n",
      "Trained batch 990 batch loss 2.59442043 epoch total loss 2.71511483\n",
      "Trained batch 991 batch loss 3.13179922 epoch total loss 2.71553516\n",
      "Trained batch 992 batch loss 2.84054208 epoch total loss 2.71566129\n",
      "Trained batch 993 batch loss 2.86516142 epoch total loss 2.71581197\n",
      "Trained batch 994 batch loss 3.00211954 epoch total loss 2.7161\n",
      "Trained batch 995 batch loss 3.12196302 epoch total loss 2.71650815\n",
      "Trained batch 996 batch loss 2.81624985 epoch total loss 2.71660805\n",
      "Trained batch 997 batch loss 2.68794489 epoch total loss 2.71657944\n",
      "Trained batch 998 batch loss 2.96741915 epoch total loss 2.71683097\n",
      "Trained batch 999 batch loss 2.79880166 epoch total loss 2.71691298\n",
      "Trained batch 1000 batch loss 2.89336705 epoch total loss 2.71708941\n",
      "Trained batch 1001 batch loss 2.6456244 epoch total loss 2.71701789\n",
      "Trained batch 1002 batch loss 2.6889987 epoch total loss 2.71698976\n",
      "Trained batch 1003 batch loss 2.551682 epoch total loss 2.71682501\n",
      "Trained batch 1004 batch loss 2.48470926 epoch total loss 2.71659374\n",
      "Trained batch 1005 batch loss 2.54768968 epoch total loss 2.71642566\n",
      "Trained batch 1006 batch loss 2.48223066 epoch total loss 2.71619272\n",
      "Trained batch 1007 batch loss 2.60096288 epoch total loss 2.71607852\n",
      "Trained batch 1008 batch loss 2.76703596 epoch total loss 2.71612906\n",
      "Trained batch 1009 batch loss 2.66259885 epoch total loss 2.71607614\n",
      "Trained batch 1010 batch loss 3.21936345 epoch total loss 2.71657443\n",
      "Trained batch 1011 batch loss 2.83895564 epoch total loss 2.71669555\n",
      "Trained batch 1012 batch loss 2.54328704 epoch total loss 2.71652412\n",
      "Trained batch 1013 batch loss 2.78553677 epoch total loss 2.71659231\n",
      "Trained batch 1014 batch loss 2.84907103 epoch total loss 2.71672297\n",
      "Trained batch 1015 batch loss 2.61171317 epoch total loss 2.71661949\n",
      "Trained batch 1016 batch loss 2.56973457 epoch total loss 2.71647501\n",
      "Trained batch 1017 batch loss 2.31545138 epoch total loss 2.71608067\n",
      "Trained batch 1018 batch loss 2.45540786 epoch total loss 2.7158246\n",
      "Trained batch 1019 batch loss 2.66206574 epoch total loss 2.71577191\n",
      "Trained batch 1020 batch loss 2.59162331 epoch total loss 2.71565\n",
      "Trained batch 1021 batch loss 2.91346788 epoch total loss 2.71584392\n",
      "Trained batch 1022 batch loss 2.74248362 epoch total loss 2.71587\n",
      "Trained batch 1023 batch loss 2.7588203 epoch total loss 2.71591187\n",
      "Trained batch 1024 batch loss 2.49512076 epoch total loss 2.71569633\n",
      "Trained batch 1025 batch loss 2.49277115 epoch total loss 2.71547866\n",
      "Trained batch 1026 batch loss 2.48556447 epoch total loss 2.71525478\n",
      "Trained batch 1027 batch loss 2.48425317 epoch total loss 2.71503\n",
      "Trained batch 1028 batch loss 2.39902616 epoch total loss 2.71472239\n",
      "Trained batch 1029 batch loss 2.69136071 epoch total loss 2.71469975\n",
      "Trained batch 1030 batch loss 2.67723513 epoch total loss 2.71466327\n",
      "Trained batch 1031 batch loss 2.62045908 epoch total loss 2.71457195\n",
      "Trained batch 1032 batch loss 2.92230248 epoch total loss 2.71477318\n",
      "Trained batch 1033 batch loss 2.74800253 epoch total loss 2.71480536\n",
      "Trained batch 1034 batch loss 2.84804249 epoch total loss 2.71493435\n",
      "Trained batch 1035 batch loss 2.87002015 epoch total loss 2.71508431\n",
      "Trained batch 1036 batch loss 2.61619473 epoch total loss 2.71498895\n",
      "Trained batch 1037 batch loss 2.57947779 epoch total loss 2.71485829\n",
      "Trained batch 1038 batch loss 2.34344292 epoch total loss 2.71450067\n",
      "Trained batch 1039 batch loss 2.56353831 epoch total loss 2.71435523\n",
      "Trained batch 1040 batch loss 2.2597332 epoch total loss 2.71391821\n",
      "Trained batch 1041 batch loss 2.34162521 epoch total loss 2.71356034\n",
      "Trained batch 1042 batch loss 2.25866365 epoch total loss 2.7131238\n",
      "Trained batch 1043 batch loss 2.28637362 epoch total loss 2.71271467\n",
      "Trained batch 1044 batch loss 2.39714861 epoch total loss 2.71241236\n",
      "Trained batch 1045 batch loss 2.51508284 epoch total loss 2.71222353\n",
      "Trained batch 1046 batch loss 2.42105412 epoch total loss 2.7119453\n",
      "Trained batch 1047 batch loss 2.44797254 epoch total loss 2.71169329\n",
      "Trained batch 1048 batch loss 2.43092871 epoch total loss 2.7114253\n",
      "Trained batch 1049 batch loss 2.4243865 epoch total loss 2.7111516\n",
      "Trained batch 1050 batch loss 2.61924076 epoch total loss 2.7110641\n",
      "Trained batch 1051 batch loss 2.90377092 epoch total loss 2.71124744\n",
      "Trained batch 1052 batch loss 2.34006619 epoch total loss 2.71089458\n",
      "Trained batch 1053 batch loss 2.15493751 epoch total loss 2.71036673\n",
      "Trained batch 1054 batch loss 2.64095402 epoch total loss 2.71030068\n",
      "Trained batch 1055 batch loss 2.8069396 epoch total loss 2.71039224\n",
      "Trained batch 1056 batch loss 2.59557033 epoch total loss 2.71028352\n",
      "Trained batch 1057 batch loss 2.82958841 epoch total loss 2.71039629\n",
      "Trained batch 1058 batch loss 2.81118894 epoch total loss 2.71049166\n",
      "Trained batch 1059 batch loss 2.67222261 epoch total loss 2.71045542\n",
      "Trained batch 1060 batch loss 2.69194627 epoch total loss 2.71043801\n",
      "Trained batch 1061 batch loss 2.59006906 epoch total loss 2.71032453\n",
      "Trained batch 1062 batch loss 2.82181025 epoch total loss 2.71042943\n",
      "Trained batch 1063 batch loss 2.9296937 epoch total loss 2.71063566\n",
      "Trained batch 1064 batch loss 2.94891715 epoch total loss 2.71085978\n",
      "Trained batch 1065 batch loss 2.76753736 epoch total loss 2.71091294\n",
      "Trained batch 1066 batch loss 2.53571415 epoch total loss 2.71074867\n",
      "Trained batch 1067 batch loss 2.9620204 epoch total loss 2.71098399\n",
      "Trained batch 1068 batch loss 3.00742149 epoch total loss 2.71126151\n",
      "Trained batch 1069 batch loss 2.7961843 epoch total loss 2.7113409\n",
      "Trained batch 1070 batch loss 2.433321 epoch total loss 2.71108103\n",
      "Trained batch 1071 batch loss 2.55442905 epoch total loss 2.71093488\n",
      "Trained batch 1072 batch loss 2.28322363 epoch total loss 2.71053576\n",
      "Trained batch 1073 batch loss 2.48820543 epoch total loss 2.71032858\n",
      "Trained batch 1074 batch loss 2.38525581 epoch total loss 2.71002603\n",
      "Trained batch 1075 batch loss 2.87380457 epoch total loss 2.71017838\n",
      "Trained batch 1076 batch loss 2.61866641 epoch total loss 2.71009326\n",
      "Trained batch 1077 batch loss 2.97445107 epoch total loss 2.71033859\n",
      "Trained batch 1078 batch loss 2.36846733 epoch total loss 2.7100215\n",
      "Trained batch 1079 batch loss 2.89298153 epoch total loss 2.71019101\n",
      "Trained batch 1080 batch loss 3.18660021 epoch total loss 2.71063209\n",
      "Trained batch 1081 batch loss 2.89454317 epoch total loss 2.71080232\n",
      "Trained batch 1082 batch loss 2.79676461 epoch total loss 2.71088171\n",
      "Trained batch 1083 batch loss 2.86720443 epoch total loss 2.71102619\n",
      "Trained batch 1084 batch loss 2.9148159 epoch total loss 2.71121407\n",
      "Trained batch 1085 batch loss 2.33759284 epoch total loss 2.71086979\n",
      "Trained batch 1086 batch loss 2.37911272 epoch total loss 2.71056437\n",
      "Trained batch 1087 batch loss 2.7163322 epoch total loss 2.71056962\n",
      "Trained batch 1088 batch loss 2.72534537 epoch total loss 2.71058321\n",
      "Trained batch 1089 batch loss 2.61338258 epoch total loss 2.7104938\n",
      "Trained batch 1090 batch loss 2.62489867 epoch total loss 2.71041536\n",
      "Trained batch 1091 batch loss 2.62764502 epoch total loss 2.71033955\n",
      "Trained batch 1092 batch loss 2.59705067 epoch total loss 2.71023583\n",
      "Trained batch 1093 batch loss 2.58303738 epoch total loss 2.71011949\n",
      "Trained batch 1094 batch loss 2.57498765 epoch total loss 2.70999599\n",
      "Trained batch 1095 batch loss 2.45597696 epoch total loss 2.709764\n",
      "Trained batch 1096 batch loss 2.72068977 epoch total loss 2.70977402\n",
      "Trained batch 1097 batch loss 2.39618444 epoch total loss 2.70948815\n",
      "Trained batch 1098 batch loss 2.57236814 epoch total loss 2.70936322\n",
      "Trained batch 1099 batch loss 2.62973046 epoch total loss 2.70929074\n",
      "Trained batch 1100 batch loss 2.42085552 epoch total loss 2.70902848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1101 batch loss 2.31086349 epoch total loss 2.7086668\n",
      "Trained batch 1102 batch loss 2.46103072 epoch total loss 2.70844197\n",
      "Trained batch 1103 batch loss 2.48259306 epoch total loss 2.70823741\n",
      "Trained batch 1104 batch loss 2.54765224 epoch total loss 2.70809174\n",
      "Trained batch 1105 batch loss 2.64489317 epoch total loss 2.70803452\n",
      "Trained batch 1106 batch loss 2.58845949 epoch total loss 2.70792627\n",
      "Trained batch 1107 batch loss 2.56282258 epoch total loss 2.70779514\n",
      "Trained batch 1108 batch loss 2.54313803 epoch total loss 2.70764661\n",
      "Trained batch 1109 batch loss 2.47947311 epoch total loss 2.70744085\n",
      "Trained batch 1110 batch loss 2.41517115 epoch total loss 2.70717764\n",
      "Trained batch 1111 batch loss 2.19981027 epoch total loss 2.70672107\n",
      "Trained batch 1112 batch loss 2.79231119 epoch total loss 2.70679784\n",
      "Trained batch 1113 batch loss 2.44858503 epoch total loss 2.70656586\n",
      "Trained batch 1114 batch loss 2.46215987 epoch total loss 2.70634627\n",
      "Trained batch 1115 batch loss 2.15140152 epoch total loss 2.70584869\n",
      "Trained batch 1116 batch loss 3.07203388 epoch total loss 2.70617676\n",
      "Trained batch 1117 batch loss 3.16499567 epoch total loss 2.70658755\n",
      "Trained batch 1118 batch loss 3.23549986 epoch total loss 2.70706081\n",
      "Trained batch 1119 batch loss 2.96804595 epoch total loss 2.70729399\n",
      "Trained batch 1120 batch loss 2.70212293 epoch total loss 2.70728946\n",
      "Trained batch 1121 batch loss 2.60487843 epoch total loss 2.70719814\n",
      "Trained batch 1122 batch loss 2.84148908 epoch total loss 2.70731783\n",
      "Trained batch 1123 batch loss 3.28416777 epoch total loss 2.70783138\n",
      "Trained batch 1124 batch loss 3.30430603 epoch total loss 2.7083621\n",
      "Trained batch 1125 batch loss 3.05776072 epoch total loss 2.70867276\n",
      "Trained batch 1126 batch loss 2.68725586 epoch total loss 2.70865369\n",
      "Trained batch 1127 batch loss 2.84390211 epoch total loss 2.70877385\n",
      "Trained batch 1128 batch loss 2.62934923 epoch total loss 2.70870352\n",
      "Trained batch 1129 batch loss 2.73028612 epoch total loss 2.70872259\n",
      "Trained batch 1130 batch loss 2.97034168 epoch total loss 2.7089541\n",
      "Trained batch 1131 batch loss 2.4486289 epoch total loss 2.70872402\n",
      "Trained batch 1132 batch loss 2.60614014 epoch total loss 2.70863342\n",
      "Trained batch 1133 batch loss 3.04951334 epoch total loss 2.70893431\n",
      "Trained batch 1134 batch loss 2.6184423 epoch total loss 2.70885444\n",
      "Trained batch 1135 batch loss 2.66975069 epoch total loss 2.70882\n",
      "Trained batch 1136 batch loss 2.68168402 epoch total loss 2.70879602\n",
      "Trained batch 1137 batch loss 2.65188742 epoch total loss 2.70874596\n",
      "Trained batch 1138 batch loss 2.65402532 epoch total loss 2.70869803\n",
      "Trained batch 1139 batch loss 2.81703806 epoch total loss 2.70879316\n",
      "Trained batch 1140 batch loss 3.04473329 epoch total loss 2.70908785\n",
      "Trained batch 1141 batch loss 2.63883352 epoch total loss 2.70902634\n",
      "Trained batch 1142 batch loss 2.48426676 epoch total loss 2.70882964\n",
      "Trained batch 1143 batch loss 2.57659793 epoch total loss 2.70871401\n",
      "Trained batch 1144 batch loss 2.43887353 epoch total loss 2.70847821\n",
      "Trained batch 1145 batch loss 2.7148788 epoch total loss 2.7084837\n",
      "Trained batch 1146 batch loss 2.62873507 epoch total loss 2.70841408\n",
      "Trained batch 1147 batch loss 2.77842474 epoch total loss 2.70847511\n",
      "Trained batch 1148 batch loss 2.57577133 epoch total loss 2.70835924\n",
      "Trained batch 1149 batch loss 2.70514631 epoch total loss 2.70835638\n",
      "Trained batch 1150 batch loss 2.56562424 epoch total loss 2.7082324\n",
      "Trained batch 1151 batch loss 2.43359256 epoch total loss 2.70799375\n",
      "Trained batch 1152 batch loss 2.17208385 epoch total loss 2.70752859\n",
      "Trained batch 1153 batch loss 2.35648012 epoch total loss 2.70722413\n",
      "Trained batch 1154 batch loss 2.60061789 epoch total loss 2.70713162\n",
      "Trained batch 1155 batch loss 2.26263952 epoch total loss 2.70674706\n",
      "Trained batch 1156 batch loss 2.57394648 epoch total loss 2.70663214\n",
      "Trained batch 1157 batch loss 2.77873278 epoch total loss 2.70669436\n",
      "Trained batch 1158 batch loss 2.24522018 epoch total loss 2.70629597\n",
      "Trained batch 1159 batch loss 2.66229272 epoch total loss 2.70625806\n",
      "Trained batch 1160 batch loss 2.36644077 epoch total loss 2.70596504\n",
      "Trained batch 1161 batch loss 2.64310813 epoch total loss 2.70591092\n",
      "Trained batch 1162 batch loss 2.45008469 epoch total loss 2.70569086\n",
      "Trained batch 1163 batch loss 2.59959793 epoch total loss 2.70559955\n",
      "Trained batch 1164 batch loss 2.52176023 epoch total loss 2.70544171\n",
      "Trained batch 1165 batch loss 2.57838249 epoch total loss 2.70533252\n",
      "Trained batch 1166 batch loss 2.59145069 epoch total loss 2.705235\n",
      "Trained batch 1167 batch loss 2.90287304 epoch total loss 2.70540428\n",
      "Trained batch 1168 batch loss 3.19490767 epoch total loss 2.70582318\n",
      "Trained batch 1169 batch loss 2.93764377 epoch total loss 2.70602179\n",
      "Trained batch 1170 batch loss 2.64712429 epoch total loss 2.70597148\n",
      "Trained batch 1171 batch loss 2.97880507 epoch total loss 2.70620441\n",
      "Trained batch 1172 batch loss 2.57719851 epoch total loss 2.70609426\n",
      "Trained batch 1173 batch loss 2.77993822 epoch total loss 2.70615721\n",
      "Trained batch 1174 batch loss 2.89416122 epoch total loss 2.70631742\n",
      "Trained batch 1175 batch loss 2.84010148 epoch total loss 2.70643115\n",
      "Trained batch 1176 batch loss 2.30799437 epoch total loss 2.70609236\n",
      "Trained batch 1177 batch loss 2.50883484 epoch total loss 2.70592475\n",
      "Trained batch 1178 batch loss 2.90357423 epoch total loss 2.7060926\n",
      "Trained batch 1179 batch loss 2.84268737 epoch total loss 2.70620847\n",
      "Trained batch 1180 batch loss 2.53356695 epoch total loss 2.70606208\n",
      "Trained batch 1181 batch loss 2.4255209 epoch total loss 2.70582461\n",
      "Trained batch 1182 batch loss 2.83532 epoch total loss 2.70593405\n",
      "Trained batch 1183 batch loss 2.7226243 epoch total loss 2.70594811\n",
      "Trained batch 1184 batch loss 2.76175594 epoch total loss 2.70599532\n",
      "Trained batch 1185 batch loss 2.68352461 epoch total loss 2.70597649\n",
      "Trained batch 1186 batch loss 2.4791739 epoch total loss 2.70578527\n",
      "Trained batch 1187 batch loss 2.94986105 epoch total loss 2.70599103\n",
      "Trained batch 1188 batch loss 2.80099869 epoch total loss 2.7060709\n",
      "Trained batch 1189 batch loss 2.72800517 epoch total loss 2.7060895\n",
      "Trained batch 1190 batch loss 2.6663394 epoch total loss 2.70605588\n",
      "Trained batch 1191 batch loss 2.58147144 epoch total loss 2.70595145\n",
      "Trained batch 1192 batch loss 2.60217905 epoch total loss 2.70586443\n",
      "Trained batch 1193 batch loss 2.40454865 epoch total loss 2.70561194\n",
      "Trained batch 1194 batch loss 2.54717207 epoch total loss 2.70547915\n",
      "Trained batch 1195 batch loss 2.36056566 epoch total loss 2.70519042\n",
      "Trained batch 1196 batch loss 2.61811256 epoch total loss 2.7051177\n",
      "Trained batch 1197 batch loss 2.84387326 epoch total loss 2.70523381\n",
      "Trained batch 1198 batch loss 2.88324857 epoch total loss 2.70538235\n",
      "Trained batch 1199 batch loss 2.98713923 epoch total loss 2.70561743\n",
      "Trained batch 1200 batch loss 2.92362118 epoch total loss 2.70579886\n",
      "Trained batch 1201 batch loss 2.60276461 epoch total loss 2.70571327\n",
      "Trained batch 1202 batch loss 2.57700968 epoch total loss 2.70560598\n",
      "Trained batch 1203 batch loss 2.7159369 epoch total loss 2.70561457\n",
      "Trained batch 1204 batch loss 2.68905616 epoch total loss 2.70560074\n",
      "Trained batch 1205 batch loss 2.65665913 epoch total loss 2.70556021\n",
      "Trained batch 1206 batch loss 2.58769679 epoch total loss 2.70546246\n",
      "Trained batch 1207 batch loss 2.7453053 epoch total loss 2.70549536\n",
      "Trained batch 1208 batch loss 2.84803891 epoch total loss 2.70561361\n",
      "Trained batch 1209 batch loss 2.76194167 epoch total loss 2.70566\n",
      "Trained batch 1210 batch loss 2.39941835 epoch total loss 2.7054069\n",
      "Trained batch 1211 batch loss 2.07402229 epoch total loss 2.70488548\n",
      "Trained batch 1212 batch loss 2.53594613 epoch total loss 2.70474625\n",
      "Trained batch 1213 batch loss 2.67195725 epoch total loss 2.70471907\n",
      "Trained batch 1214 batch loss 2.90057755 epoch total loss 2.70488048\n",
      "Trained batch 1215 batch loss 2.80198717 epoch total loss 2.70496035\n",
      "Trained batch 1216 batch loss 2.83533049 epoch total loss 2.70506763\n",
      "Trained batch 1217 batch loss 2.6383729 epoch total loss 2.70501304\n",
      "Trained batch 1218 batch loss 2.84248209 epoch total loss 2.70512581\n",
      "Trained batch 1219 batch loss 2.65173268 epoch total loss 2.70508194\n",
      "Trained batch 1220 batch loss 2.50004435 epoch total loss 2.70491385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1221 batch loss 2.41467404 epoch total loss 2.70467615\n",
      "Trained batch 1222 batch loss 2.35618019 epoch total loss 2.704391\n",
      "Trained batch 1223 batch loss 2.37630105 epoch total loss 2.70412278\n",
      "Trained batch 1224 batch loss 2.35779905 epoch total loss 2.70383978\n",
      "Trained batch 1225 batch loss 2.52029228 epoch total loss 2.70369\n",
      "Trained batch 1226 batch loss 2.48147845 epoch total loss 2.70350885\n",
      "Trained batch 1227 batch loss 2.44026375 epoch total loss 2.70329404\n",
      "Trained batch 1228 batch loss 2.97484779 epoch total loss 2.70351529\n",
      "Trained batch 1229 batch loss 2.6851697 epoch total loss 2.70350027\n",
      "Trained batch 1230 batch loss 2.58473659 epoch total loss 2.70340371\n",
      "Trained batch 1231 batch loss 2.46619201 epoch total loss 2.70321107\n",
      "Trained batch 1232 batch loss 2.69292116 epoch total loss 2.70320272\n",
      "Trained batch 1233 batch loss 3.15196848 epoch total loss 2.70356655\n",
      "Trained batch 1234 batch loss 2.91761947 epoch total loss 2.70374012\n",
      "Trained batch 1235 batch loss 2.68956757 epoch total loss 2.70372844\n",
      "Trained batch 1236 batch loss 2.91908097 epoch total loss 2.70390296\n",
      "Trained batch 1237 batch loss 2.61073446 epoch total loss 2.70382762\n",
      "Trained batch 1238 batch loss 2.69920015 epoch total loss 2.7038238\n",
      "Trained batch 1239 batch loss 2.7212739 epoch total loss 2.70383787\n",
      "Trained batch 1240 batch loss 2.76429796 epoch total loss 2.70388675\n",
      "Trained batch 1241 batch loss 2.74538231 epoch total loss 2.70392013\n",
      "Trained batch 1242 batch loss 2.70168376 epoch total loss 2.70391846\n",
      "Trained batch 1243 batch loss 2.76669455 epoch total loss 2.70396876\n",
      "Trained batch 1244 batch loss 2.63650966 epoch total loss 2.70391464\n",
      "Trained batch 1245 batch loss 2.70810556 epoch total loss 2.70391774\n",
      "Trained batch 1246 batch loss 2.82855654 epoch total loss 2.70401788\n",
      "Trained batch 1247 batch loss 2.95610809 epoch total loss 2.70422\n",
      "Trained batch 1248 batch loss 3.02034521 epoch total loss 2.70447326\n",
      "Trained batch 1249 batch loss 2.86719465 epoch total loss 2.70460343\n",
      "Trained batch 1250 batch loss 3.00191331 epoch total loss 2.70484138\n",
      "Trained batch 1251 batch loss 2.75782561 epoch total loss 2.70488381\n",
      "Trained batch 1252 batch loss 2.56893182 epoch total loss 2.70477509\n",
      "Trained batch 1253 batch loss 2.67982078 epoch total loss 2.70475531\n",
      "Trained batch 1254 batch loss 2.6763103 epoch total loss 2.70473266\n",
      "Trained batch 1255 batch loss 3.06127644 epoch total loss 2.70501661\n",
      "Trained batch 1256 batch loss 2.91405582 epoch total loss 2.70518303\n",
      "Trained batch 1257 batch loss 3.13925052 epoch total loss 2.70552826\n",
      "Trained batch 1258 batch loss 3.14383602 epoch total loss 2.70587683\n",
      "Trained batch 1259 batch loss 2.67923975 epoch total loss 2.70585561\n",
      "Trained batch 1260 batch loss 2.59257603 epoch total loss 2.70576549\n",
      "Trained batch 1261 batch loss 2.56034565 epoch total loss 2.70565033\n",
      "Trained batch 1262 batch loss 2.78947735 epoch total loss 2.70571661\n",
      "Trained batch 1263 batch loss 2.5062747 epoch total loss 2.70555878\n",
      "Trained batch 1264 batch loss 2.63648033 epoch total loss 2.70550418\n",
      "Trained batch 1265 batch loss 2.856318 epoch total loss 2.70562339\n",
      "Trained batch 1266 batch loss 2.67180777 epoch total loss 2.70559669\n",
      "Trained batch 1267 batch loss 2.87641382 epoch total loss 2.70573163\n",
      "Trained batch 1268 batch loss 2.71678543 epoch total loss 2.70574021\n",
      "Trained batch 1269 batch loss 2.72892809 epoch total loss 2.70575857\n",
      "Trained batch 1270 batch loss 2.88385057 epoch total loss 2.70589876\n",
      "Trained batch 1271 batch loss 2.62392497 epoch total loss 2.70583439\n",
      "Trained batch 1272 batch loss 2.48016882 epoch total loss 2.70565701\n",
      "Trained batch 1273 batch loss 2.37864232 epoch total loss 2.70540023\n",
      "Trained batch 1274 batch loss 2.36536932 epoch total loss 2.7051332\n",
      "Trained batch 1275 batch loss 2.35803223 epoch total loss 2.70486093\n",
      "Trained batch 1276 batch loss 2.36151958 epoch total loss 2.70459199\n",
      "Trained batch 1277 batch loss 2.46603346 epoch total loss 2.70440507\n",
      "Trained batch 1278 batch loss 2.96131372 epoch total loss 2.70460629\n",
      "Trained batch 1279 batch loss 2.90629697 epoch total loss 2.70476389\n",
      "Trained batch 1280 batch loss 2.82517719 epoch total loss 2.70485806\n",
      "Trained batch 1281 batch loss 2.7060802 epoch total loss 2.70485902\n",
      "Trained batch 1282 batch loss 2.9697 epoch total loss 2.70506549\n",
      "Trained batch 1283 batch loss 2.79349494 epoch total loss 2.70513439\n",
      "Trained batch 1284 batch loss 2.76595879 epoch total loss 2.70518184\n",
      "Trained batch 1285 batch loss 2.98486042 epoch total loss 2.70539951\n",
      "Trained batch 1286 batch loss 3.2132411 epoch total loss 2.70579433\n",
      "Trained batch 1287 batch loss 2.8317759 epoch total loss 2.70589209\n",
      "Trained batch 1288 batch loss 2.70504332 epoch total loss 2.70589137\n",
      "Trained batch 1289 batch loss 2.78784728 epoch total loss 2.70595503\n",
      "Trained batch 1290 batch loss 2.58228731 epoch total loss 2.70585918\n",
      "Trained batch 1291 batch loss 2.86390066 epoch total loss 2.70598173\n",
      "Trained batch 1292 batch loss 2.84937572 epoch total loss 2.7060926\n",
      "Trained batch 1293 batch loss 2.62990189 epoch total loss 2.70603371\n",
      "Trained batch 1294 batch loss 2.74493384 epoch total loss 2.70606375\n",
      "Trained batch 1295 batch loss 2.91703105 epoch total loss 2.70622659\n",
      "Trained batch 1296 batch loss 2.8217597 epoch total loss 2.70631576\n",
      "Trained batch 1297 batch loss 2.73586512 epoch total loss 2.70633864\n",
      "Trained batch 1298 batch loss 2.45079899 epoch total loss 2.70614147\n",
      "Trained batch 1299 batch loss 3.08915353 epoch total loss 2.7064364\n",
      "Trained batch 1300 batch loss 2.91978121 epoch total loss 2.70660043\n",
      "Trained batch 1301 batch loss 2.9723258 epoch total loss 2.70680475\n",
      "Trained batch 1302 batch loss 2.7292223 epoch total loss 2.70682192\n",
      "Trained batch 1303 batch loss 2.81111 epoch total loss 2.70690203\n",
      "Trained batch 1304 batch loss 2.56763697 epoch total loss 2.70679522\n",
      "Trained batch 1305 batch loss 2.78930593 epoch total loss 2.7068584\n",
      "Trained batch 1306 batch loss 2.97966409 epoch total loss 2.70706725\n",
      "Trained batch 1307 batch loss 3.09553981 epoch total loss 2.70736456\n",
      "Trained batch 1308 batch loss 3.21162152 epoch total loss 2.70775\n",
      "Trained batch 1309 batch loss 3.23272157 epoch total loss 2.7081511\n",
      "Trained batch 1310 batch loss 3.14108396 epoch total loss 2.70848155\n",
      "Trained batch 1311 batch loss 2.88890743 epoch total loss 2.70861912\n",
      "Trained batch 1312 batch loss 2.4960618 epoch total loss 2.70845723\n",
      "Trained batch 1313 batch loss 2.25540638 epoch total loss 2.70811224\n",
      "Trained batch 1314 batch loss 2.54407263 epoch total loss 2.70798731\n",
      "Trained batch 1315 batch loss 2.57472062 epoch total loss 2.70788598\n",
      "Trained batch 1316 batch loss 2.35564876 epoch total loss 2.70761847\n",
      "Trained batch 1317 batch loss 2.47534919 epoch total loss 2.70744205\n",
      "Trained batch 1318 batch loss 2.63862014 epoch total loss 2.70738983\n",
      "Trained batch 1319 batch loss 2.85477734 epoch total loss 2.70750165\n",
      "Trained batch 1320 batch loss 2.68440676 epoch total loss 2.70748401\n",
      "Trained batch 1321 batch loss 2.60137296 epoch total loss 2.70740366\n",
      "Trained batch 1322 batch loss 2.61474133 epoch total loss 2.70733356\n",
      "Trained batch 1323 batch loss 2.67216158 epoch total loss 2.70730686\n",
      "Trained batch 1324 batch loss 2.68234062 epoch total loss 2.70728803\n",
      "Trained batch 1325 batch loss 2.7326808 epoch total loss 2.70730734\n",
      "Trained batch 1326 batch loss 3.03302956 epoch total loss 2.70755291\n",
      "Trained batch 1327 batch loss 2.8874011 epoch total loss 2.70768833\n",
      "Trained batch 1328 batch loss 2.97752237 epoch total loss 2.7078917\n",
      "Trained batch 1329 batch loss 2.8911674 epoch total loss 2.70802951\n",
      "Trained batch 1330 batch loss 2.91504288 epoch total loss 2.7081852\n",
      "Trained batch 1331 batch loss 2.81089878 epoch total loss 2.70826221\n",
      "Trained batch 1332 batch loss 2.51283121 epoch total loss 2.70811558\n",
      "Trained batch 1333 batch loss 2.54136705 epoch total loss 2.70799041\n",
      "Trained batch 1334 batch loss 2.69912148 epoch total loss 2.70798373\n",
      "Trained batch 1335 batch loss 2.37622094 epoch total loss 2.7077353\n",
      "Trained batch 1336 batch loss 2.4439013 epoch total loss 2.70753789\n",
      "Trained batch 1337 batch loss 2.68012476 epoch total loss 2.70751739\n",
      "Trained batch 1338 batch loss 2.82422805 epoch total loss 2.70760465\n",
      "Trained batch 1339 batch loss 2.7575531 epoch total loss 2.70764184\n",
      "Trained batch 1340 batch loss 2.55492067 epoch total loss 2.70752788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1341 batch loss 2.80287576 epoch total loss 2.70759916\n",
      "Trained batch 1342 batch loss 2.44657063 epoch total loss 2.70740461\n",
      "Trained batch 1343 batch loss 2.52196169 epoch total loss 2.70726657\n",
      "Trained batch 1344 batch loss 2.56751347 epoch total loss 2.70716262\n",
      "Trained batch 1345 batch loss 2.44739699 epoch total loss 2.7069695\n",
      "Trained batch 1346 batch loss 2.47676539 epoch total loss 2.70679855\n",
      "Trained batch 1347 batch loss 2.4140451 epoch total loss 2.70658112\n",
      "Trained batch 1348 batch loss 2.64579439 epoch total loss 2.70653605\n",
      "Trained batch 1349 batch loss 2.52183771 epoch total loss 2.7063992\n",
      "Trained batch 1350 batch loss 2.55343485 epoch total loss 2.70628572\n",
      "Trained batch 1351 batch loss 2.58222413 epoch total loss 2.70619392\n",
      "Trained batch 1352 batch loss 2.5957756 epoch total loss 2.70611238\n",
      "Trained batch 1353 batch loss 2.63401699 epoch total loss 2.70605898\n",
      "Trained batch 1354 batch loss 2.56523418 epoch total loss 2.70595503\n",
      "Trained batch 1355 batch loss 2.6105423 epoch total loss 2.7058847\n",
      "Trained batch 1356 batch loss 2.36521626 epoch total loss 2.7056334\n",
      "Trained batch 1357 batch loss 2.42946959 epoch total loss 2.70542979\n",
      "Trained batch 1358 batch loss 2.28663969 epoch total loss 2.70512152\n",
      "Trained batch 1359 batch loss 2.38128233 epoch total loss 2.7048831\n",
      "Trained batch 1360 batch loss 2.46051717 epoch total loss 2.70470357\n",
      "Trained batch 1361 batch loss 2.61261106 epoch total loss 2.70463586\n",
      "Trained batch 1362 batch loss 2.63076329 epoch total loss 2.7045815\n",
      "Trained batch 1363 batch loss 2.71993041 epoch total loss 2.70459294\n",
      "Trained batch 1364 batch loss 2.58522844 epoch total loss 2.70450544\n",
      "Trained batch 1365 batch loss 2.86703634 epoch total loss 2.70462441\n",
      "Trained batch 1366 batch loss 2.72648048 epoch total loss 2.70464039\n",
      "Trained batch 1367 batch loss 2.490978 epoch total loss 2.70448422\n",
      "Trained batch 1368 batch loss 2.62049222 epoch total loss 2.70442271\n",
      "Trained batch 1369 batch loss 2.57001972 epoch total loss 2.70432472\n",
      "Trained batch 1370 batch loss 2.41885042 epoch total loss 2.70411634\n",
      "Trained batch 1371 batch loss 2.48025131 epoch total loss 2.70395303\n",
      "Trained batch 1372 batch loss 2.84199286 epoch total loss 2.70405364\n",
      "Trained batch 1373 batch loss 2.95733213 epoch total loss 2.70423818\n",
      "Trained batch 1374 batch loss 2.77071381 epoch total loss 2.70428658\n",
      "Trained batch 1375 batch loss 2.6656971 epoch total loss 2.70425844\n",
      "Trained batch 1376 batch loss 2.59208727 epoch total loss 2.7041769\n",
      "Trained batch 1377 batch loss 2.72896266 epoch total loss 2.70419502\n",
      "Trained batch 1378 batch loss 2.60204458 epoch total loss 2.70412087\n",
      "Trained batch 1379 batch loss 2.56875372 epoch total loss 2.70402288\n",
      "Trained batch 1380 batch loss 2.61600804 epoch total loss 2.70395899\n",
      "Trained batch 1381 batch loss 2.71734262 epoch total loss 2.70396852\n",
      "Trained batch 1382 batch loss 2.67571378 epoch total loss 2.70394826\n",
      "Trained batch 1383 batch loss 2.50244451 epoch total loss 2.70380259\n",
      "Trained batch 1384 batch loss 2.20449686 epoch total loss 2.70344186\n",
      "Trained batch 1385 batch loss 2.31489611 epoch total loss 2.70316124\n",
      "Trained batch 1386 batch loss 2.5718689 epoch total loss 2.70306659\n",
      "Trained batch 1387 batch loss 2.50290203 epoch total loss 2.70292211\n",
      "Trained batch 1388 batch loss 2.84670496 epoch total loss 2.70302582\n",
      "Trained batch 1389 batch loss 2.63223362 epoch total loss 2.7029748\n",
      "Trained batch 1390 batch loss 2.61624289 epoch total loss 2.70291257\n",
      "Trained batch 1391 batch loss 2.3792243 epoch total loss 2.70267963\n",
      "Trained batch 1392 batch loss 2.80386758 epoch total loss 2.70275259\n",
      "Trained batch 1393 batch loss 2.43068361 epoch total loss 2.70255709\n",
      "Trained batch 1394 batch loss 2.65882254 epoch total loss 2.70252585\n",
      "Trained batch 1395 batch loss 2.84566069 epoch total loss 2.70262837\n",
      "Trained batch 1396 batch loss 2.47864 epoch total loss 2.70246816\n",
      "Trained batch 1397 batch loss 2.61851645 epoch total loss 2.70240808\n",
      "Trained batch 1398 batch loss 2.91525865 epoch total loss 2.70256019\n",
      "Trained batch 1399 batch loss 2.460289 epoch total loss 2.70238709\n",
      "Trained batch 1400 batch loss 2.50724411 epoch total loss 2.70224762\n",
      "Trained batch 1401 batch loss 2.36568785 epoch total loss 2.70200753\n",
      "Trained batch 1402 batch loss 2.6003623 epoch total loss 2.70193505\n",
      "Trained batch 1403 batch loss 2.66617775 epoch total loss 2.70190954\n",
      "Trained batch 1404 batch loss 2.46112394 epoch total loss 2.70173812\n",
      "Trained batch 1405 batch loss 2.63104367 epoch total loss 2.70168781\n",
      "Trained batch 1406 batch loss 2.56661654 epoch total loss 2.70159173\n",
      "Trained batch 1407 batch loss 2.26777744 epoch total loss 2.70128345\n",
      "Trained batch 1408 batch loss 2.46382499 epoch total loss 2.70111489\n",
      "Trained batch 1409 batch loss 2.56426573 epoch total loss 2.70101762\n",
      "Trained batch 1410 batch loss 2.50700521 epoch total loss 2.70088\n",
      "Trained batch 1411 batch loss 3.07298088 epoch total loss 2.70114374\n",
      "Trained batch 1412 batch loss 2.8014586 epoch total loss 2.70121503\n",
      "Trained batch 1413 batch loss 2.59228468 epoch total loss 2.70113778\n",
      "Trained batch 1414 batch loss 2.96320462 epoch total loss 2.70132303\n",
      "Trained batch 1415 batch loss 3.03322649 epoch total loss 2.70155764\n",
      "Trained batch 1416 batch loss 2.59999609 epoch total loss 2.70148611\n",
      "Trained batch 1417 batch loss 3.10487962 epoch total loss 2.70177078\n",
      "Trained batch 1418 batch loss 2.84819 epoch total loss 2.70187402\n",
      "Trained batch 1419 batch loss 2.7441628 epoch total loss 2.70190382\n",
      "Trained batch 1420 batch loss 2.71333027 epoch total loss 2.70191193\n",
      "Trained batch 1421 batch loss 2.63754463 epoch total loss 2.70186639\n",
      "Trained batch 1422 batch loss 2.85129809 epoch total loss 2.70197153\n",
      "Trained batch 1423 batch loss 2.67943788 epoch total loss 2.7019558\n",
      "Trained batch 1424 batch loss 2.82856369 epoch total loss 2.70204473\n",
      "Trained batch 1425 batch loss 2.64186621 epoch total loss 2.70200253\n",
      "Trained batch 1426 batch loss 2.71356344 epoch total loss 2.70201063\n",
      "Trained batch 1427 batch loss 2.75264812 epoch total loss 2.70204616\n",
      "Trained batch 1428 batch loss 2.76730585 epoch total loss 2.70209193\n",
      "Trained batch 1429 batch loss 2.48395 epoch total loss 2.70193911\n",
      "Trained batch 1430 batch loss 2.63723373 epoch total loss 2.70189381\n",
      "Trained batch 1431 batch loss 2.50278068 epoch total loss 2.70175457\n",
      "Trained batch 1432 batch loss 2.43805575 epoch total loss 2.70157051\n",
      "Trained batch 1433 batch loss 2.49410295 epoch total loss 2.70142579\n",
      "Trained batch 1434 batch loss 2.4109714 epoch total loss 2.70122313\n",
      "Trained batch 1435 batch loss 2.39361644 epoch total loss 2.7010088\n",
      "Trained batch 1436 batch loss 2.52668428 epoch total loss 2.7008872\n",
      "Trained batch 1437 batch loss 2.69756722 epoch total loss 2.70088482\n",
      "Trained batch 1438 batch loss 2.58458424 epoch total loss 2.700804\n",
      "Trained batch 1439 batch loss 2.57153201 epoch total loss 2.70071411\n",
      "Trained batch 1440 batch loss 2.4844892 epoch total loss 2.70056391\n",
      "Trained batch 1441 batch loss 2.68632054 epoch total loss 2.70055389\n",
      "Trained batch 1442 batch loss 2.51734138 epoch total loss 2.70042682\n",
      "Trained batch 1443 batch loss 2.66166615 epoch total loss 2.7004\n",
      "Trained batch 1444 batch loss 2.57779646 epoch total loss 2.70031524\n",
      "Trained batch 1445 batch loss 2.47944164 epoch total loss 2.70016241\n",
      "Trained batch 1446 batch loss 2.29986238 epoch total loss 2.69988561\n",
      "Trained batch 1447 batch loss 2.56379223 epoch total loss 2.69979143\n",
      "Trained batch 1448 batch loss 2.42335 epoch total loss 2.69960046\n",
      "Trained batch 1449 batch loss 2.79379654 epoch total loss 2.69966531\n",
      "Trained batch 1450 batch loss 2.55427027 epoch total loss 2.69956517\n",
      "Trained batch 1451 batch loss 2.81179428 epoch total loss 2.69964242\n",
      "Trained batch 1452 batch loss 2.83886313 epoch total loss 2.69973826\n",
      "Trained batch 1453 batch loss 2.57029772 epoch total loss 2.69964933\n",
      "Trained batch 1454 batch loss 2.49357772 epoch total loss 2.69950747\n",
      "Trained batch 1455 batch loss 2.79153252 epoch total loss 2.69957089\n",
      "Trained batch 1456 batch loss 2.71142673 epoch total loss 2.699579\n",
      "Trained batch 1457 batch loss 2.82353878 epoch total loss 2.69966388\n",
      "Trained batch 1458 batch loss 2.76288795 epoch total loss 2.69970727\n",
      "Trained batch 1459 batch loss 3.03325272 epoch total loss 2.69993591\n",
      "Trained batch 1460 batch loss 2.23240328 epoch total loss 2.69961572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1461 batch loss 2.76275182 epoch total loss 2.69965887\n",
      "Trained batch 1462 batch loss 2.88453937 epoch total loss 2.69978523\n",
      "Trained batch 1463 batch loss 2.77582932 epoch total loss 2.69983745\n",
      "Trained batch 1464 batch loss 2.65672922 epoch total loss 2.69980788\n",
      "Trained batch 1465 batch loss 2.54249072 epoch total loss 2.69970059\n",
      "Trained batch 1466 batch loss 2.66201544 epoch total loss 2.69967484\n",
      "Trained batch 1467 batch loss 2.89915442 epoch total loss 2.69981098\n",
      "Trained batch 1468 batch loss 2.68195772 epoch total loss 2.69979858\n",
      "Trained batch 1469 batch loss 2.9266634 epoch total loss 2.69995308\n",
      "Trained batch 1470 batch loss 3.09119582 epoch total loss 2.70021939\n",
      "Trained batch 1471 batch loss 2.59937906 epoch total loss 2.70015073\n",
      "Trained batch 1472 batch loss 2.67306614 epoch total loss 2.70013237\n",
      "Trained batch 1473 batch loss 2.65726066 epoch total loss 2.70010328\n",
      "Trained batch 1474 batch loss 2.67563629 epoch total loss 2.70008659\n",
      "Trained batch 1475 batch loss 2.42680836 epoch total loss 2.69990134\n",
      "Trained batch 1476 batch loss 2.56969786 epoch total loss 2.69981313\n",
      "Trained batch 1477 batch loss 2.57345057 epoch total loss 2.69972754\n",
      "Trained batch 1478 batch loss 2.71221733 epoch total loss 2.69973588\n",
      "Trained batch 1479 batch loss 2.88911 epoch total loss 2.69986391\n",
      "Trained batch 1480 batch loss 2.87321854 epoch total loss 2.69998121\n",
      "Trained batch 1481 batch loss 2.71852756 epoch total loss 2.69999361\n",
      "Trained batch 1482 batch loss 2.90247655 epoch total loss 2.70013046\n",
      "Trained batch 1483 batch loss 2.374367 epoch total loss 2.69991064\n",
      "Trained batch 1484 batch loss 2.73138642 epoch total loss 2.69993186\n",
      "Trained batch 1485 batch loss 2.40261745 epoch total loss 2.69973159\n",
      "Trained batch 1486 batch loss 2.61877251 epoch total loss 2.69967723\n",
      "Trained batch 1487 batch loss 2.99614429 epoch total loss 2.69987655\n",
      "Trained batch 1488 batch loss 2.44566631 epoch total loss 2.6997056\n",
      "Trained batch 1489 batch loss 2.590734 epoch total loss 2.69963241\n",
      "Trained batch 1490 batch loss 2.35332274 epoch total loss 2.6994\n",
      "Trained batch 1491 batch loss 2.48810816 epoch total loss 2.69925833\n",
      "Trained batch 1492 batch loss 2.52082038 epoch total loss 2.69913864\n",
      "Trained batch 1493 batch loss 2.46033716 epoch total loss 2.69897866\n",
      "Trained batch 1494 batch loss 2.7490921 epoch total loss 2.69901228\n",
      "Trained batch 1495 batch loss 2.67087889 epoch total loss 2.69899344\n",
      "Trained batch 1496 batch loss 2.57963467 epoch total loss 2.69891357\n",
      "Trained batch 1497 batch loss 2.76844954 epoch total loss 2.69896\n",
      "Trained batch 1498 batch loss 2.69028234 epoch total loss 2.69895434\n",
      "Trained batch 1499 batch loss 2.49850106 epoch total loss 2.69882059\n",
      "Trained batch 1500 batch loss 2.32537055 epoch total loss 2.69857168\n",
      "Trained batch 1501 batch loss 2.6750648 epoch total loss 2.69855595\n",
      "Trained batch 1502 batch loss 2.69007659 epoch total loss 2.69855046\n",
      "Trained batch 1503 batch loss 2.51523137 epoch total loss 2.69842839\n",
      "Trained batch 1504 batch loss 2.76755381 epoch total loss 2.69847441\n",
      "Trained batch 1505 batch loss 2.54858255 epoch total loss 2.69837475\n",
      "Trained batch 1506 batch loss 2.4226408 epoch total loss 2.69819164\n",
      "Trained batch 1507 batch loss 2.65999579 epoch total loss 2.69816613\n",
      "Trained batch 1508 batch loss 2.51936102 epoch total loss 2.69804764\n",
      "Trained batch 1509 batch loss 2.75683546 epoch total loss 2.6980865\n",
      "Trained batch 1510 batch loss 2.89052 epoch total loss 2.69821405\n",
      "Trained batch 1511 batch loss 2.46902776 epoch total loss 2.69806242\n",
      "Trained batch 1512 batch loss 2.48431 epoch total loss 2.69792104\n",
      "Trained batch 1513 batch loss 2.85802 epoch total loss 2.69802666\n",
      "Trained batch 1514 batch loss 2.56931472 epoch total loss 2.69794178\n",
      "Trained batch 1515 batch loss 2.68718171 epoch total loss 2.69793463\n",
      "Trained batch 1516 batch loss 2.78503895 epoch total loss 2.69799232\n",
      "Trained batch 1517 batch loss 2.50951767 epoch total loss 2.69786811\n",
      "Trained batch 1518 batch loss 2.45302057 epoch total loss 2.6977067\n",
      "Trained batch 1519 batch loss 2.16079354 epoch total loss 2.69735336\n",
      "Trained batch 1520 batch loss 2.41062164 epoch total loss 2.69716477\n",
      "Trained batch 1521 batch loss 2.794415 epoch total loss 2.69722867\n",
      "Trained batch 1522 batch loss 3.11274886 epoch total loss 2.69750166\n",
      "Trained batch 1523 batch loss 2.90946364 epoch total loss 2.69764113\n",
      "Trained batch 1524 batch loss 3.07862234 epoch total loss 2.697891\n",
      "Trained batch 1525 batch loss 2.84861135 epoch total loss 2.69799\n",
      "Trained batch 1526 batch loss 2.88414621 epoch total loss 2.69811201\n",
      "Trained batch 1527 batch loss 2.47240424 epoch total loss 2.69796395\n",
      "Trained batch 1528 batch loss 2.59834504 epoch total loss 2.69789863\n",
      "Trained batch 1529 batch loss 2.19869804 epoch total loss 2.69757223\n",
      "Trained batch 1530 batch loss 2.38953686 epoch total loss 2.69737101\n",
      "Trained batch 1531 batch loss 2.0436523 epoch total loss 2.69694376\n",
      "Trained batch 1532 batch loss 2.56711292 epoch total loss 2.69685888\n",
      "Trained batch 1533 batch loss 2.89562726 epoch total loss 2.69698858\n",
      "Trained batch 1534 batch loss 2.94036984 epoch total loss 2.69714713\n",
      "Trained batch 1535 batch loss 2.87786674 epoch total loss 2.69726491\n",
      "Trained batch 1536 batch loss 3.01848793 epoch total loss 2.69747424\n",
      "Trained batch 1537 batch loss 2.82312131 epoch total loss 2.69755602\n",
      "Trained batch 1538 batch loss 2.89182639 epoch total loss 2.69768214\n",
      "Trained batch 1539 batch loss 2.60636377 epoch total loss 2.69762278\n",
      "Trained batch 1540 batch loss 2.60775137 epoch total loss 2.6975646\n",
      "Trained batch 1541 batch loss 2.54906487 epoch total loss 2.69746804\n",
      "Trained batch 1542 batch loss 2.77615166 epoch total loss 2.6975193\n",
      "Trained batch 1543 batch loss 2.86220551 epoch total loss 2.69762611\n",
      "Trained batch 1544 batch loss 2.73931384 epoch total loss 2.69765306\n",
      "Trained batch 1545 batch loss 2.67198706 epoch total loss 2.69763637\n",
      "Trained batch 1546 batch loss 2.70662475 epoch total loss 2.69764209\n",
      "Trained batch 1547 batch loss 2.79348421 epoch total loss 2.69770408\n",
      "Trained batch 1548 batch loss 2.5028336 epoch total loss 2.69757819\n",
      "Trained batch 1549 batch loss 2.52140141 epoch total loss 2.69746447\n",
      "Trained batch 1550 batch loss 2.47763467 epoch total loss 2.69732261\n",
      "Trained batch 1551 batch loss 2.70708513 epoch total loss 2.69732881\n",
      "Trained batch 1552 batch loss 2.53760839 epoch total loss 2.69722605\n",
      "Trained batch 1553 batch loss 2.54167533 epoch total loss 2.69712567\n",
      "Trained batch 1554 batch loss 2.63648605 epoch total loss 2.69708681\n",
      "Trained batch 1555 batch loss 2.69114757 epoch total loss 2.69708276\n",
      "Trained batch 1556 batch loss 2.50661 epoch total loss 2.69696069\n",
      "Trained batch 1557 batch loss 2.68044949 epoch total loss 2.6969502\n",
      "Trained batch 1558 batch loss 2.50232363 epoch total loss 2.69682527\n",
      "Trained batch 1559 batch loss 2.71172333 epoch total loss 2.69683504\n",
      "Trained batch 1560 batch loss 2.58933663 epoch total loss 2.69676614\n",
      "Trained batch 1561 batch loss 2.7626586 epoch total loss 2.69680834\n",
      "Trained batch 1562 batch loss 2.73260045 epoch total loss 2.69683123\n",
      "Trained batch 1563 batch loss 2.61839581 epoch total loss 2.69678068\n",
      "Trained batch 1564 batch loss 2.69128418 epoch total loss 2.69677734\n",
      "Trained batch 1565 batch loss 2.62322307 epoch total loss 2.69673014\n",
      "Trained batch 1566 batch loss 2.65205479 epoch total loss 2.69670153\n",
      "Trained batch 1567 batch loss 2.4863925 epoch total loss 2.6965673\n",
      "Trained batch 1568 batch loss 2.47681522 epoch total loss 2.69642735\n",
      "Trained batch 1569 batch loss 2.44938803 epoch total loss 2.69626975\n",
      "Trained batch 1570 batch loss 2.34875393 epoch total loss 2.69604826\n",
      "Trained batch 1571 batch loss 2.44125557 epoch total loss 2.69588614\n",
      "Trained batch 1572 batch loss 2.701092 epoch total loss 2.69588971\n",
      "Trained batch 1573 batch loss 2.53592801 epoch total loss 2.69578815\n",
      "Trained batch 1574 batch loss 2.78188491 epoch total loss 2.69584274\n",
      "Trained batch 1575 batch loss 2.60639644 epoch total loss 2.695786\n",
      "Trained batch 1576 batch loss 2.61881042 epoch total loss 2.69573689\n",
      "Trained batch 1577 batch loss 2.71312952 epoch total loss 2.69574785\n",
      "Trained batch 1578 batch loss 2.87035513 epoch total loss 2.69585824\n",
      "Trained batch 1579 batch loss 2.76396275 epoch total loss 2.69590163\n",
      "Trained batch 1580 batch loss 2.83180714 epoch total loss 2.6959877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1581 batch loss 2.78697515 epoch total loss 2.6960454\n",
      "Trained batch 1582 batch loss 2.61689663 epoch total loss 2.69599533\n",
      "Trained batch 1583 batch loss 2.88922358 epoch total loss 2.69611716\n",
      "Trained batch 1584 batch loss 2.87932944 epoch total loss 2.69623303\n",
      "Trained batch 1585 batch loss 2.84684443 epoch total loss 2.69632792\n",
      "Trained batch 1586 batch loss 3.00294447 epoch total loss 2.69652128\n",
      "Trained batch 1587 batch loss 2.76469827 epoch total loss 2.6965642\n",
      "Trained batch 1588 batch loss 2.67199969 epoch total loss 2.69654846\n",
      "Trained batch 1589 batch loss 2.5644381 epoch total loss 2.69646549\n",
      "Trained batch 1590 batch loss 2.38702559 epoch total loss 2.69627094\n",
      "Trained batch 1591 batch loss 2.49039817 epoch total loss 2.69614148\n",
      "Trained batch 1592 batch loss 2.75393438 epoch total loss 2.69617772\n",
      "Trained batch 1593 batch loss 2.87362432 epoch total loss 2.69628906\n",
      "Trained batch 1594 batch loss 2.80690098 epoch total loss 2.69635868\n",
      "Trained batch 1595 batch loss 2.75092101 epoch total loss 2.69639277\n",
      "Trained batch 1596 batch loss 2.58190966 epoch total loss 2.69632125\n",
      "Trained batch 1597 batch loss 2.50129986 epoch total loss 2.69619918\n",
      "Trained batch 1598 batch loss 2.35246968 epoch total loss 2.69598413\n",
      "Trained batch 1599 batch loss 2.61913729 epoch total loss 2.69593596\n",
      "Trained batch 1600 batch loss 2.34942865 epoch total loss 2.69571972\n",
      "Trained batch 1601 batch loss 2.37460327 epoch total loss 2.69551897\n",
      "Trained batch 1602 batch loss 2.58069301 epoch total loss 2.69544721\n",
      "Trained batch 1603 batch loss 2.43808198 epoch total loss 2.69528651\n",
      "Trained batch 1604 batch loss 2.38931608 epoch total loss 2.69509578\n",
      "Trained batch 1605 batch loss 2.42454505 epoch total loss 2.69492698\n",
      "Trained batch 1606 batch loss 2.66130877 epoch total loss 2.694906\n",
      "Trained batch 1607 batch loss 2.48549962 epoch total loss 2.69477558\n",
      "Trained batch 1608 batch loss 2.50482225 epoch total loss 2.69465756\n",
      "Trained batch 1609 batch loss 2.53632736 epoch total loss 2.6945591\n",
      "Trained batch 1610 batch loss 2.79791236 epoch total loss 2.69462323\n",
      "Trained batch 1611 batch loss 2.65632772 epoch total loss 2.69459939\n",
      "Trained batch 1612 batch loss 2.557688 epoch total loss 2.69451427\n",
      "Trained batch 1613 batch loss 2.43214417 epoch total loss 2.69435167\n",
      "Trained batch 1614 batch loss 2.34647703 epoch total loss 2.69413638\n",
      "Trained batch 1615 batch loss 2.62232113 epoch total loss 2.69409204\n",
      "Trained batch 1616 batch loss 2.3357873 epoch total loss 2.69387031\n",
      "Trained batch 1617 batch loss 2.66304183 epoch total loss 2.69385123\n",
      "Trained batch 1618 batch loss 2.46474314 epoch total loss 2.69370985\n",
      "Trained batch 1619 batch loss 2.51666546 epoch total loss 2.69360042\n",
      "Trained batch 1620 batch loss 2.36535 epoch total loss 2.69339776\n",
      "Trained batch 1621 batch loss 2.84886646 epoch total loss 2.69349337\n",
      "Trained batch 1622 batch loss 2.72528863 epoch total loss 2.69351292\n",
      "Trained batch 1623 batch loss 2.64538717 epoch total loss 2.69348335\n",
      "Trained batch 1624 batch loss 2.4956181 epoch total loss 2.69336152\n",
      "Trained batch 1625 batch loss 2.33373642 epoch total loss 2.69314\n",
      "Trained batch 1626 batch loss 2.46233606 epoch total loss 2.69299817\n",
      "Trained batch 1627 batch loss 2.36672139 epoch total loss 2.69279766\n",
      "Trained batch 1628 batch loss 2.06106782 epoch total loss 2.69240952\n",
      "Trained batch 1629 batch loss 1.98253381 epoch total loss 2.69197369\n",
      "Trained batch 1630 batch loss 1.91059101 epoch total loss 2.69149423\n",
      "Trained batch 1631 batch loss 2.29517984 epoch total loss 2.69125152\n",
      "Trained batch 1632 batch loss 2.69870067 epoch total loss 2.69125605\n",
      "Trained batch 1633 batch loss 2.91780424 epoch total loss 2.69139481\n",
      "Trained batch 1634 batch loss 2.89533687 epoch total loss 2.69151974\n",
      "Trained batch 1635 batch loss 2.71293449 epoch total loss 2.69153285\n",
      "Trained batch 1636 batch loss 2.83387136 epoch total loss 2.69161987\n",
      "Trained batch 1637 batch loss 2.67097592 epoch total loss 2.69160724\n",
      "Trained batch 1638 batch loss 2.55643964 epoch total loss 2.69152498\n",
      "Trained batch 1639 batch loss 2.79542685 epoch total loss 2.69158816\n",
      "Trained batch 1640 batch loss 2.69353724 epoch total loss 2.69158936\n",
      "Trained batch 1641 batch loss 2.74268436 epoch total loss 2.69162059\n",
      "Trained batch 1642 batch loss 2.56538081 epoch total loss 2.69154358\n",
      "Trained batch 1643 batch loss 2.16175151 epoch total loss 2.691221\n",
      "Trained batch 1644 batch loss 2.43280482 epoch total loss 2.69106388\n",
      "Trained batch 1645 batch loss 2.34104824 epoch total loss 2.69085097\n",
      "Trained batch 1646 batch loss 2.29449511 epoch total loss 2.69061\n",
      "Trained batch 1647 batch loss 2.694278 epoch total loss 2.69061232\n",
      "Trained batch 1648 batch loss 2.67417049 epoch total loss 2.69060254\n",
      "Trained batch 1649 batch loss 2.38534546 epoch total loss 2.69041729\n",
      "Trained batch 1650 batch loss 2.84991097 epoch total loss 2.69051409\n",
      "Trained batch 1651 batch loss 2.43781614 epoch total loss 2.69036102\n",
      "Trained batch 1652 batch loss 2.73812771 epoch total loss 2.69039\n",
      "Trained batch 1653 batch loss 2.80931306 epoch total loss 2.69046187\n",
      "Trained batch 1654 batch loss 2.69265604 epoch total loss 2.6904633\n",
      "Trained batch 1655 batch loss 2.5907073 epoch total loss 2.69040322\n",
      "Trained batch 1656 batch loss 2.83261871 epoch total loss 2.69048905\n",
      "Trained batch 1657 batch loss 2.86962509 epoch total loss 2.69059706\n",
      "Trained batch 1658 batch loss 2.48745728 epoch total loss 2.69047451\n",
      "Trained batch 1659 batch loss 2.63415408 epoch total loss 2.69044065\n",
      "Trained batch 1660 batch loss 2.55875897 epoch total loss 2.69036126\n",
      "Trained batch 1661 batch loss 2.67897463 epoch total loss 2.69035435\n",
      "Trained batch 1662 batch loss 2.46242595 epoch total loss 2.69021726\n",
      "Trained batch 1663 batch loss 2.3168807 epoch total loss 2.6899929\n",
      "Trained batch 1664 batch loss 2.20636439 epoch total loss 2.68970227\n",
      "Trained batch 1665 batch loss 2.37517929 epoch total loss 2.68951321\n",
      "Trained batch 1666 batch loss 2.34996128 epoch total loss 2.6893096\n",
      "Trained batch 1667 batch loss 2.38443613 epoch total loss 2.68912649\n",
      "Trained batch 1668 batch loss 2.44898748 epoch total loss 2.68898273\n",
      "Trained batch 1669 batch loss 2.41228962 epoch total loss 2.68881679\n",
      "Trained batch 1670 batch loss 2.34386659 epoch total loss 2.68861\n",
      "Trained batch 1671 batch loss 2.58731747 epoch total loss 2.68854952\n",
      "Trained batch 1672 batch loss 2.47199035 epoch total loss 2.6884203\n",
      "Trained batch 1673 batch loss 2.19563341 epoch total loss 2.68812585\n",
      "Trained batch 1674 batch loss 2.39702511 epoch total loss 2.6879518\n",
      "Trained batch 1675 batch loss 2.51746297 epoch total loss 2.68785\n",
      "Trained batch 1676 batch loss 2.59338284 epoch total loss 2.68779373\n",
      "Trained batch 1677 batch loss 2.08900952 epoch total loss 2.68743658\n",
      "Trained batch 1678 batch loss 2.37509537 epoch total loss 2.68725038\n",
      "Trained batch 1679 batch loss 2.51497579 epoch total loss 2.68714786\n",
      "Trained batch 1680 batch loss 2.83541298 epoch total loss 2.68723607\n",
      "Trained batch 1681 batch loss 2.99195814 epoch total loss 2.68741751\n",
      "Trained batch 1682 batch loss 3.04090285 epoch total loss 2.68762779\n",
      "Trained batch 1683 batch loss 2.51577353 epoch total loss 2.68752551\n",
      "Trained batch 1684 batch loss 2.27967215 epoch total loss 2.68728352\n",
      "Trained batch 1685 batch loss 2.68287158 epoch total loss 2.68728089\n",
      "Trained batch 1686 batch loss 2.74043822 epoch total loss 2.68731236\n",
      "Trained batch 1687 batch loss 2.54994869 epoch total loss 2.68723083\n",
      "Trained batch 1688 batch loss 2.73909 epoch total loss 2.68726158\n",
      "Trained batch 1689 batch loss 2.64319205 epoch total loss 2.68723559\n",
      "Trained batch 1690 batch loss 2.58697939 epoch total loss 2.68717623\n",
      "Trained batch 1691 batch loss 2.42820024 epoch total loss 2.68702292\n",
      "Trained batch 1692 batch loss 2.68006134 epoch total loss 2.68701887\n",
      "Trained batch 1693 batch loss 2.60805964 epoch total loss 2.68697214\n",
      "Trained batch 1694 batch loss 2.72899866 epoch total loss 2.68699694\n",
      "Trained batch 1695 batch loss 2.95586252 epoch total loss 2.68715572\n",
      "Trained batch 1696 batch loss 2.94434714 epoch total loss 2.68730736\n",
      "Trained batch 1697 batch loss 2.71214318 epoch total loss 2.6873219\n",
      "Trained batch 1698 batch loss 2.76625299 epoch total loss 2.68736839\n",
      "Trained batch 1699 batch loss 2.64497733 epoch total loss 2.68734336\n",
      "Trained batch 1700 batch loss 2.61766291 epoch total loss 2.68730235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1701 batch loss 2.61371613 epoch total loss 2.6872592\n",
      "Trained batch 1702 batch loss 2.73767829 epoch total loss 2.68728876\n",
      "Trained batch 1703 batch loss 2.68266964 epoch total loss 2.68728614\n",
      "Trained batch 1704 batch loss 2.56904554 epoch total loss 2.68721652\n",
      "Trained batch 1705 batch loss 2.91414952 epoch total loss 2.68734956\n",
      "Trained batch 1706 batch loss 2.68084455 epoch total loss 2.68734574\n",
      "Trained batch 1707 batch loss 2.68160105 epoch total loss 2.68734241\n",
      "Trained batch 1708 batch loss 2.43693638 epoch total loss 2.68719578\n",
      "Trained batch 1709 batch loss 2.75076389 epoch total loss 2.68723321\n",
      "Trained batch 1710 batch loss 2.38817906 epoch total loss 2.68705821\n",
      "Trained batch 1711 batch loss 2.37064791 epoch total loss 2.6868732\n",
      "Trained batch 1712 batch loss 2.63334942 epoch total loss 2.68684196\n",
      "Trained batch 1713 batch loss 2.49483871 epoch total loss 2.68673\n",
      "Trained batch 1714 batch loss 2.41570759 epoch total loss 2.6865716\n",
      "Trained batch 1715 batch loss 2.37694287 epoch total loss 2.68639112\n",
      "Trained batch 1716 batch loss 2.28964901 epoch total loss 2.68615985\n",
      "Trained batch 1717 batch loss 2.28758693 epoch total loss 2.68592763\n",
      "Trained batch 1718 batch loss 2.38042212 epoch total loss 2.68574977\n",
      "Trained batch 1719 batch loss 2.57531 epoch total loss 2.6856854\n",
      "Trained batch 1720 batch loss 2.43826 epoch total loss 2.68554187\n",
      "Trained batch 1721 batch loss 2.56196213 epoch total loss 2.68547\n",
      "Trained batch 1722 batch loss 2.80360293 epoch total loss 2.68553877\n",
      "Trained batch 1723 batch loss 2.47657108 epoch total loss 2.68541741\n",
      "Trained batch 1724 batch loss 2.33591509 epoch total loss 2.68521476\n",
      "Trained batch 1725 batch loss 2.15837812 epoch total loss 2.68490911\n",
      "Trained batch 1726 batch loss 2.57445359 epoch total loss 2.68484497\n",
      "Trained batch 1727 batch loss 2.95240688 epoch total loss 2.68500018\n",
      "Trained batch 1728 batch loss 2.4718852 epoch total loss 2.68487668\n",
      "Trained batch 1729 batch loss 2.56301856 epoch total loss 2.68480611\n",
      "Trained batch 1730 batch loss 2.37892532 epoch total loss 2.6846292\n",
      "Trained batch 1731 batch loss 2.40606785 epoch total loss 2.68446851\n",
      "Trained batch 1732 batch loss 2.81330085 epoch total loss 2.68454289\n",
      "Trained batch 1733 batch loss 2.69738412 epoch total loss 2.68455029\n",
      "Trained batch 1734 batch loss 2.77491474 epoch total loss 2.6846025\n",
      "Trained batch 1735 batch loss 2.69184494 epoch total loss 2.68460655\n",
      "Trained batch 1736 batch loss 2.8305316 epoch total loss 2.68469071\n",
      "Trained batch 1737 batch loss 2.69477749 epoch total loss 2.68469644\n",
      "Trained batch 1738 batch loss 2.55455923 epoch total loss 2.68462181\n",
      "Trained batch 1739 batch loss 2.61976027 epoch total loss 2.68458438\n",
      "Trained batch 1740 batch loss 2.2525897 epoch total loss 2.68433595\n",
      "Trained batch 1741 batch loss 2.14895844 epoch total loss 2.68402839\n",
      "Trained batch 1742 batch loss 2.22073603 epoch total loss 2.68376255\n",
      "Trained batch 1743 batch loss 2.08594656 epoch total loss 2.68341947\n",
      "Trained batch 1744 batch loss 2.05752468 epoch total loss 2.68306065\n",
      "Trained batch 1745 batch loss 2.39070034 epoch total loss 2.68289304\n",
      "Trained batch 1746 batch loss 2.53767848 epoch total loss 2.68280983\n",
      "Trained batch 1747 batch loss 2.67246747 epoch total loss 2.68280387\n",
      "Trained batch 1748 batch loss 3.11209846 epoch total loss 2.68304968\n",
      "Trained batch 1749 batch loss 3.13279223 epoch total loss 2.68330669\n",
      "Trained batch 1750 batch loss 3.44306231 epoch total loss 2.68374085\n",
      "Trained batch 1751 batch loss 3.54008412 epoch total loss 2.68422985\n",
      "Trained batch 1752 batch loss 3.32657981 epoch total loss 2.68459654\n",
      "Trained batch 1753 batch loss 2.40899348 epoch total loss 2.68443942\n",
      "Trained batch 1754 batch loss 2.51433825 epoch total loss 2.68434238\n",
      "Trained batch 1755 batch loss 2.15219212 epoch total loss 2.68403912\n",
      "Trained batch 1756 batch loss 2.56123018 epoch total loss 2.68396902\n",
      "Trained batch 1757 batch loss 2.68716669 epoch total loss 2.68397093\n",
      "Trained batch 1758 batch loss 2.80504656 epoch total loss 2.68403983\n",
      "Trained batch 1759 batch loss 2.77260661 epoch total loss 2.68409014\n",
      "Trained batch 1760 batch loss 2.79504585 epoch total loss 2.68415308\n",
      "Trained batch 1761 batch loss 2.8438015 epoch total loss 2.68424368\n",
      "Trained batch 1762 batch loss 2.71049809 epoch total loss 2.68425846\n",
      "Trained batch 1763 batch loss 2.71873236 epoch total loss 2.68427801\n",
      "Trained batch 1764 batch loss 2.56723833 epoch total loss 2.68421197\n",
      "Trained batch 1765 batch loss 2.50268054 epoch total loss 2.68410897\n",
      "Trained batch 1766 batch loss 2.75009441 epoch total loss 2.68414617\n",
      "Trained batch 1767 batch loss 2.9594264 epoch total loss 2.68430209\n",
      "Trained batch 1768 batch loss 2.838166 epoch total loss 2.68438911\n",
      "Trained batch 1769 batch loss 2.63416862 epoch total loss 2.68436074\n",
      "Trained batch 1770 batch loss 2.62260318 epoch total loss 2.68432593\n",
      "Trained batch 1771 batch loss 2.63641143 epoch total loss 2.68429875\n",
      "Trained batch 1772 batch loss 2.56983 epoch total loss 2.68423414\n",
      "Trained batch 1773 batch loss 2.42277956 epoch total loss 2.6840868\n",
      "Trained batch 1774 batch loss 2.59662676 epoch total loss 2.68403745\n",
      "Trained batch 1775 batch loss 2.37779069 epoch total loss 2.68386507\n",
      "Trained batch 1776 batch loss 2.58865523 epoch total loss 2.68381143\n",
      "Trained batch 1777 batch loss 2.56418467 epoch total loss 2.68374395\n",
      "Trained batch 1778 batch loss 3.04501343 epoch total loss 2.68394709\n",
      "Trained batch 1779 batch loss 2.55586 epoch total loss 2.68387508\n",
      "Trained batch 1780 batch loss 2.50626087 epoch total loss 2.68377542\n",
      "Trained batch 1781 batch loss 2.52942491 epoch total loss 2.68368864\n",
      "Trained batch 1782 batch loss 2.03303409 epoch total loss 2.68332362\n",
      "Trained batch 1783 batch loss 2.29178596 epoch total loss 2.68310404\n",
      "Trained batch 1784 batch loss 2.69293451 epoch total loss 2.68310952\n",
      "Trained batch 1785 batch loss 2.7165339 epoch total loss 2.68312812\n",
      "Trained batch 1786 batch loss 2.83828831 epoch total loss 2.68321514\n",
      "Trained batch 1787 batch loss 2.65515065 epoch total loss 2.68319941\n",
      "Trained batch 1788 batch loss 2.7190702 epoch total loss 2.68321967\n",
      "Trained batch 1789 batch loss 2.69844532 epoch total loss 2.68322802\n",
      "Trained batch 1790 batch loss 2.88348 epoch total loss 2.68333983\n",
      "Trained batch 1791 batch loss 2.76427126 epoch total loss 2.6833849\n",
      "Trained batch 1792 batch loss 2.61625051 epoch total loss 2.68334746\n",
      "Trained batch 1793 batch loss 2.57812953 epoch total loss 2.68328881\n",
      "Trained batch 1794 batch loss 2.67596364 epoch total loss 2.68328452\n",
      "Trained batch 1795 batch loss 2.62522864 epoch total loss 2.6832521\n",
      "Trained batch 1796 batch loss 2.65517044 epoch total loss 2.6832366\n",
      "Trained batch 1797 batch loss 2.69575453 epoch total loss 2.68324351\n",
      "Trained batch 1798 batch loss 2.66284204 epoch total loss 2.68323231\n",
      "Trained batch 1799 batch loss 2.78336763 epoch total loss 2.68328786\n",
      "Trained batch 1800 batch loss 2.75199246 epoch total loss 2.68332601\n",
      "Trained batch 1801 batch loss 2.54528451 epoch total loss 2.68324947\n",
      "Trained batch 1802 batch loss 2.60557842 epoch total loss 2.68320632\n",
      "Trained batch 1803 batch loss 2.5585897 epoch total loss 2.68313718\n",
      "Trained batch 1804 batch loss 2.48190141 epoch total loss 2.6830256\n",
      "Trained batch 1805 batch loss 2.37931633 epoch total loss 2.68285751\n",
      "Trained batch 1806 batch loss 2.71501064 epoch total loss 2.68287516\n",
      "Trained batch 1807 batch loss 2.53465509 epoch total loss 2.68279314\n",
      "Trained batch 1808 batch loss 2.66882229 epoch total loss 2.68278551\n",
      "Trained batch 1809 batch loss 2.63172626 epoch total loss 2.68275738\n",
      "Trained batch 1810 batch loss 2.45735025 epoch total loss 2.68263292\n",
      "Trained batch 1811 batch loss 2.43583417 epoch total loss 2.68249679\n",
      "Trained batch 1812 batch loss 2.51402473 epoch total loss 2.6824038\n",
      "Trained batch 1813 batch loss 2.35398769 epoch total loss 2.6822226\n",
      "Trained batch 1814 batch loss 2.25189567 epoch total loss 2.68198538\n",
      "Trained batch 1815 batch loss 2.4254477 epoch total loss 2.681844\n",
      "Trained batch 1816 batch loss 2.2948904 epoch total loss 2.68163085\n",
      "Trained batch 1817 batch loss 2.54081368 epoch total loss 2.6815536\n",
      "Trained batch 1818 batch loss 2.73804975 epoch total loss 2.68158484\n",
      "Trained batch 1819 batch loss 2.60521626 epoch total loss 2.68154263\n",
      "Trained batch 1820 batch loss 2.65184474 epoch total loss 2.68152642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1821 batch loss 2.6782763 epoch total loss 2.68152452\n",
      "Trained batch 1822 batch loss 2.41586041 epoch total loss 2.68137884\n",
      "Trained batch 1823 batch loss 2.87345147 epoch total loss 2.68148422\n",
      "Trained batch 1824 batch loss 2.86252761 epoch total loss 2.6815834\n",
      "Trained batch 1825 batch loss 2.74720931 epoch total loss 2.68161917\n",
      "Trained batch 1826 batch loss 2.78266358 epoch total loss 2.68167448\n",
      "Trained batch 1827 batch loss 2.49101281 epoch total loss 2.68157029\n",
      "Trained batch 1828 batch loss 2.72654867 epoch total loss 2.68159485\n",
      "Trained batch 1829 batch loss 2.62464714 epoch total loss 2.68156385\n",
      "Trained batch 1830 batch loss 2.52834916 epoch total loss 2.68148\n",
      "Trained batch 1831 batch loss 2.54434562 epoch total loss 2.68140507\n",
      "Trained batch 1832 batch loss 2.86040664 epoch total loss 2.68150282\n",
      "Trained batch 1833 batch loss 2.78971291 epoch total loss 2.68156171\n",
      "Trained batch 1834 batch loss 2.68538165 epoch total loss 2.68156385\n",
      "Trained batch 1835 batch loss 2.52027273 epoch total loss 2.68147612\n",
      "Trained batch 1836 batch loss 2.62270498 epoch total loss 2.68144417\n",
      "Trained batch 1837 batch loss 2.6258347 epoch total loss 2.68141389\n",
      "Trained batch 1838 batch loss 2.60575294 epoch total loss 2.68137288\n",
      "Trained batch 1839 batch loss 2.55972099 epoch total loss 2.6813066\n",
      "Trained batch 1840 batch loss 2.41104 epoch total loss 2.68115973\n",
      "Trained batch 1841 batch loss 2.57695746 epoch total loss 2.68110323\n",
      "Trained batch 1842 batch loss 2.41935563 epoch total loss 2.68096113\n",
      "Trained batch 1843 batch loss 2.8701334 epoch total loss 2.68106389\n",
      "Trained batch 1844 batch loss 2.82630658 epoch total loss 2.68114257\n",
      "Trained batch 1845 batch loss 2.71171761 epoch total loss 2.68115926\n",
      "Trained batch 1846 batch loss 2.69157052 epoch total loss 2.68116474\n",
      "Trained batch 1847 batch loss 2.70906925 epoch total loss 2.68117976\n",
      "Trained batch 1848 batch loss 2.92221451 epoch total loss 2.68131042\n",
      "Trained batch 1849 batch loss 2.62483454 epoch total loss 2.68128\n",
      "Trained batch 1850 batch loss 2.20589137 epoch total loss 2.68102312\n",
      "Trained batch 1851 batch loss 2.43060565 epoch total loss 2.6808877\n",
      "Trained batch 1852 batch loss 2.42057323 epoch total loss 2.68074703\n",
      "Trained batch 1853 batch loss 2.37667 epoch total loss 2.680583\n",
      "Trained batch 1854 batch loss 2.71559954 epoch total loss 2.68060184\n",
      "Trained batch 1855 batch loss 2.86761808 epoch total loss 2.68070269\n",
      "Trained batch 1856 batch loss 2.7301383 epoch total loss 2.68072939\n",
      "Trained batch 1857 batch loss 2.97673655 epoch total loss 2.68088865\n",
      "Trained batch 1858 batch loss 2.76224 epoch total loss 2.68093228\n",
      "Trained batch 1859 batch loss 2.57346 epoch total loss 2.68087435\n",
      "Trained batch 1860 batch loss 2.93581843 epoch total loss 2.68101168\n",
      "Trained batch 1861 batch loss 2.8396132 epoch total loss 2.68109703\n",
      "Trained batch 1862 batch loss 2.69418764 epoch total loss 2.68110418\n",
      "Trained batch 1863 batch loss 3.00585151 epoch total loss 2.68127847\n",
      "Trained batch 1864 batch loss 2.86087227 epoch total loss 2.68137479\n",
      "Trained batch 1865 batch loss 2.6917119 epoch total loss 2.68138027\n",
      "Trained batch 1866 batch loss 2.75268888 epoch total loss 2.68141866\n",
      "Trained batch 1867 batch loss 2.82148147 epoch total loss 2.68149376\n",
      "Trained batch 1868 batch loss 2.57891321 epoch total loss 2.68143892\n",
      "Trained batch 1869 batch loss 2.49636173 epoch total loss 2.68134\n",
      "Trained batch 1870 batch loss 2.28456879 epoch total loss 2.68112779\n",
      "Trained batch 1871 batch loss 2.46811819 epoch total loss 2.68101406\n",
      "Trained batch 1872 batch loss 2.44142675 epoch total loss 2.68088603\n",
      "Trained batch 1873 batch loss 2.79118919 epoch total loss 2.68094492\n",
      "Trained batch 1874 batch loss 3.21406674 epoch total loss 2.68122911\n",
      "Trained batch 1875 batch loss 2.74152541 epoch total loss 2.68126154\n",
      "Trained batch 1876 batch loss 3.01196384 epoch total loss 2.68143797\n",
      "Trained batch 1877 batch loss 2.69780731 epoch total loss 2.68144655\n",
      "Trained batch 1878 batch loss 2.81318521 epoch total loss 2.68151665\n",
      "Trained batch 1879 batch loss 2.91196012 epoch total loss 2.68163943\n",
      "Trained batch 1880 batch loss 2.80378437 epoch total loss 2.68170428\n",
      "Trained batch 1881 batch loss 2.67503738 epoch total loss 2.68170071\n",
      "Trained batch 1882 batch loss 2.70906472 epoch total loss 2.68171501\n",
      "Trained batch 1883 batch loss 2.58384013 epoch total loss 2.68166327\n",
      "Trained batch 1884 batch loss 2.66643715 epoch total loss 2.68165517\n",
      "Trained batch 1885 batch loss 2.57958937 epoch total loss 2.68160105\n",
      "Trained batch 1886 batch loss 2.43825626 epoch total loss 2.68147206\n",
      "Trained batch 1887 batch loss 2.68312573 epoch total loss 2.68147302\n",
      "Trained batch 1888 batch loss 2.73455477 epoch total loss 2.68150091\n",
      "Trained batch 1889 batch loss 2.7100625 epoch total loss 2.68151593\n",
      "Trained batch 1890 batch loss 2.57039142 epoch total loss 2.68145728\n",
      "Trained batch 1891 batch loss 2.71575832 epoch total loss 2.6814754\n",
      "Trained batch 1892 batch loss 2.62195039 epoch total loss 2.68144393\n",
      "Trained batch 1893 batch loss 2.49422741 epoch total loss 2.68134499\n",
      "Trained batch 1894 batch loss 2.67357659 epoch total loss 2.68134069\n",
      "Trained batch 1895 batch loss 2.55616879 epoch total loss 2.68127465\n",
      "Trained batch 1896 batch loss 2.7471118 epoch total loss 2.68130946\n",
      "Trained batch 1897 batch loss 2.34019327 epoch total loss 2.68112969\n",
      "Trained batch 1898 batch loss 2.15701199 epoch total loss 2.68085361\n",
      "Trained batch 1899 batch loss 2.64363313 epoch total loss 2.68083405\n",
      "Trained batch 1900 batch loss 2.41304827 epoch total loss 2.68069315\n",
      "Trained batch 1901 batch loss 2.30113506 epoch total loss 2.68049359\n",
      "Trained batch 1902 batch loss 2.18920803 epoch total loss 2.68023515\n",
      "Trained batch 1903 batch loss 2.12997484 epoch total loss 2.67994595\n",
      "Trained batch 1904 batch loss 2.20317817 epoch total loss 2.67969537\n",
      "Trained batch 1905 batch loss 2.31428552 epoch total loss 2.67950368\n",
      "Trained batch 1906 batch loss 2.91397333 epoch total loss 2.6796267\n",
      "Trained batch 1907 batch loss 3.01659966 epoch total loss 2.67980337\n",
      "Trained batch 1908 batch loss 2.78813052 epoch total loss 2.67986035\n",
      "Trained batch 1909 batch loss 2.68579221 epoch total loss 2.67986345\n",
      "Trained batch 1910 batch loss 3.03351 epoch total loss 2.6800487\n",
      "Trained batch 1911 batch loss 3.1947 epoch total loss 2.68031812\n",
      "Trained batch 1912 batch loss 2.8386004 epoch total loss 2.68040085\n",
      "Trained batch 1913 batch loss 2.95821977 epoch total loss 2.68054581\n",
      "Trained batch 1914 batch loss 3.06949306 epoch total loss 2.68074894\n",
      "Trained batch 1915 batch loss 2.92658472 epoch total loss 2.68087745\n",
      "Trained batch 1916 batch loss 2.88740301 epoch total loss 2.68098521\n",
      "Trained batch 1917 batch loss 3.04632401 epoch total loss 2.68117571\n",
      "Trained batch 1918 batch loss 2.87080717 epoch total loss 2.68127465\n",
      "Trained batch 1919 batch loss 2.86035824 epoch total loss 2.68136787\n",
      "Trained batch 1920 batch loss 2.88304615 epoch total loss 2.68147278\n",
      "Trained batch 1921 batch loss 2.50844812 epoch total loss 2.68138266\n",
      "Trained batch 1922 batch loss 2.43537211 epoch total loss 2.68125463\n",
      "Trained batch 1923 batch loss 2.66803074 epoch total loss 2.68124771\n",
      "Trained batch 1924 batch loss 2.62951851 epoch total loss 2.68122077\n",
      "Trained batch 1925 batch loss 2.52848458 epoch total loss 2.68114138\n",
      "Trained batch 1926 batch loss 2.68569922 epoch total loss 2.68114376\n",
      "Trained batch 1927 batch loss 2.53523397 epoch total loss 2.68106794\n",
      "Trained batch 1928 batch loss 2.41226673 epoch total loss 2.68092847\n",
      "Trained batch 1929 batch loss 2.76370144 epoch total loss 2.68097138\n",
      "Trained batch 1930 batch loss 2.59187937 epoch total loss 2.68092513\n",
      "Trained batch 1931 batch loss 2.4309 epoch total loss 2.68079567\n",
      "Trained batch 1932 batch loss 2.84071302 epoch total loss 2.6808784\n",
      "Trained batch 1933 batch loss 2.91312957 epoch total loss 2.68099856\n",
      "Trained batch 1934 batch loss 2.95732856 epoch total loss 2.68114138\n",
      "Trained batch 1935 batch loss 2.29620719 epoch total loss 2.68094254\n",
      "Trained batch 1936 batch loss 2.5148654 epoch total loss 2.6808567\n",
      "Trained batch 1937 batch loss 2.67973185 epoch total loss 2.68085623\n",
      "Trained batch 1938 batch loss 2.50975084 epoch total loss 2.68076777\n",
      "Trained batch 1939 batch loss 2.7350359 epoch total loss 2.68079567\n",
      "Trained batch 1940 batch loss 2.49541593 epoch total loss 2.6807003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1941 batch loss 2.71247435 epoch total loss 2.68071675\n",
      "Trained batch 1942 batch loss 2.02803493 epoch total loss 2.68038034\n",
      "Trained batch 1943 batch loss 2.40293765 epoch total loss 2.68023753\n",
      "Trained batch 1944 batch loss 2.52878737 epoch total loss 2.68015981\n",
      "Trained batch 1945 batch loss 2.57650185 epoch total loss 2.6801064\n",
      "Trained batch 1946 batch loss 2.71573734 epoch total loss 2.68012476\n",
      "Trained batch 1947 batch loss 2.43037 epoch total loss 2.67999649\n",
      "Trained batch 1948 batch loss 2.20067501 epoch total loss 2.67975044\n",
      "Trained batch 1949 batch loss 2.39258957 epoch total loss 2.6796031\n",
      "Trained batch 1950 batch loss 2.24556589 epoch total loss 2.67938042\n",
      "Trained batch 1951 batch loss 2.3634038 epoch total loss 2.67921853\n",
      "Trained batch 1952 batch loss 2.54046941 epoch total loss 2.67914748\n",
      "Trained batch 1953 batch loss 2.25286579 epoch total loss 2.67892909\n",
      "Trained batch 1954 batch loss 2.32246518 epoch total loss 2.6787467\n",
      "Trained batch 1955 batch loss 2.54380631 epoch total loss 2.6786778\n",
      "Trained batch 1956 batch loss 3.09383059 epoch total loss 2.67889\n",
      "Trained batch 1957 batch loss 2.78531218 epoch total loss 2.67894411\n",
      "Trained batch 1958 batch loss 2.53123856 epoch total loss 2.67886877\n",
      "Trained batch 1959 batch loss 2.6024487 epoch total loss 2.67883\n",
      "Trained batch 1960 batch loss 2.67643595 epoch total loss 2.67882848\n",
      "Trained batch 1961 batch loss 2.37720346 epoch total loss 2.67867494\n",
      "Trained batch 1962 batch loss 2.47721195 epoch total loss 2.67857218\n",
      "Trained batch 1963 batch loss 2.79082799 epoch total loss 2.6786294\n",
      "Trained batch 1964 batch loss 2.54640603 epoch total loss 2.67856193\n",
      "Trained batch 1965 batch loss 2.28597164 epoch total loss 2.67836237\n",
      "Trained batch 1966 batch loss 2.8819313 epoch total loss 2.67846584\n",
      "Trained batch 1967 batch loss 2.89050841 epoch total loss 2.67857361\n",
      "Trained batch 1968 batch loss 2.86290026 epoch total loss 2.67866731\n",
      "Trained batch 1969 batch loss 3.0637126 epoch total loss 2.67886281\n",
      "Trained batch 1970 batch loss 3.47399426 epoch total loss 2.67926645\n",
      "Trained batch 1971 batch loss 3.14128828 epoch total loss 2.67950082\n",
      "Trained batch 1972 batch loss 2.53069019 epoch total loss 2.67942524\n",
      "Trained batch 1973 batch loss 3.21119499 epoch total loss 2.67969489\n",
      "Trained batch 1974 batch loss 2.90517473 epoch total loss 2.67980909\n",
      "Trained batch 1975 batch loss 2.95507073 epoch total loss 2.67994857\n",
      "Trained batch 1976 batch loss 2.82329321 epoch total loss 2.68002105\n",
      "Trained batch 1977 batch loss 2.38692284 epoch total loss 2.67987275\n",
      "Trained batch 1978 batch loss 2.965904 epoch total loss 2.68001723\n",
      "Trained batch 1979 batch loss 2.91019869 epoch total loss 2.68013358\n",
      "Trained batch 1980 batch loss 2.68161392 epoch total loss 2.6801343\n",
      "Trained batch 1981 batch loss 2.74073601 epoch total loss 2.68016505\n",
      "Trained batch 1982 batch loss 2.72454309 epoch total loss 2.68018746\n",
      "Trained batch 1983 batch loss 2.84846878 epoch total loss 2.68027234\n",
      "Trained batch 1984 batch loss 2.80184531 epoch total loss 2.68033361\n",
      "Trained batch 1985 batch loss 2.69474792 epoch total loss 2.68034077\n",
      "Trained batch 1986 batch loss 2.58590388 epoch total loss 2.68029332\n",
      "Trained batch 1987 batch loss 2.71259379 epoch total loss 2.68030953\n",
      "Trained batch 1988 batch loss 2.46725941 epoch total loss 2.68020225\n",
      "Trained batch 1989 batch loss 2.7028985 epoch total loss 2.68021393\n",
      "Trained batch 1990 batch loss 3.08676887 epoch total loss 2.68041825\n",
      "Trained batch 1991 batch loss 2.82025266 epoch total loss 2.68048859\n",
      "Trained batch 1992 batch loss 2.77474642 epoch total loss 2.68053579\n",
      "Trained batch 1993 batch loss 2.58206391 epoch total loss 2.68048644\n",
      "Trained batch 1994 batch loss 2.74285769 epoch total loss 2.68051767\n",
      "Trained batch 1995 batch loss 2.45154667 epoch total loss 2.68040299\n",
      "Trained batch 1996 batch loss 2.49312663 epoch total loss 2.68030906\n",
      "Trained batch 1997 batch loss 2.49181867 epoch total loss 2.68021464\n",
      "Trained batch 1998 batch loss 2.59809589 epoch total loss 2.68017364\n",
      "Trained batch 1999 batch loss 2.48108864 epoch total loss 2.68007398\n",
      "Trained batch 2000 batch loss 2.46384406 epoch total loss 2.67996573\n",
      "Trained batch 2001 batch loss 2.54277515 epoch total loss 2.67989731\n",
      "Trained batch 2002 batch loss 2.60441542 epoch total loss 2.67985964\n",
      "Trained batch 2003 batch loss 2.88081074 epoch total loss 2.67996\n",
      "Trained batch 2004 batch loss 2.74132895 epoch total loss 2.67999053\n",
      "Trained batch 2005 batch loss 3.15458155 epoch total loss 2.68022752\n",
      "Trained batch 2006 batch loss 3.2173214 epoch total loss 2.68049502\n",
      "Trained batch 2007 batch loss 3.11289954 epoch total loss 2.68071055\n",
      "Trained batch 2008 batch loss 2.76523685 epoch total loss 2.68075252\n",
      "Trained batch 2009 batch loss 2.73310161 epoch total loss 2.6807785\n",
      "Trained batch 2010 batch loss 2.82803631 epoch total loss 2.68085194\n",
      "Trained batch 2011 batch loss 2.80544806 epoch total loss 2.68091393\n",
      "Trained batch 2012 batch loss 2.68868756 epoch total loss 2.68091774\n",
      "Trained batch 2013 batch loss 2.39923215 epoch total loss 2.68077779\n",
      "Trained batch 2014 batch loss 2.79698348 epoch total loss 2.68083549\n",
      "Trained batch 2015 batch loss 2.5658474 epoch total loss 2.6807785\n",
      "Trained batch 2016 batch loss 2.61343789 epoch total loss 2.68074489\n",
      "Trained batch 2017 batch loss 2.05579066 epoch total loss 2.68043494\n",
      "Trained batch 2018 batch loss 2.84817076 epoch total loss 2.68051815\n",
      "Trained batch 2019 batch loss 2.67697453 epoch total loss 2.68051624\n",
      "Trained batch 2020 batch loss 2.6439023 epoch total loss 2.68049812\n",
      "Trained batch 2021 batch loss 2.75081491 epoch total loss 2.68053317\n",
      "Trained batch 2022 batch loss 2.68379903 epoch total loss 2.6805346\n",
      "Trained batch 2023 batch loss 2.44617105 epoch total loss 2.68041873\n",
      "Trained batch 2024 batch loss 2.34834862 epoch total loss 2.6802547\n",
      "Trained batch 2025 batch loss 2.35976624 epoch total loss 2.68009639\n",
      "Trained batch 2026 batch loss 2.26309061 epoch total loss 2.67989063\n",
      "Trained batch 2027 batch loss 2.45392346 epoch total loss 2.67977929\n",
      "Trained batch 2028 batch loss 2.31884718 epoch total loss 2.67960119\n",
      "Trained batch 2029 batch loss 2.18639255 epoch total loss 2.67935824\n",
      "Trained batch 2030 batch loss 2.25539112 epoch total loss 2.67914939\n",
      "Trained batch 2031 batch loss 2.85671568 epoch total loss 2.67923689\n",
      "Trained batch 2032 batch loss 3.13211679 epoch total loss 2.67946\n",
      "Trained batch 2033 batch loss 3.17301846 epoch total loss 2.67970252\n",
      "Trained batch 2034 batch loss 3.44262266 epoch total loss 2.68007755\n",
      "Trained batch 2035 batch loss 2.90735912 epoch total loss 2.68018913\n",
      "Trained batch 2036 batch loss 3.26451349 epoch total loss 2.68047619\n",
      "Trained batch 2037 batch loss 3.03255272 epoch total loss 2.68064928\n",
      "Trained batch 2038 batch loss 2.81361055 epoch total loss 2.68071437\n",
      "Trained batch 2039 batch loss 3.01601648 epoch total loss 2.68087888\n",
      "Trained batch 2040 batch loss 2.71077752 epoch total loss 2.68089366\n",
      "Trained batch 2041 batch loss 3.21298504 epoch total loss 2.68115425\n",
      "Trained batch 2042 batch loss 3.02917099 epoch total loss 2.68132472\n",
      "Trained batch 2043 batch loss 2.98159838 epoch total loss 2.68147159\n",
      "Trained batch 2044 batch loss 2.74699712 epoch total loss 2.68150377\n",
      "Trained batch 2045 batch loss 2.75315022 epoch total loss 2.68153858\n",
      "Trained batch 2046 batch loss 2.71154237 epoch total loss 2.68155336\n",
      "Trained batch 2047 batch loss 2.78839779 epoch total loss 2.68160558\n",
      "Trained batch 2048 batch loss 2.63824463 epoch total loss 2.68158436\n",
      "Trained batch 2049 batch loss 2.74351501 epoch total loss 2.68161464\n",
      "Trained batch 2050 batch loss 2.69945192 epoch total loss 2.68162322\n",
      "Trained batch 2051 batch loss 2.36871076 epoch total loss 2.68147063\n",
      "Trained batch 2052 batch loss 2.57992935 epoch total loss 2.68142128\n",
      "Trained batch 2053 batch loss 2.39686394 epoch total loss 2.68128276\n",
      "Trained batch 2054 batch loss 2.49512959 epoch total loss 2.68119216\n",
      "Trained batch 2055 batch loss 2.89015532 epoch total loss 2.68129373\n",
      "Trained batch 2056 batch loss 2.61153698 epoch total loss 2.68125963\n",
      "Trained batch 2057 batch loss 2.72140789 epoch total loss 2.68127918\n",
      "Trained batch 2058 batch loss 2.57304811 epoch total loss 2.68122649\n",
      "Trained batch 2059 batch loss 2.69611692 epoch total loss 2.68123388\n",
      "Trained batch 2060 batch loss 2.73110175 epoch total loss 2.68125796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2061 batch loss 2.61402059 epoch total loss 2.68122554\n",
      "Trained batch 2062 batch loss 2.85520029 epoch total loss 2.6813097\n",
      "Trained batch 2063 batch loss 2.74322939 epoch total loss 2.68133974\n",
      "Trained batch 2064 batch loss 2.68459892 epoch total loss 2.68134141\n",
      "Trained batch 2065 batch loss 2.92806268 epoch total loss 2.68146086\n",
      "Trained batch 2066 batch loss 2.79506826 epoch total loss 2.68151593\n",
      "Trained batch 2067 batch loss 2.68813562 epoch total loss 2.68151903\n",
      "Trained batch 2068 batch loss 2.81819129 epoch total loss 2.68158507\n",
      "Trained batch 2069 batch loss 3.04444504 epoch total loss 2.68176055\n",
      "Trained batch 2070 batch loss 2.67637539 epoch total loss 2.68175793\n",
      "Trained batch 2071 batch loss 2.67733097 epoch total loss 2.68175578\n",
      "Trained batch 2072 batch loss 3.01547766 epoch total loss 2.68191671\n",
      "Trained batch 2073 batch loss 2.80568552 epoch total loss 2.68197656\n",
      "Trained batch 2074 batch loss 2.62599421 epoch total loss 2.68194962\n",
      "Trained batch 2075 batch loss 2.27727938 epoch total loss 2.68175459\n",
      "Trained batch 2076 batch loss 2.49453783 epoch total loss 2.68166447\n",
      "Trained batch 2077 batch loss 2.49172544 epoch total loss 2.68157291\n",
      "Trained batch 2078 batch loss 2.58688021 epoch total loss 2.68152738\n",
      "Trained batch 2079 batch loss 2.55575681 epoch total loss 2.68146682\n",
      "Trained batch 2080 batch loss 2.34146261 epoch total loss 2.68130326\n",
      "Trained batch 2081 batch loss 2.28875113 epoch total loss 2.68111467\n",
      "Trained batch 2082 batch loss 2.36963868 epoch total loss 2.68096495\n",
      "Trained batch 2083 batch loss 2.42252421 epoch total loss 2.68084073\n",
      "Trained batch 2084 batch loss 2.38032579 epoch total loss 2.68069673\n",
      "Trained batch 2085 batch loss 2.58385754 epoch total loss 2.68065023\n",
      "Trained batch 2086 batch loss 2.69243383 epoch total loss 2.68065596\n",
      "Trained batch 2087 batch loss 2.84546232 epoch total loss 2.68073487\n",
      "Trained batch 2088 batch loss 2.6252625 epoch total loss 2.68070841\n",
      "Trained batch 2089 batch loss 2.83696222 epoch total loss 2.68078327\n",
      "Trained batch 2090 batch loss 2.4795208 epoch total loss 2.68068695\n",
      "Trained batch 2091 batch loss 2.75765586 epoch total loss 2.68072391\n",
      "Trained batch 2092 batch loss 2.79633617 epoch total loss 2.68077922\n",
      "Trained batch 2093 batch loss 2.61572337 epoch total loss 2.68074799\n",
      "Trained batch 2094 batch loss 2.56521916 epoch total loss 2.68069291\n",
      "Trained batch 2095 batch loss 2.47231936 epoch total loss 2.68059349\n",
      "Trained batch 2096 batch loss 2.75728822 epoch total loss 2.68063\n",
      "Trained batch 2097 batch loss 3.18924642 epoch total loss 2.68087268\n",
      "Trained batch 2098 batch loss 2.95872593 epoch total loss 2.681005\n",
      "Trained batch 2099 batch loss 2.67276597 epoch total loss 2.68100119\n",
      "Trained batch 2100 batch loss 2.38399959 epoch total loss 2.68085957\n",
      "Trained batch 2101 batch loss 2.57732511 epoch total loss 2.68081021\n",
      "Trained batch 2102 batch loss 2.89965272 epoch total loss 2.68091416\n",
      "Trained batch 2103 batch loss 2.60633683 epoch total loss 2.68087888\n",
      "Trained batch 2104 batch loss 2.8655889 epoch total loss 2.68096662\n",
      "Trained batch 2105 batch loss 2.94102716 epoch total loss 2.68109\n",
      "Trained batch 2106 batch loss 2.78536987 epoch total loss 2.68113971\n",
      "Trained batch 2107 batch loss 2.80171013 epoch total loss 2.68119693\n",
      "Trained batch 2108 batch loss 2.39498711 epoch total loss 2.68106103\n",
      "Trained batch 2109 batch loss 2.65808535 epoch total loss 2.6810503\n",
      "Trained batch 2110 batch loss 2.7432847 epoch total loss 2.68107963\n",
      "Trained batch 2111 batch loss 2.42337489 epoch total loss 2.68095756\n",
      "Trained batch 2112 batch loss 2.54912686 epoch total loss 2.68089533\n",
      "Trained batch 2113 batch loss 2.47217488 epoch total loss 2.68079638\n",
      "Trained batch 2114 batch loss 2.36611462 epoch total loss 2.68064761\n",
      "Trained batch 2115 batch loss 2.82753849 epoch total loss 2.68071723\n",
      "Trained batch 2116 batch loss 2.59616423 epoch total loss 2.68067718\n",
      "Trained batch 2117 batch loss 2.62843537 epoch total loss 2.68065262\n",
      "Trained batch 2118 batch loss 2.61838126 epoch total loss 2.68062305\n",
      "Trained batch 2119 batch loss 2.63546133 epoch total loss 2.6806016\n",
      "Trained batch 2120 batch loss 2.72263193 epoch total loss 2.68062139\n",
      "Trained batch 2121 batch loss 2.78779745 epoch total loss 2.68067193\n",
      "Trained batch 2122 batch loss 2.2629621 epoch total loss 2.68047523\n",
      "Trained batch 2123 batch loss 2.42106223 epoch total loss 2.68035293\n",
      "Trained batch 2124 batch loss 2.54021955 epoch total loss 2.68028688\n",
      "Trained batch 2125 batch loss 2.71626139 epoch total loss 2.68030381\n",
      "Trained batch 2126 batch loss 2.27130842 epoch total loss 2.68011141\n",
      "Trained batch 2127 batch loss 2.40117884 epoch total loss 2.67998052\n",
      "Trained batch 2128 batch loss 2.52803636 epoch total loss 2.67990899\n",
      "Trained batch 2129 batch loss 2.55166388 epoch total loss 2.67984867\n",
      "Trained batch 2130 batch loss 2.61229134 epoch total loss 2.67981696\n",
      "Trained batch 2131 batch loss 2.30946779 epoch total loss 2.67964339\n",
      "Trained batch 2132 batch loss 2.65711117 epoch total loss 2.67963266\n",
      "Trained batch 2133 batch loss 2.49412775 epoch total loss 2.67954588\n",
      "Trained batch 2134 batch loss 2.87449646 epoch total loss 2.67963719\n",
      "Trained batch 2135 batch loss 2.87404 epoch total loss 2.67972827\n",
      "Trained batch 2136 batch loss 2.79013538 epoch total loss 2.67977977\n",
      "Trained batch 2137 batch loss 2.76356387 epoch total loss 2.67981911\n",
      "Trained batch 2138 batch loss 2.60917497 epoch total loss 2.67978621\n",
      "Trained batch 2139 batch loss 2.93458891 epoch total loss 2.67990518\n",
      "Trained batch 2140 batch loss 2.81216121 epoch total loss 2.67996693\n",
      "Trained batch 2141 batch loss 3.1217494 epoch total loss 2.68017316\n",
      "Trained batch 2142 batch loss 2.84226131 epoch total loss 2.68024898\n",
      "Trained batch 2143 batch loss 2.92028618 epoch total loss 2.68036103\n",
      "Trained batch 2144 batch loss 3.21061492 epoch total loss 2.68060827\n",
      "Trained batch 2145 batch loss 3.01656675 epoch total loss 2.68076491\n",
      "Trained batch 2146 batch loss 2.61521339 epoch total loss 2.6807344\n",
      "Trained batch 2147 batch loss 2.51733732 epoch total loss 2.68065834\n",
      "Trained batch 2148 batch loss 2.29853606 epoch total loss 2.68048048\n",
      "Trained batch 2149 batch loss 2.34983301 epoch total loss 2.68032646\n",
      "Trained batch 2150 batch loss 2.86051702 epoch total loss 2.68041015\n",
      "Trained batch 2151 batch loss 2.62189865 epoch total loss 2.68038297\n",
      "Trained batch 2152 batch loss 2.74364901 epoch total loss 2.68041253\n",
      "Trained batch 2153 batch loss 2.38922787 epoch total loss 2.68027711\n",
      "Trained batch 2154 batch loss 2.32600689 epoch total loss 2.68011284\n",
      "Trained batch 2155 batch loss 2.15539455 epoch total loss 2.67986917\n",
      "Trained batch 2156 batch loss 2.31432581 epoch total loss 2.67969966\n",
      "Trained batch 2157 batch loss 2.4515121 epoch total loss 2.67959404\n",
      "Trained batch 2158 batch loss 2.43320131 epoch total loss 2.67947984\n",
      "Trained batch 2159 batch loss 2.46920872 epoch total loss 2.67938232\n",
      "Trained batch 2160 batch loss 2.58737969 epoch total loss 2.67934\n",
      "Trained batch 2161 batch loss 2.13969135 epoch total loss 2.67909\n",
      "Trained batch 2162 batch loss 2.27531266 epoch total loss 2.67890334\n",
      "Trained batch 2163 batch loss 2.1270957 epoch total loss 2.67864823\n",
      "Trained batch 2164 batch loss 2.18534017 epoch total loss 2.67842031\n",
      "Trained batch 2165 batch loss 2.2022903 epoch total loss 2.67820024\n",
      "Trained batch 2166 batch loss 2.85782862 epoch total loss 2.67828321\n",
      "Trained batch 2167 batch loss 2.33216619 epoch total loss 2.67812347\n",
      "Trained batch 2168 batch loss 2.93602753 epoch total loss 2.67824244\n",
      "Trained batch 2169 batch loss 2.30035162 epoch total loss 2.67806816\n",
      "Trained batch 2170 batch loss 2.28025031 epoch total loss 2.67788482\n",
      "Trained batch 2171 batch loss 2.4717977 epoch total loss 2.67779\n",
      "Trained batch 2172 batch loss 2.22873616 epoch total loss 2.67758298\n",
      "Trained batch 2173 batch loss 2.35058904 epoch total loss 2.67743254\n",
      "Trained batch 2174 batch loss 2.18816042 epoch total loss 2.67720747\n",
      "Trained batch 2175 batch loss 2.22438765 epoch total loss 2.67699933\n",
      "Trained batch 2176 batch loss 2.07768893 epoch total loss 2.67672396\n",
      "Trained batch 2177 batch loss 2.21892309 epoch total loss 2.67651367\n",
      "Trained batch 2178 batch loss 2.38692856 epoch total loss 2.6763804\n",
      "Trained batch 2179 batch loss 2.79734731 epoch total loss 2.67643595\n",
      "Trained batch 2180 batch loss 2.91624236 epoch total loss 2.67654586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2181 batch loss 3.03431416 epoch total loss 2.67671\n",
      "Trained batch 2182 batch loss 2.82619095 epoch total loss 2.67677832\n",
      "Trained batch 2183 batch loss 2.8031621 epoch total loss 2.67683625\n",
      "Trained batch 2184 batch loss 3.05124426 epoch total loss 2.67700768\n",
      "Trained batch 2185 batch loss 2.60772872 epoch total loss 2.6769762\n",
      "Trained batch 2186 batch loss 2.77874517 epoch total loss 2.6770227\n",
      "Trained batch 2187 batch loss 2.81781578 epoch total loss 2.67708707\n",
      "Trained batch 2188 batch loss 3.14373803 epoch total loss 2.67730021\n",
      "Trained batch 2189 batch loss 2.84472299 epoch total loss 2.67737675\n",
      "Trained batch 2190 batch loss 2.64671612 epoch total loss 2.67736268\n",
      "Trained batch 2191 batch loss 2.57298422 epoch total loss 2.677315\n",
      "Trained batch 2192 batch loss 2.68455076 epoch total loss 2.67731833\n",
      "Trained batch 2193 batch loss 2.91175842 epoch total loss 2.67742515\n",
      "Trained batch 2194 batch loss 2.81608462 epoch total loss 2.67748833\n",
      "Trained batch 2195 batch loss 2.90365934 epoch total loss 2.67759132\n",
      "Trained batch 2196 batch loss 2.71455812 epoch total loss 2.67760801\n",
      "Trained batch 2197 batch loss 2.72800422 epoch total loss 2.67763114\n",
      "Trained batch 2198 batch loss 2.42769909 epoch total loss 2.67751741\n",
      "Trained batch 2199 batch loss 2.54087257 epoch total loss 2.67745519\n",
      "Trained batch 2200 batch loss 2.55998611 epoch total loss 2.67740178\n",
      "Trained batch 2201 batch loss 2.76277471 epoch total loss 2.67744064\n",
      "Trained batch 2202 batch loss 2.18731213 epoch total loss 2.6772182\n",
      "Trained batch 2203 batch loss 2.268574 epoch total loss 2.67703271\n",
      "Trained batch 2204 batch loss 2.26011658 epoch total loss 2.67684364\n",
      "Trained batch 2205 batch loss 2.36626291 epoch total loss 2.67670274\n",
      "Trained batch 2206 batch loss 2.56551623 epoch total loss 2.67665219\n",
      "Trained batch 2207 batch loss 2.56687069 epoch total loss 2.6766026\n",
      "Trained batch 2208 batch loss 2.73990679 epoch total loss 2.67663121\n",
      "Trained batch 2209 batch loss 3.05211782 epoch total loss 2.6768012\n",
      "Trained batch 2210 batch loss 2.66304612 epoch total loss 2.67679501\n",
      "Trained batch 2211 batch loss 2.5799551 epoch total loss 2.67675114\n",
      "Trained batch 2212 batch loss 2.88565516 epoch total loss 2.67684555\n",
      "Trained batch 2213 batch loss 3.01954031 epoch total loss 2.67700052\n",
      "Trained batch 2214 batch loss 2.81422424 epoch total loss 2.67706251\n",
      "Trained batch 2215 batch loss 2.72452307 epoch total loss 2.67708397\n",
      "Trained batch 2216 batch loss 2.89416695 epoch total loss 2.67718196\n",
      "Trained batch 2217 batch loss 2.30046749 epoch total loss 2.67701197\n",
      "Trained batch 2218 batch loss 1.95275819 epoch total loss 2.67668533\n",
      "Trained batch 2219 batch loss 2.46013689 epoch total loss 2.67658782\n",
      "Trained batch 2220 batch loss 1.95586658 epoch total loss 2.67626309\n",
      "Trained batch 2221 batch loss 2.24484396 epoch total loss 2.67606878\n",
      "Trained batch 2222 batch loss 2.36964726 epoch total loss 2.67593098\n",
      "Trained batch 2223 batch loss 2.61720014 epoch total loss 2.67590451\n",
      "Trained batch 2224 batch loss 2.49838305 epoch total loss 2.67582464\n",
      "Trained batch 2225 batch loss 2.44140625 epoch total loss 2.67571926\n",
      "Trained batch 2226 batch loss 2.37491918 epoch total loss 2.67558432\n",
      "Trained batch 2227 batch loss 2.32776356 epoch total loss 2.67542791\n",
      "Trained batch 2228 batch loss 2.51457167 epoch total loss 2.67535591\n",
      "Trained batch 2229 batch loss 2.38085127 epoch total loss 2.67522383\n",
      "Trained batch 2230 batch loss 2.27463031 epoch total loss 2.67504406\n",
      "Trained batch 2231 batch loss 2.48986983 epoch total loss 2.67496085\n",
      "Trained batch 2232 batch loss 2.29739 epoch total loss 2.67479181\n",
      "Trained batch 2233 batch loss 2.58475685 epoch total loss 2.67475152\n",
      "Trained batch 2234 batch loss 2.72766924 epoch total loss 2.67477512\n",
      "Trained batch 2235 batch loss 2.61121678 epoch total loss 2.67474675\n",
      "Trained batch 2236 batch loss 2.44499302 epoch total loss 2.67464399\n",
      "Trained batch 2237 batch loss 2.82936454 epoch total loss 2.67471313\n",
      "Trained batch 2238 batch loss 2.74396276 epoch total loss 2.67474437\n",
      "Trained batch 2239 batch loss 2.27654672 epoch total loss 2.67456627\n",
      "Trained batch 2240 batch loss 2.23590422 epoch total loss 2.67437053\n",
      "Trained batch 2241 batch loss 2.29123211 epoch total loss 2.67419934\n",
      "Trained batch 2242 batch loss 2.41578841 epoch total loss 2.67408419\n",
      "Trained batch 2243 batch loss 2.3491869 epoch total loss 2.67393947\n",
      "Trained batch 2244 batch loss 2.46562 epoch total loss 2.67384672\n",
      "Trained batch 2245 batch loss 2.54864025 epoch total loss 2.67379093\n",
      "Trained batch 2246 batch loss 2.53592682 epoch total loss 2.67372966\n",
      "Trained batch 2247 batch loss 2.55451488 epoch total loss 2.67367673\n",
      "Trained batch 2248 batch loss 2.85694098 epoch total loss 2.67375827\n",
      "Trained batch 2249 batch loss 2.68254232 epoch total loss 2.67376208\n",
      "Trained batch 2250 batch loss 3.06789017 epoch total loss 2.67393732\n",
      "Trained batch 2251 batch loss 2.80184937 epoch total loss 2.67399406\n",
      "Trained batch 2252 batch loss 3.15076137 epoch total loss 2.67420578\n",
      "Trained batch 2253 batch loss 3.10275912 epoch total loss 2.67439604\n",
      "Trained batch 2254 batch loss 2.93367672 epoch total loss 2.67451096\n",
      "Trained batch 2255 batch loss 2.53864908 epoch total loss 2.67445064\n",
      "Trained batch 2256 batch loss 2.18863773 epoch total loss 2.67423534\n",
      "Trained batch 2257 batch loss 2.60933447 epoch total loss 2.6742065\n",
      "Trained batch 2258 batch loss 2.56491518 epoch total loss 2.6741581\n",
      "Trained batch 2259 batch loss 2.42016959 epoch total loss 2.6740458\n",
      "Trained batch 2260 batch loss 2.59582806 epoch total loss 2.67401123\n",
      "Trained batch 2261 batch loss 2.39741969 epoch total loss 2.67388892\n",
      "Trained batch 2262 batch loss 2.74036908 epoch total loss 2.67391825\n",
      "Trained batch 2263 batch loss 2.84855032 epoch total loss 2.67399526\n",
      "Trained batch 2264 batch loss 2.88290024 epoch total loss 2.67408752\n",
      "Trained batch 2265 batch loss 2.78438234 epoch total loss 2.67413616\n",
      "Trained batch 2266 batch loss 2.73735499 epoch total loss 2.67416406\n",
      "Trained batch 2267 batch loss 2.68119287 epoch total loss 2.67416716\n",
      "Trained batch 2268 batch loss 2.44913554 epoch total loss 2.67406797\n",
      "Trained batch 2269 batch loss 3.23495 epoch total loss 2.67431521\n",
      "Trained batch 2270 batch loss 3.14580631 epoch total loss 2.67452288\n",
      "Trained batch 2271 batch loss 2.94985628 epoch total loss 2.67464399\n",
      "Trained batch 2272 batch loss 3.23653793 epoch total loss 2.67489123\n",
      "Trained batch 2273 batch loss 3.01589203 epoch total loss 2.67504144\n",
      "Trained batch 2274 batch loss 2.47871685 epoch total loss 2.67495489\n",
      "Trained batch 2275 batch loss 2.46825552 epoch total loss 2.67486405\n",
      "Trained batch 2276 batch loss 2.6030302 epoch total loss 2.67483258\n",
      "Trained batch 2277 batch loss 2.55620885 epoch total loss 2.67478037\n",
      "Trained batch 2278 batch loss 2.49246097 epoch total loss 2.6747005\n",
      "Trained batch 2279 batch loss 2.06940269 epoch total loss 2.6744349\n",
      "Trained batch 2280 batch loss 1.95364881 epoch total loss 2.67411876\n",
      "Trained batch 2281 batch loss 2.41431475 epoch total loss 2.67400503\n",
      "Trained batch 2282 batch loss 2.3952117 epoch total loss 2.67388272\n",
      "Trained batch 2283 batch loss 2.62505 epoch total loss 2.67386127\n",
      "Trained batch 2284 batch loss 2.4474771 epoch total loss 2.67376208\n",
      "Trained batch 2285 batch loss 2.46007609 epoch total loss 2.67366838\n",
      "Trained batch 2286 batch loss 2.07305574 epoch total loss 2.67340589\n",
      "Trained batch 2287 batch loss 1.86078751 epoch total loss 2.67305064\n",
      "Trained batch 2288 batch loss 2.08918571 epoch total loss 2.67279553\n",
      "Trained batch 2289 batch loss 2.01089287 epoch total loss 2.67250609\n",
      "Trained batch 2290 batch loss 2.27742648 epoch total loss 2.67233372\n",
      "Trained batch 2291 batch loss 2.13064766 epoch total loss 2.67209721\n",
      "Trained batch 2292 batch loss 2.49403834 epoch total loss 2.67201972\n",
      "Trained batch 2293 batch loss 2.20508981 epoch total loss 2.67181611\n",
      "Trained batch 2294 batch loss 2.51606131 epoch total loss 2.67174816\n",
      "Trained batch 2295 batch loss 2.46692228 epoch total loss 2.67165875\n",
      "Trained batch 2296 batch loss 2.70442843 epoch total loss 2.6716733\n",
      "Trained batch 2297 batch loss 2.75773907 epoch total loss 2.67171073\n",
      "Trained batch 2298 batch loss 2.87402439 epoch total loss 2.67179871\n",
      "Trained batch 2299 batch loss 2.7247839 epoch total loss 2.67182159\n",
      "Trained batch 2300 batch loss 2.94798326 epoch total loss 2.67194176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2301 batch loss 2.72770262 epoch total loss 2.67196584\n",
      "Trained batch 2302 batch loss 2.76967192 epoch total loss 2.67200828\n",
      "Trained batch 2303 batch loss 2.76771927 epoch total loss 2.67204976\n",
      "Trained batch 2304 batch loss 2.62235236 epoch total loss 2.6720283\n",
      "Trained batch 2305 batch loss 2.51813483 epoch total loss 2.67196155\n",
      "Trained batch 2306 batch loss 2.50986505 epoch total loss 2.67189097\n",
      "Trained batch 2307 batch loss 2.47607422 epoch total loss 2.6718061\n",
      "Trained batch 2308 batch loss 2.61717129 epoch total loss 2.67178249\n",
      "Trained batch 2309 batch loss 2.48361254 epoch total loss 2.67170095\n",
      "Trained batch 2310 batch loss 2.43879271 epoch total loss 2.6716\n",
      "Trained batch 2311 batch loss 2.40767527 epoch total loss 2.67148614\n",
      "Trained batch 2312 batch loss 2.46422791 epoch total loss 2.67139649\n",
      "Trained batch 2313 batch loss 2.43417978 epoch total loss 2.67129374\n",
      "Trained batch 2314 batch loss 2.39213824 epoch total loss 2.6711731\n",
      "Trained batch 2315 batch loss 2.77180505 epoch total loss 2.67121673\n",
      "Trained batch 2316 batch loss 2.82108688 epoch total loss 2.67128158\n",
      "Trained batch 2317 batch loss 2.84750319 epoch total loss 2.67135763\n",
      "Trained batch 2318 batch loss 2.75072455 epoch total loss 2.67139173\n",
      "Trained batch 2319 batch loss 2.79273987 epoch total loss 2.67144418\n",
      "Trained batch 2320 batch loss 2.66571665 epoch total loss 2.67144156\n",
      "Trained batch 2321 batch loss 2.78101301 epoch total loss 2.671489\n",
      "Trained batch 2322 batch loss 2.81797457 epoch total loss 2.67155194\n",
      "Trained batch 2323 batch loss 2.60366464 epoch total loss 2.67152262\n",
      "Trained batch 2324 batch loss 2.8516345 epoch total loss 2.6716\n",
      "Trained batch 2325 batch loss 2.51079273 epoch total loss 2.67153096\n",
      "Trained batch 2326 batch loss 2.54428697 epoch total loss 2.67147636\n",
      "Trained batch 2327 batch loss 2.51135564 epoch total loss 2.67140746\n",
      "Trained batch 2328 batch loss 2.23287201 epoch total loss 2.67121911\n",
      "Trained batch 2329 batch loss 2.12114191 epoch total loss 2.67098284\n",
      "Trained batch 2330 batch loss 2.40418673 epoch total loss 2.6708684\n",
      "Trained batch 2331 batch loss 2.39143109 epoch total loss 2.67074871\n",
      "Trained batch 2332 batch loss 2.74098659 epoch total loss 2.67077899\n",
      "Trained batch 2333 batch loss 2.34068513 epoch total loss 2.67063737\n",
      "Trained batch 2334 batch loss 2.85143113 epoch total loss 2.67071486\n",
      "Trained batch 2335 batch loss 2.87028074 epoch total loss 2.67080045\n",
      "Trained batch 2336 batch loss 2.61054659 epoch total loss 2.67077446\n",
      "Trained batch 2337 batch loss 2.5676322 epoch total loss 2.67073035\n",
      "Trained batch 2338 batch loss 2.65101075 epoch total loss 2.67072201\n",
      "Trained batch 2339 batch loss 2.50094056 epoch total loss 2.67064929\n",
      "Trained batch 2340 batch loss 2.45946503 epoch total loss 2.67055917\n",
      "Trained batch 2341 batch loss 2.54618168 epoch total loss 2.670506\n",
      "Trained batch 2342 batch loss 2.65390849 epoch total loss 2.67049909\n",
      "Trained batch 2343 batch loss 2.66485429 epoch total loss 2.6704967\n",
      "Trained batch 2344 batch loss 2.73587561 epoch total loss 2.6705246\n",
      "Trained batch 2345 batch loss 2.25465703 epoch total loss 2.67034721\n",
      "Trained batch 2346 batch loss 2.24423456 epoch total loss 2.67016554\n",
      "Trained batch 2347 batch loss 2.2357502 epoch total loss 2.66998053\n",
      "Trained batch 2348 batch loss 2.25779271 epoch total loss 2.66980505\n",
      "Trained batch 2349 batch loss 2.86227202 epoch total loss 2.66988707\n",
      "Trained batch 2350 batch loss 2.80876088 epoch total loss 2.66994596\n",
      "Trained batch 2351 batch loss 2.42492199 epoch total loss 2.66984177\n",
      "Trained batch 2352 batch loss 2.52281332 epoch total loss 2.6697793\n",
      "Trained batch 2353 batch loss 2.68615031 epoch total loss 2.66978621\n",
      "Trained batch 2354 batch loss 2.48106 epoch total loss 2.66970587\n",
      "Trained batch 2355 batch loss 2.78726935 epoch total loss 2.6697557\n",
      "Trained batch 2356 batch loss 2.90187836 epoch total loss 2.6698544\n",
      "Trained batch 2357 batch loss 2.40845299 epoch total loss 2.66974354\n",
      "Trained batch 2358 batch loss 2.13172221 epoch total loss 2.66951537\n",
      "Trained batch 2359 batch loss 2.04901123 epoch total loss 2.66925216\n",
      "Trained batch 2360 batch loss 2.82477975 epoch total loss 2.6693182\n",
      "Trained batch 2361 batch loss 2.77464175 epoch total loss 2.66936255\n",
      "Trained batch 2362 batch loss 2.88266516 epoch total loss 2.66945314\n",
      "Trained batch 2363 batch loss 3.08477902 epoch total loss 2.66962886\n",
      "Trained batch 2364 batch loss 2.89379764 epoch total loss 2.66972351\n",
      "Trained batch 2365 batch loss 2.54828739 epoch total loss 2.66967225\n",
      "Trained batch 2366 batch loss 2.39152074 epoch total loss 2.66955471\n",
      "Trained batch 2367 batch loss 2.40082026 epoch total loss 2.66944122\n",
      "Trained batch 2368 batch loss 2.23161602 epoch total loss 2.66925621\n",
      "Trained batch 2369 batch loss 2.35875225 epoch total loss 2.66912532\n",
      "Trained batch 2370 batch loss 2.2434 epoch total loss 2.66894555\n",
      "Trained batch 2371 batch loss 2.32059669 epoch total loss 2.66879869\n",
      "Trained batch 2372 batch loss 2.2687459 epoch total loss 2.66863\n",
      "Trained batch 2373 batch loss 2.27454758 epoch total loss 2.66846371\n",
      "Trained batch 2374 batch loss 2.1353848 epoch total loss 2.66823912\n",
      "Trained batch 2375 batch loss 2.2247982 epoch total loss 2.66805243\n",
      "Trained batch 2376 batch loss 2.15523577 epoch total loss 2.66783667\n",
      "Trained batch 2377 batch loss 2.15668058 epoch total loss 2.66762161\n",
      "Trained batch 2378 batch loss 2.27982068 epoch total loss 2.66745853\n",
      "Trained batch 2379 batch loss 2.44192219 epoch total loss 2.66736364\n",
      "Trained batch 2380 batch loss 2.50506139 epoch total loss 2.66729546\n",
      "Trained batch 2381 batch loss 2.67088175 epoch total loss 2.66729689\n",
      "Trained batch 2382 batch loss 2.43801308 epoch total loss 2.66720057\n",
      "Trained batch 2383 batch loss 2.44884086 epoch total loss 2.66710901\n",
      "Trained batch 2384 batch loss 2.82004929 epoch total loss 2.66717315\n",
      "Trained batch 2385 batch loss 2.60673761 epoch total loss 2.66714787\n",
      "Trained batch 2386 batch loss 2.56941628 epoch total loss 2.66710687\n",
      "Trained batch 2387 batch loss 2.60123515 epoch total loss 2.66707921\n",
      "Trained batch 2388 batch loss 2.74278712 epoch total loss 2.66711068\n",
      "Trained batch 2389 batch loss 2.54771686 epoch total loss 2.66706085\n",
      "Trained batch 2390 batch loss 2.54193068 epoch total loss 2.66700864\n",
      "Trained batch 2391 batch loss 2.60492802 epoch total loss 2.66698265\n",
      "Trained batch 2392 batch loss 2.64434481 epoch total loss 2.66697311\n",
      "Trained batch 2393 batch loss 2.77045393 epoch total loss 2.66701651\n",
      "Trained batch 2394 batch loss 2.6127398 epoch total loss 2.66699386\n",
      "Trained batch 2395 batch loss 2.68861604 epoch total loss 2.66700268\n",
      "Trained batch 2396 batch loss 2.71832442 epoch total loss 2.66702414\n",
      "Trained batch 2397 batch loss 2.25609207 epoch total loss 2.66685271\n",
      "Trained batch 2398 batch loss 2.15952229 epoch total loss 2.66664124\n",
      "Trained batch 2399 batch loss 2.39919519 epoch total loss 2.66652966\n",
      "Trained batch 2400 batch loss 2.33829927 epoch total loss 2.66639304\n",
      "Trained batch 2401 batch loss 2.32127881 epoch total loss 2.66624928\n",
      "Trained batch 2402 batch loss 2.06316471 epoch total loss 2.66599822\n",
      "Trained batch 2403 batch loss 2.68719196 epoch total loss 2.6660068\n",
      "Trained batch 2404 batch loss 3.13466692 epoch total loss 2.66620183\n",
      "Trained batch 2405 batch loss 2.49588776 epoch total loss 2.66613126\n",
      "Trained batch 2406 batch loss 2.86722279 epoch total loss 2.6662147\n",
      "Trained batch 2407 batch loss 2.74167657 epoch total loss 2.66624618\n",
      "Trained batch 2408 batch loss 2.80458093 epoch total loss 2.66630363\n",
      "Trained batch 2409 batch loss 3.50234985 epoch total loss 2.66665077\n",
      "Trained batch 2410 batch loss 3.06223345 epoch total loss 2.6668148\n",
      "Trained batch 2411 batch loss 3.18907022 epoch total loss 2.66703129\n",
      "Trained batch 2412 batch loss 2.92876244 epoch total loss 2.66713977\n",
      "Trained batch 2413 batch loss 2.73121977 epoch total loss 2.66716647\n",
      "Trained batch 2414 batch loss 2.51986051 epoch total loss 2.66710544\n",
      "Trained batch 2415 batch loss 2.8421433 epoch total loss 2.66717792\n",
      "Trained batch 2416 batch loss 2.73309422 epoch total loss 2.6672051\n",
      "Trained batch 2417 batch loss 2.4544 epoch total loss 2.66711712\n",
      "Trained batch 2418 batch loss 2.35969663 epoch total loss 2.66699028\n",
      "Trained batch 2419 batch loss 2.39635444 epoch total loss 2.66687846\n",
      "Trained batch 2420 batch loss 2.41638708 epoch total loss 2.66677499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2421 batch loss 2.45263577 epoch total loss 2.66668653\n",
      "Trained batch 2422 batch loss 2.56071758 epoch total loss 2.66664267\n",
      "Trained batch 2423 batch loss 2.74264526 epoch total loss 2.6666739\n",
      "Trained batch 2424 batch loss 2.76351309 epoch total loss 2.66671395\n",
      "Trained batch 2425 batch loss 2.42350769 epoch total loss 2.66661358\n",
      "Trained batch 2426 batch loss 2.80256438 epoch total loss 2.66666985\n",
      "Trained batch 2427 batch loss 2.55831456 epoch total loss 2.66662502\n",
      "Trained batch 2428 batch loss 2.72436428 epoch total loss 2.66664863\n",
      "Trained batch 2429 batch loss 2.63327551 epoch total loss 2.66663504\n",
      "Trained batch 2430 batch loss 2.50669718 epoch total loss 2.66656923\n",
      "Trained batch 2431 batch loss 2.68558311 epoch total loss 2.6665771\n",
      "Trained batch 2432 batch loss 2.64017558 epoch total loss 2.66656613\n",
      "Trained batch 2433 batch loss 2.44215 epoch total loss 2.6664741\n",
      "Trained batch 2434 batch loss 2.63948941 epoch total loss 2.6664629\n",
      "Trained batch 2435 batch loss 2.57849288 epoch total loss 2.6664269\n",
      "Trained batch 2436 batch loss 2.65764475 epoch total loss 2.66642332\n",
      "Trained batch 2437 batch loss 2.77219415 epoch total loss 2.66646671\n",
      "Trained batch 2438 batch loss 2.65939283 epoch total loss 2.66646361\n",
      "Trained batch 2439 batch loss 2.49449158 epoch total loss 2.66639328\n",
      "Trained batch 2440 batch loss 2.29653883 epoch total loss 2.66624165\n",
      "Trained batch 2441 batch loss 2.50254774 epoch total loss 2.66617441\n",
      "Trained batch 2442 batch loss 2.33392811 epoch total loss 2.66603851\n",
      "Trained batch 2443 batch loss 2.41365695 epoch total loss 2.66593504\n",
      "Trained batch 2444 batch loss 2.17859983 epoch total loss 2.66573572\n",
      "Trained batch 2445 batch loss 2.28322315 epoch total loss 2.66557932\n",
      "Trained batch 2446 batch loss 2.4392817 epoch total loss 2.66548681\n",
      "Trained batch 2447 batch loss 2.26088691 epoch total loss 2.66532135\n",
      "Trained batch 2448 batch loss 2.47293925 epoch total loss 2.66524291\n",
      "Trained batch 2449 batch loss 2.07443666 epoch total loss 2.66500163\n",
      "Trained batch 2450 batch loss 2.52891088 epoch total loss 2.66494608\n",
      "Trained batch 2451 batch loss 2.35505152 epoch total loss 2.66481948\n",
      "Trained batch 2452 batch loss 2.54561281 epoch total loss 2.66477084\n",
      "Trained batch 2453 batch loss 2.44359565 epoch total loss 2.66468048\n",
      "Trained batch 2454 batch loss 2.50641489 epoch total loss 2.66461611\n",
      "Trained batch 2455 batch loss 2.53973341 epoch total loss 2.66456509\n",
      "Trained batch 2456 batch loss 2.2743969 epoch total loss 2.6644063\n",
      "Trained batch 2457 batch loss 2.18981 epoch total loss 2.66421318\n",
      "Trained batch 2458 batch loss 2.31797838 epoch total loss 2.66407228\n",
      "Trained batch 2459 batch loss 2.73850441 epoch total loss 2.66410232\n",
      "Trained batch 2460 batch loss 2.59665823 epoch total loss 2.6640749\n",
      "Trained batch 2461 batch loss 2.55672574 epoch total loss 2.66403127\n",
      "Trained batch 2462 batch loss 2.69610023 epoch total loss 2.66404438\n",
      "Trained batch 2463 batch loss 2.70613432 epoch total loss 2.66406155\n",
      "Trained batch 2464 batch loss 2.67050648 epoch total loss 2.66406417\n",
      "Trained batch 2465 batch loss 2.66965818 epoch total loss 2.66406631\n",
      "Trained batch 2466 batch loss 2.89763856 epoch total loss 2.66416097\n",
      "Trained batch 2467 batch loss 2.85537434 epoch total loss 2.66423845\n",
      "Trained batch 2468 batch loss 2.71488214 epoch total loss 2.66425896\n",
      "Trained batch 2469 batch loss 2.55950928 epoch total loss 2.66421652\n",
      "Trained batch 2470 batch loss 2.65658855 epoch total loss 2.66421342\n",
      "Trained batch 2471 batch loss 2.66239691 epoch total loss 2.66421294\n",
      "Trained batch 2472 batch loss 2.37578511 epoch total loss 2.66409636\n",
      "Trained batch 2473 batch loss 2.65966725 epoch total loss 2.66409445\n",
      "Trained batch 2474 batch loss 2.58588719 epoch total loss 2.66406298\n",
      "Trained batch 2475 batch loss 2.56508279 epoch total loss 2.66402292\n",
      "Trained batch 2476 batch loss 2.54614139 epoch total loss 2.66397524\n",
      "Trained batch 2477 batch loss 2.4279902 epoch total loss 2.66387987\n",
      "Trained batch 2478 batch loss 2.25646639 epoch total loss 2.6637156\n",
      "Trained batch 2479 batch loss 2.65296173 epoch total loss 2.66371107\n",
      "Trained batch 2480 batch loss 2.7726655 epoch total loss 2.66375494\n",
      "Trained batch 2481 batch loss 2.70215034 epoch total loss 2.66377044\n",
      "Trained batch 2482 batch loss 3.04034233 epoch total loss 2.66392231\n",
      "Trained batch 2483 batch loss 2.5116241 epoch total loss 2.66386104\n",
      "Trained batch 2484 batch loss 2.7127552 epoch total loss 2.66388059\n",
      "Trained batch 2485 batch loss 2.34260178 epoch total loss 2.66375136\n",
      "Trained batch 2486 batch loss 2.77370548 epoch total loss 2.66379571\n",
      "Trained batch 2487 batch loss 2.64814472 epoch total loss 2.66378951\n",
      "Trained batch 2488 batch loss 2.61046028 epoch total loss 2.66376781\n",
      "Trained batch 2489 batch loss 2.49958014 epoch total loss 2.66370201\n",
      "Trained batch 2490 batch loss 2.69465923 epoch total loss 2.66371441\n",
      "Trained batch 2491 batch loss 2.70408487 epoch total loss 2.66373062\n",
      "Trained batch 2492 batch loss 2.42861462 epoch total loss 2.66363645\n",
      "Trained batch 2493 batch loss 2.63508463 epoch total loss 2.663625\n",
      "Trained batch 2494 batch loss 2.83028936 epoch total loss 2.66369176\n",
      "Trained batch 2495 batch loss 2.80450678 epoch total loss 2.66374826\n",
      "Trained batch 2496 batch loss 2.92039323 epoch total loss 2.66385102\n",
      "Trained batch 2497 batch loss 3.08769631 epoch total loss 2.66402078\n",
      "Trained batch 2498 batch loss 2.57433176 epoch total loss 2.66398501\n",
      "Trained batch 2499 batch loss 2.23373532 epoch total loss 2.66381288\n",
      "Trained batch 2500 batch loss 2.42114472 epoch total loss 2.66371584\n",
      "Trained batch 2501 batch loss 2.97473526 epoch total loss 2.66384\n",
      "Trained batch 2502 batch loss 3.01930475 epoch total loss 2.66398239\n",
      "Trained batch 2503 batch loss 2.72746277 epoch total loss 2.66400766\n",
      "Trained batch 2504 batch loss 2.49512744 epoch total loss 2.66394019\n",
      "Trained batch 2505 batch loss 2.41066766 epoch total loss 2.6638391\n",
      "Trained batch 2506 batch loss 2.92947984 epoch total loss 2.6639452\n",
      "Trained batch 2507 batch loss 2.74411964 epoch total loss 2.66397715\n",
      "Trained batch 2508 batch loss 2.41767144 epoch total loss 2.66387892\n",
      "Trained batch 2509 batch loss 2.66380548 epoch total loss 2.66387868\n",
      "Trained batch 2510 batch loss 2.66396093 epoch total loss 2.66387892\n",
      "Trained batch 2511 batch loss 2.61411929 epoch total loss 2.66385913\n",
      "Trained batch 2512 batch loss 2.60198379 epoch total loss 2.66383457\n",
      "Trained batch 2513 batch loss 2.60917425 epoch total loss 2.66381288\n",
      "Trained batch 2514 batch loss 2.76643 epoch total loss 2.66385365\n",
      "Trained batch 2515 batch loss 2.37048435 epoch total loss 2.66373706\n",
      "Trained batch 2516 batch loss 3.14719725 epoch total loss 2.66392922\n",
      "Trained batch 2517 batch loss 2.74867177 epoch total loss 2.66396284\n",
      "Trained batch 2518 batch loss 2.63907409 epoch total loss 2.66395283\n",
      "Trained batch 2519 batch loss 2.65442753 epoch total loss 2.66394901\n",
      "Trained batch 2520 batch loss 2.32416725 epoch total loss 2.66381431\n",
      "Trained batch 2521 batch loss 2.34234667 epoch total loss 2.66368675\n",
      "Trained batch 2522 batch loss 2.19054723 epoch total loss 2.66349912\n",
      "Trained batch 2523 batch loss 2.75339723 epoch total loss 2.66353464\n",
      "Trained batch 2524 batch loss 2.50912905 epoch total loss 2.66347361\n",
      "Trained batch 2525 batch loss 2.81982541 epoch total loss 2.66353559\n",
      "Trained batch 2526 batch loss 2.87581325 epoch total loss 2.66361976\n",
      "Trained batch 2527 batch loss 2.90502977 epoch total loss 2.66371536\n",
      "Trained batch 2528 batch loss 3.10966921 epoch total loss 2.66389179\n",
      "Trained batch 2529 batch loss 3.15457368 epoch total loss 2.66408587\n",
      "Trained batch 2530 batch loss 3.07018042 epoch total loss 2.66424632\n",
      "Trained batch 2531 batch loss 2.91300035 epoch total loss 2.66434479\n",
      "Trained batch 2532 batch loss 2.46876717 epoch total loss 2.66426754\n",
      "Trained batch 2533 batch loss 3.22746062 epoch total loss 2.66449\n",
      "Trained batch 2534 batch loss 2.61279416 epoch total loss 2.66446948\n",
      "Trained batch 2535 batch loss 2.56637239 epoch total loss 2.66443086\n",
      "Trained batch 2536 batch loss 2.59626389 epoch total loss 2.66440392\n",
      "Trained batch 2537 batch loss 2.54387522 epoch total loss 2.66435647\n",
      "Trained batch 2538 batch loss 2.74419355 epoch total loss 2.66438794\n",
      "Trained batch 2539 batch loss 2.15357709 epoch total loss 2.66418672\n",
      "Trained batch 2540 batch loss 2.42445588 epoch total loss 2.6640923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2541 batch loss 2.62049294 epoch total loss 2.66407514\n",
      "Trained batch 2542 batch loss 2.673383 epoch total loss 2.66407871\n",
      "Trained batch 2543 batch loss 2.76091576 epoch total loss 2.66411686\n",
      "Trained batch 2544 batch loss 2.7528162 epoch total loss 2.66415167\n",
      "Trained batch 2545 batch loss 2.85152078 epoch total loss 2.66422534\n",
      "Trained batch 2546 batch loss 2.83721042 epoch total loss 2.66429329\n",
      "Trained batch 2547 batch loss 2.7220397 epoch total loss 2.66431618\n",
      "Trained batch 2548 batch loss 2.32347059 epoch total loss 2.66418219\n",
      "Trained batch 2549 batch loss 2.50522852 epoch total loss 2.66412\n",
      "Trained batch 2550 batch loss 2.37191415 epoch total loss 2.66400552\n",
      "Trained batch 2551 batch loss 2.31892061 epoch total loss 2.66387\n",
      "Trained batch 2552 batch loss 2.51602602 epoch total loss 2.66381216\n",
      "Trained batch 2553 batch loss 2.39919519 epoch total loss 2.66370869\n",
      "Trained batch 2554 batch loss 2.28627658 epoch total loss 2.66356087\n",
      "Trained batch 2555 batch loss 2.23555613 epoch total loss 2.66339326\n",
      "Trained batch 2556 batch loss 2.34198 epoch total loss 2.66326737\n",
      "Trained batch 2557 batch loss 2.48428726 epoch total loss 2.66319752\n",
      "Trained batch 2558 batch loss 2.41232705 epoch total loss 2.66309929\n",
      "Trained batch 2559 batch loss 2.37240911 epoch total loss 2.6629858\n",
      "Trained batch 2560 batch loss 2.20962143 epoch total loss 2.66280866\n",
      "Trained batch 2561 batch loss 2.27672529 epoch total loss 2.66265798\n",
      "Trained batch 2562 batch loss 2.49437141 epoch total loss 2.66259217\n",
      "Trained batch 2563 batch loss 2.57420325 epoch total loss 2.6625576\n",
      "Trained batch 2564 batch loss 2.66808128 epoch total loss 2.66255975\n",
      "Trained batch 2565 batch loss 2.46703267 epoch total loss 2.66248345\n",
      "Trained batch 2566 batch loss 2.63053513 epoch total loss 2.66247082\n",
      "Trained batch 2567 batch loss 2.67287636 epoch total loss 2.66247487\n",
      "Trained batch 2568 batch loss 2.81769943 epoch total loss 2.66253543\n",
      "Trained batch 2569 batch loss 2.60422277 epoch total loss 2.66251278\n",
      "Trained batch 2570 batch loss 2.65029573 epoch total loss 2.66250801\n",
      "Trained batch 2571 batch loss 2.41561747 epoch total loss 2.66241193\n",
      "Trained batch 2572 batch loss 2.82971 epoch total loss 2.66247702\n",
      "Trained batch 2573 batch loss 3.01695228 epoch total loss 2.66261482\n",
      "Trained batch 2574 batch loss 3.10338974 epoch total loss 2.66278601\n",
      "Trained batch 2575 batch loss 2.7841053 epoch total loss 2.66283321\n",
      "Trained batch 2576 batch loss 2.84093904 epoch total loss 2.66290236\n",
      "Trained batch 2577 batch loss 2.71989107 epoch total loss 2.66292429\n",
      "Trained batch 2578 batch loss 2.67532468 epoch total loss 2.66292906\n",
      "Trained batch 2579 batch loss 2.62659955 epoch total loss 2.66291499\n",
      "Trained batch 2580 batch loss 2.5939889 epoch total loss 2.66288805\n",
      "Trained batch 2581 batch loss 2.52737284 epoch total loss 2.6628356\n",
      "Trained batch 2582 batch loss 2.47577047 epoch total loss 2.66276312\n",
      "Trained batch 2583 batch loss 2.485461 epoch total loss 2.66269445\n",
      "Trained batch 2584 batch loss 2.44334936 epoch total loss 2.66260958\n",
      "Trained batch 2585 batch loss 2.52483 epoch total loss 2.66255617\n",
      "Trained batch 2586 batch loss 3.08534265 epoch total loss 2.66271973\n",
      "Trained batch 2587 batch loss 2.8630631 epoch total loss 2.66279745\n",
      "Trained batch 2588 batch loss 2.72785187 epoch total loss 2.66282248\n",
      "Trained batch 2589 batch loss 2.95330524 epoch total loss 2.66293478\n",
      "Trained batch 2590 batch loss 3.1799221 epoch total loss 2.6631341\n",
      "Trained batch 2591 batch loss 3.29263401 epoch total loss 2.66337705\n",
      "Trained batch 2592 batch loss 3.06316352 epoch total loss 2.6635313\n",
      "Trained batch 2593 batch loss 2.6501503 epoch total loss 2.6635263\n",
      "Trained batch 2594 batch loss 2.44029188 epoch total loss 2.66344023\n",
      "Trained batch 2595 batch loss 2.71948957 epoch total loss 2.66346192\n",
      "Trained batch 2596 batch loss 3.09454298 epoch total loss 2.6636281\n",
      "Trained batch 2597 batch loss 2.77959967 epoch total loss 2.66367269\n",
      "Trained batch 2598 batch loss 2.89092088 epoch total loss 2.66376019\n",
      "Trained batch 2599 batch loss 2.71027279 epoch total loss 2.66377831\n",
      "Trained batch 2600 batch loss 2.60103631 epoch total loss 2.66375422\n",
      "Trained batch 2601 batch loss 2.96509337 epoch total loss 2.66387\n",
      "Trained batch 2602 batch loss 2.9265852 epoch total loss 2.66397119\n",
      "Trained batch 2603 batch loss 2.87893963 epoch total loss 2.66405368\n",
      "Trained batch 2604 batch loss 2.6243155 epoch total loss 2.66403842\n",
      "Trained batch 2605 batch loss 2.53885102 epoch total loss 2.6639905\n",
      "Trained batch 2606 batch loss 2.36156631 epoch total loss 2.66387439\n",
      "Trained batch 2607 batch loss 2.14691138 epoch total loss 2.66367602\n",
      "Trained batch 2608 batch loss 2.95717382 epoch total loss 2.66378856\n",
      "Trained batch 2609 batch loss 3.21857548 epoch total loss 2.66400123\n",
      "Trained batch 2610 batch loss 2.80698061 epoch total loss 2.66405606\n",
      "Trained batch 2611 batch loss 2.87259984 epoch total loss 2.66413593\n",
      "Trained batch 2612 batch loss 2.77517509 epoch total loss 2.66417861\n",
      "Trained batch 2613 batch loss 2.60613203 epoch total loss 2.6641562\n",
      "Trained batch 2614 batch loss 2.6204226 epoch total loss 2.66413975\n",
      "Trained batch 2615 batch loss 3.29926872 epoch total loss 2.66438246\n",
      "Trained batch 2616 batch loss 2.68601346 epoch total loss 2.6643908\n",
      "Trained batch 2617 batch loss 2.92273 epoch total loss 2.66448951\n",
      "Trained batch 2618 batch loss 2.87084246 epoch total loss 2.66456842\n",
      "Trained batch 2619 batch loss 2.89651608 epoch total loss 2.66465688\n",
      "Trained batch 2620 batch loss 2.45233703 epoch total loss 2.66457582\n",
      "Trained batch 2621 batch loss 2.47740388 epoch total loss 2.66450429\n",
      "Trained batch 2622 batch loss 2.45594692 epoch total loss 2.6644249\n",
      "Trained batch 2623 batch loss 2.61826706 epoch total loss 2.66440725\n",
      "Trained batch 2624 batch loss 2.42524147 epoch total loss 2.66431618\n",
      "Trained batch 2625 batch loss 2.65969491 epoch total loss 2.66431427\n",
      "Trained batch 2626 batch loss 2.65193629 epoch total loss 2.6643095\n",
      "Trained batch 2627 batch loss 2.57243156 epoch total loss 2.66427469\n",
      "Trained batch 2628 batch loss 2.63507962 epoch total loss 2.66426349\n",
      "Trained batch 2629 batch loss 2.79631019 epoch total loss 2.66431379\n",
      "Trained batch 2630 batch loss 2.68456221 epoch total loss 2.66432142\n",
      "Trained batch 2631 batch loss 2.7783289 epoch total loss 2.66436481\n",
      "Trained batch 2632 batch loss 2.78809428 epoch total loss 2.66441178\n",
      "Trained batch 2633 batch loss 2.79839921 epoch total loss 2.6644628\n",
      "Trained batch 2634 batch loss 2.8313179 epoch total loss 2.66452622\n",
      "Trained batch 2635 batch loss 2.69544172 epoch total loss 2.66453791\n",
      "Trained batch 2636 batch loss 2.71040535 epoch total loss 2.66455531\n",
      "Trained batch 2637 batch loss 2.65183234 epoch total loss 2.6645503\n",
      "Trained batch 2638 batch loss 3.06155705 epoch total loss 2.66470098\n",
      "Trained batch 2639 batch loss 2.44439745 epoch total loss 2.6646173\n",
      "Trained batch 2640 batch loss 2.5916543 epoch total loss 2.66459\n",
      "Trained batch 2641 batch loss 2.32446098 epoch total loss 2.6644609\n",
      "Trained batch 2642 batch loss 2.315516 epoch total loss 2.66432881\n",
      "Trained batch 2643 batch loss 2.33098412 epoch total loss 2.66420269\n",
      "Trained batch 2644 batch loss 2.4886241 epoch total loss 2.66413641\n",
      "Trained batch 2645 batch loss 2.61884928 epoch total loss 2.66411924\n",
      "Trained batch 2646 batch loss 2.75270224 epoch total loss 2.66415262\n",
      "Trained batch 2647 batch loss 2.79653692 epoch total loss 2.66420269\n",
      "Trained batch 2648 batch loss 2.52918196 epoch total loss 2.66415167\n",
      "Trained batch 2649 batch loss 2.48958254 epoch total loss 2.66408587\n",
      "Trained batch 2650 batch loss 2.43995333 epoch total loss 2.66400123\n",
      "Trained batch 2651 batch loss 2.63631296 epoch total loss 2.66399074\n",
      "Trained batch 2652 batch loss 2.63639688 epoch total loss 2.66398048\n",
      "Trained batch 2653 batch loss 2.25510693 epoch total loss 2.66382623\n",
      "Trained batch 2654 batch loss 2.47847915 epoch total loss 2.66375637\n",
      "Trained batch 2655 batch loss 2.76192713 epoch total loss 2.66379333\n",
      "Trained batch 2656 batch loss 2.37935686 epoch total loss 2.66368628\n",
      "Trained batch 2657 batch loss 2.89388633 epoch total loss 2.66377282\n",
      "Trained batch 2658 batch loss 3.00439167 epoch total loss 2.66390109\n",
      "Trained batch 2659 batch loss 2.6326623 epoch total loss 2.66388941\n",
      "Trained batch 2660 batch loss 2.92207146 epoch total loss 2.66398621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2661 batch loss 2.60494447 epoch total loss 2.66396403\n",
      "Trained batch 2662 batch loss 2.47833014 epoch total loss 2.66389441\n",
      "Trained batch 2663 batch loss 2.56348658 epoch total loss 2.66385674\n",
      "Trained batch 2664 batch loss 2.72901917 epoch total loss 2.6638813\n",
      "Trained batch 2665 batch loss 2.64466381 epoch total loss 2.66387391\n",
      "Trained batch 2666 batch loss 2.76083088 epoch total loss 2.66391039\n",
      "Trained batch 2667 batch loss 2.57398558 epoch total loss 2.66387677\n",
      "Trained batch 2668 batch loss 2.66206694 epoch total loss 2.66387606\n",
      "Trained batch 2669 batch loss 2.43698788 epoch total loss 2.66379094\n",
      "Trained batch 2670 batch loss 2.76095533 epoch total loss 2.66382742\n",
      "Trained batch 2671 batch loss 2.8526032 epoch total loss 2.66389799\n",
      "Trained batch 2672 batch loss 2.84223437 epoch total loss 2.66396475\n",
      "Trained batch 2673 batch loss 2.72462821 epoch total loss 2.6639874\n",
      "Trained batch 2674 batch loss 2.61071062 epoch total loss 2.66396761\n",
      "Trained batch 2675 batch loss 2.44904137 epoch total loss 2.66388726\n",
      "Trained batch 2676 batch loss 2.68002343 epoch total loss 2.66389346\n",
      "Trained batch 2677 batch loss 2.93721581 epoch total loss 2.66399527\n",
      "Trained batch 2678 batch loss 2.83133245 epoch total loss 2.66405797\n",
      "Trained batch 2679 batch loss 2.72692537 epoch total loss 2.66408157\n",
      "Trained batch 2680 batch loss 2.39023542 epoch total loss 2.66397929\n",
      "Trained batch 2681 batch loss 2.50537539 epoch total loss 2.66392016\n",
      "Trained batch 2682 batch loss 2.39782906 epoch total loss 2.66382098\n",
      "Trained batch 2683 batch loss 2.82266617 epoch total loss 2.66388\n",
      "Trained batch 2684 batch loss 2.49009943 epoch total loss 2.6638155\n",
      "Trained batch 2685 batch loss 2.60679054 epoch total loss 2.66379428\n",
      "Trained batch 2686 batch loss 2.92021203 epoch total loss 2.66389\n",
      "Trained batch 2687 batch loss 2.50966287 epoch total loss 2.66383243\n",
      "Trained batch 2688 batch loss 2.48651266 epoch total loss 2.66376638\n",
      "Trained batch 2689 batch loss 2.43418741 epoch total loss 2.66368103\n",
      "Trained batch 2690 batch loss 2.56663799 epoch total loss 2.66364479\n",
      "Trained batch 2691 batch loss 2.55389333 epoch total loss 2.66360402\n",
      "Trained batch 2692 batch loss 2.1568675 epoch total loss 2.66341567\n",
      "Trained batch 2693 batch loss 2.46747231 epoch total loss 2.66334295\n",
      "Trained batch 2694 batch loss 2.5087328 epoch total loss 2.66328549\n",
      "Trained batch 2695 batch loss 2.46955585 epoch total loss 2.66321373\n",
      "Trained batch 2696 batch loss 2.61082077 epoch total loss 2.66319418\n",
      "Trained batch 2697 batch loss 2.71693087 epoch total loss 2.66321421\n",
      "Trained batch 2698 batch loss 2.72612524 epoch total loss 2.66323733\n",
      "Trained batch 2699 batch loss 2.60136414 epoch total loss 2.66321468\n",
      "Trained batch 2700 batch loss 2.43251777 epoch total loss 2.66312909\n",
      "Trained batch 2701 batch loss 2.33929682 epoch total loss 2.66300941\n",
      "Trained batch 2702 batch loss 2.84068441 epoch total loss 2.66307521\n",
      "Trained batch 2703 batch loss 2.63405013 epoch total loss 2.66306448\n",
      "Trained batch 2704 batch loss 2.93393803 epoch total loss 2.66316462\n",
      "Trained batch 2705 batch loss 2.62813616 epoch total loss 2.66315174\n",
      "Trained batch 2706 batch loss 2.45486164 epoch total loss 2.66307473\n",
      "Trained batch 2707 batch loss 2.49247217 epoch total loss 2.66301179\n",
      "Trained batch 2708 batch loss 2.54000449 epoch total loss 2.66296649\n",
      "Trained batch 2709 batch loss 2.35372829 epoch total loss 2.66285205\n",
      "Trained batch 2710 batch loss 2.70467591 epoch total loss 2.66286755\n",
      "Trained batch 2711 batch loss 2.15873671 epoch total loss 2.66268158\n",
      "Trained batch 2712 batch loss 2.87372637 epoch total loss 2.6627593\n",
      "Trained batch 2713 batch loss 2.62731552 epoch total loss 2.66274643\n",
      "Trained batch 2714 batch loss 2.23030281 epoch total loss 2.66258693\n",
      "Trained batch 2715 batch loss 2.42792225 epoch total loss 2.66250062\n",
      "Trained batch 2716 batch loss 2.47126842 epoch total loss 2.66243\n",
      "Trained batch 2717 batch loss 2.35323381 epoch total loss 2.66231632\n",
      "Trained batch 2718 batch loss 2.36620331 epoch total loss 2.66220737\n",
      "Trained batch 2719 batch loss 2.583076 epoch total loss 2.66217804\n",
      "Trained batch 2720 batch loss 2.89497757 epoch total loss 2.66226387\n",
      "Trained batch 2721 batch loss 2.90134883 epoch total loss 2.66235161\n",
      "Trained batch 2722 batch loss 3.06015778 epoch total loss 2.66249776\n",
      "Trained batch 2723 batch loss 3.07706308 epoch total loss 2.66265\n",
      "Trained batch 2724 batch loss 2.8013494 epoch total loss 2.66270089\n",
      "Trained batch 2725 batch loss 2.8865509 epoch total loss 2.66278315\n",
      "Trained batch 2726 batch loss 2.81020975 epoch total loss 2.66283703\n",
      "Trained batch 2727 batch loss 2.85236335 epoch total loss 2.66290665\n",
      "Trained batch 2728 batch loss 2.59316301 epoch total loss 2.66288114\n",
      "Trained batch 2729 batch loss 2.37963057 epoch total loss 2.66277719\n",
      "Trained batch 2730 batch loss 2.56476641 epoch total loss 2.66274142\n",
      "Trained batch 2731 batch loss 2.27626705 epoch total loss 2.6626\n",
      "Trained batch 2732 batch loss 2.46215296 epoch total loss 2.66252661\n",
      "Trained batch 2733 batch loss 2.03988433 epoch total loss 2.66229868\n",
      "Trained batch 2734 batch loss 1.91145205 epoch total loss 2.66202426\n",
      "Trained batch 2735 batch loss 1.97185957 epoch total loss 2.66177177\n",
      "Trained batch 2736 batch loss 1.92662239 epoch total loss 2.66150308\n",
      "Trained batch 2737 batch loss 2.21253824 epoch total loss 2.66133904\n",
      "Trained batch 2738 batch loss 2.70445967 epoch total loss 2.66135478\n",
      "Trained batch 2739 batch loss 2.57514167 epoch total loss 2.66132331\n",
      "Trained batch 2740 batch loss 2.58766794 epoch total loss 2.66129661\n",
      "Trained batch 2741 batch loss 2.7790339 epoch total loss 2.66133952\n",
      "Trained batch 2742 batch loss 2.82611489 epoch total loss 2.6613996\n",
      "Trained batch 2743 batch loss 2.89812756 epoch total loss 2.66148591\n",
      "Trained batch 2744 batch loss 2.92565155 epoch total loss 2.66158223\n",
      "Trained batch 2745 batch loss 2.64228916 epoch total loss 2.66157508\n",
      "Trained batch 2746 batch loss 2.47651672 epoch total loss 2.66150761\n",
      "Trained batch 2747 batch loss 2.49298525 epoch total loss 2.66144633\n",
      "Trained batch 2748 batch loss 2.85335183 epoch total loss 2.66151619\n",
      "Trained batch 2749 batch loss 2.81226802 epoch total loss 2.66157126\n",
      "Trained batch 2750 batch loss 2.67754459 epoch total loss 2.66157699\n",
      "Trained batch 2751 batch loss 2.77537942 epoch total loss 2.66161847\n",
      "Trained batch 2752 batch loss 2.53257346 epoch total loss 2.6615715\n",
      "Trained batch 2753 batch loss 2.47441792 epoch total loss 2.66150379\n",
      "Trained batch 2754 batch loss 2.50570083 epoch total loss 2.66144705\n",
      "Trained batch 2755 batch loss 2.42322898 epoch total loss 2.66136074\n",
      "Trained batch 2756 batch loss 2.79802036 epoch total loss 2.66141033\n",
      "Trained batch 2757 batch loss 2.90882397 epoch total loss 2.6615\n",
      "Trained batch 2758 batch loss 2.92212105 epoch total loss 2.66159463\n",
      "Trained batch 2759 batch loss 3.06515932 epoch total loss 2.66174078\n",
      "Trained batch 2760 batch loss 2.81921506 epoch total loss 2.66179776\n",
      "Trained batch 2761 batch loss 2.54760742 epoch total loss 2.66175628\n",
      "Trained batch 2762 batch loss 2.51394272 epoch total loss 2.66170287\n",
      "Trained batch 2763 batch loss 2.63466477 epoch total loss 2.6616931\n",
      "Trained batch 2764 batch loss 2.40840912 epoch total loss 2.66160154\n",
      "Trained batch 2765 batch loss 2.73221111 epoch total loss 2.66162705\n",
      "Trained batch 2766 batch loss 2.61604881 epoch total loss 2.6616106\n",
      "Trained batch 2767 batch loss 2.67693305 epoch total loss 2.66161609\n",
      "Trained batch 2768 batch loss 2.55098963 epoch total loss 2.66157603\n",
      "Trained batch 2769 batch loss 2.47238827 epoch total loss 2.66150761\n",
      "Trained batch 2770 batch loss 2.7503891 epoch total loss 2.66153979\n",
      "Trained batch 2771 batch loss 2.95213461 epoch total loss 2.6616447\n",
      "Trained batch 2772 batch loss 2.89200974 epoch total loss 2.66172791\n",
      "Trained batch 2773 batch loss 2.89422131 epoch total loss 2.66181159\n",
      "Trained batch 2774 batch loss 3.09271979 epoch total loss 2.66196704\n",
      "Trained batch 2775 batch loss 2.60954237 epoch total loss 2.66194797\n",
      "Trained batch 2776 batch loss 2.4787178 epoch total loss 2.66188192\n",
      "Trained batch 2777 batch loss 2.24368238 epoch total loss 2.66173124\n",
      "Trained batch 2778 batch loss 2.34967136 epoch total loss 2.66161895\n",
      "Trained batch 2779 batch loss 2.49769258 epoch total loss 2.66156\n",
      "Trained batch 2780 batch loss 2.04758263 epoch total loss 2.66133904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2781 batch loss 2.39357018 epoch total loss 2.66124272\n",
      "Epoch 2 train loss 2.661242723464966\n",
      "Validated batch 1 batch loss 2.48303986\n",
      "Validated batch 2 batch loss 2.35830069\n",
      "Validated batch 3 batch loss 2.7211051\n",
      "Validated batch 4 batch loss 2.60170102\n",
      "Validated batch 5 batch loss 2.88181114\n",
      "Validated batch 6 batch loss 2.48031902\n",
      "Validated batch 7 batch loss 2.75808144\n",
      "Validated batch 8 batch loss 2.29706\n",
      "Validated batch 9 batch loss 2.69501686\n",
      "Validated batch 10 batch loss 2.83509135\n",
      "Validated batch 11 batch loss 3.02634811\n",
      "Validated batch 12 batch loss 2.68250537\n",
      "Validated batch 13 batch loss 2.16237617\n",
      "Validated batch 14 batch loss 2.55986357\n",
      "Validated batch 15 batch loss 2.62879276\n",
      "Validated batch 16 batch loss 2.76979876\n",
      "Validated batch 17 batch loss 2.59978151\n",
      "Validated batch 18 batch loss 2.45935035\n",
      "Validated batch 19 batch loss 2.80367947\n",
      "Validated batch 20 batch loss 2.78261137\n",
      "Validated batch 21 batch loss 2.63833261\n",
      "Validated batch 22 batch loss 2.52402353\n",
      "Validated batch 23 batch loss 2.5541234\n",
      "Validated batch 24 batch loss 1.999547\n",
      "Validated batch 25 batch loss 1.96085441\n",
      "Validated batch 26 batch loss 2.81360555\n",
      "Validated batch 27 batch loss 2.58622646\n",
      "Validated batch 28 batch loss 2.63042307\n",
      "Validated batch 29 batch loss 2.4311924\n",
      "Validated batch 30 batch loss 2.5372715\n",
      "Validated batch 31 batch loss 2.43533754\n",
      "Validated batch 32 batch loss 2.71051931\n",
      "Validated batch 33 batch loss 2.70587659\n",
      "Validated batch 34 batch loss 2.30397463\n",
      "Validated batch 35 batch loss 2.36223292\n",
      "Validated batch 36 batch loss 2.52381253\n",
      "Validated batch 37 batch loss 2.85234189\n",
      "Validated batch 38 batch loss 2.56390047\n",
      "Validated batch 39 batch loss 3.02591586\n",
      "Validated batch 40 batch loss 2.52486038\n",
      "Validated batch 41 batch loss 2.74198461\n",
      "Validated batch 42 batch loss 2.70737028\n",
      "Validated batch 43 batch loss 2.67883849\n",
      "Validated batch 44 batch loss 2.5368011\n",
      "Validated batch 45 batch loss 2.2221818\n",
      "Validated batch 46 batch loss 2.53845334\n",
      "Validated batch 47 batch loss 2.32344699\n",
      "Validated batch 48 batch loss 2.65551567\n",
      "Validated batch 49 batch loss 2.88539743\n",
      "Validated batch 50 batch loss 2.31731272\n",
      "Validated batch 51 batch loss 2.72339749\n",
      "Validated batch 52 batch loss 2.13601351\n",
      "Validated batch 53 batch loss 2.2374649\n",
      "Validated batch 54 batch loss 2.61339021\n",
      "Validated batch 55 batch loss 2.64801526\n",
      "Validated batch 56 batch loss 2.29940891\n",
      "Validated batch 57 batch loss 3.21292639\n",
      "Validated batch 58 batch loss 2.31563115\n",
      "Validated batch 59 batch loss 2.53250885\n",
      "Validated batch 60 batch loss 2.52115893\n",
      "Validated batch 61 batch loss 2.2733376\n",
      "Validated batch 62 batch loss 2.3360858\n",
      "Validated batch 63 batch loss 2.38569975\n",
      "Validated batch 64 batch loss 2.60913658\n",
      "Validated batch 65 batch loss 2.65865493\n",
      "Validated batch 66 batch loss 2.3741653\n",
      "Validated batch 67 batch loss 2.53382874\n",
      "Validated batch 68 batch loss 2.42436051\n",
      "Validated batch 69 batch loss 2.7169528\n",
      "Validated batch 70 batch loss 2.18033266\n",
      "Validated batch 71 batch loss 2.380481\n",
      "Validated batch 72 batch loss 2.69103861\n",
      "Validated batch 73 batch loss 2.43164587\n",
      "Validated batch 74 batch loss 2.58996725\n",
      "Validated batch 75 batch loss 2.74408054\n",
      "Validated batch 76 batch loss 2.7224822\n",
      "Validated batch 77 batch loss 2.89546394\n",
      "Validated batch 78 batch loss 2.6138916\n",
      "Validated batch 79 batch loss 2.52287769\n",
      "Validated batch 80 batch loss 2.54116869\n",
      "Validated batch 81 batch loss 2.92304945\n",
      "Validated batch 82 batch loss 2.60832047\n",
      "Validated batch 83 batch loss 2.32218838\n",
      "Validated batch 84 batch loss 2.21233\n",
      "Validated batch 85 batch loss 2.2716608\n",
      "Validated batch 86 batch loss 2.72722673\n",
      "Validated batch 87 batch loss 2.40055418\n",
      "Validated batch 88 batch loss 2.36797094\n",
      "Validated batch 89 batch loss 2.64634228\n",
      "Validated batch 90 batch loss 2.79383421\n",
      "Validated batch 91 batch loss 2.9844985\n",
      "Validated batch 92 batch loss 2.77056289\n",
      "Validated batch 93 batch loss 2.52846\n",
      "Validated batch 94 batch loss 2.56796932\n",
      "Validated batch 95 batch loss 2.92778516\n",
      "Validated batch 96 batch loss 2.60483503\n",
      "Validated batch 97 batch loss 2.51868725\n",
      "Validated batch 98 batch loss 2.33337736\n",
      "Validated batch 99 batch loss 2.69081354\n",
      "Validated batch 100 batch loss 2.31390834\n",
      "Validated batch 101 batch loss 2.51675344\n",
      "Validated batch 102 batch loss 2.6774857\n",
      "Validated batch 103 batch loss 2.46454525\n",
      "Validated batch 104 batch loss 2.82191491\n",
      "Validated batch 105 batch loss 2.56347775\n",
      "Validated batch 106 batch loss 2.70460939\n",
      "Validated batch 107 batch loss 2.31965566\n",
      "Validated batch 108 batch loss 2.61636162\n",
      "Validated batch 109 batch loss 2.44285226\n",
      "Validated batch 110 batch loss 2.38197279\n",
      "Validated batch 111 batch loss 3.08372784\n",
      "Validated batch 112 batch loss 2.6815834\n",
      "Validated batch 113 batch loss 2.58968496\n",
      "Validated batch 114 batch loss 2.4694581\n",
      "Validated batch 115 batch loss 2.3703084\n",
      "Validated batch 116 batch loss 2.25600314\n",
      "Validated batch 117 batch loss 2.50133753\n",
      "Validated batch 118 batch loss 2.44985127\n",
      "Validated batch 119 batch loss 3.01260591\n",
      "Validated batch 120 batch loss 3.01510191\n",
      "Validated batch 121 batch loss 2.44263935\n",
      "Validated batch 122 batch loss 2.91035748\n",
      "Validated batch 123 batch loss 2.85728073\n",
      "Validated batch 124 batch loss 2.82398319\n",
      "Validated batch 125 batch loss 2.63304973\n",
      "Validated batch 126 batch loss 2.37313652\n",
      "Validated batch 127 batch loss 2.74079967\n",
      "Validated batch 128 batch loss 3.14493656\n",
      "Validated batch 129 batch loss 2.60935497\n",
      "Validated batch 130 batch loss 2.59221053\n",
      "Validated batch 131 batch loss 2.74223256\n",
      "Validated batch 132 batch loss 2.7839551\n",
      "Validated batch 133 batch loss 2.41356349\n",
      "Validated batch 134 batch loss 2.86882496\n",
      "Validated batch 135 batch loss 2.4928627\n",
      "Validated batch 136 batch loss 1.99782598\n",
      "Validated batch 137 batch loss 2.26418638\n",
      "Validated batch 138 batch loss 2.45869946\n",
      "Validated batch 139 batch loss 2.829983\n",
      "Validated batch 140 batch loss 2.79708052\n",
      "Validated batch 141 batch loss 2.70667171\n",
      "Validated batch 142 batch loss 2.51814938\n",
      "Validated batch 143 batch loss 2.88739872\n",
      "Validated batch 144 batch loss 2.52304745\n",
      "Validated batch 145 batch loss 2.43593407\n",
      "Validated batch 146 batch loss 2.55830097\n",
      "Validated batch 147 batch loss 2.67614651\n",
      "Validated batch 148 batch loss 2.44274807\n",
      "Validated batch 149 batch loss 2.51712513\n",
      "Validated batch 150 batch loss 2.87227678\n",
      "Validated batch 151 batch loss 2.44380212\n",
      "Validated batch 152 batch loss 2.68008852\n",
      "Validated batch 153 batch loss 2.69963932\n",
      "Validated batch 154 batch loss 2.39484119\n",
      "Validated batch 155 batch loss 2.34825158\n",
      "Validated batch 156 batch loss 2.39327574\n",
      "Validated batch 157 batch loss 2.90088558\n",
      "Validated batch 158 batch loss 2.7146678\n",
      "Validated batch 159 batch loss 2.41438675\n",
      "Validated batch 160 batch loss 2.58426666\n",
      "Validated batch 161 batch loss 2.75078559\n",
      "Validated batch 162 batch loss 2.70872235\n",
      "Validated batch 163 batch loss 2.46131468\n",
      "Validated batch 164 batch loss 2.86928201\n",
      "Validated batch 165 batch loss 2.36622405\n",
      "Validated batch 166 batch loss 2.51751947\n",
      "Validated batch 167 batch loss 2.48827553\n",
      "Validated batch 168 batch loss 2.64980459\n",
      "Validated batch 169 batch loss 2.32877731\n",
      "Validated batch 170 batch loss 2.80123472\n",
      "Validated batch 171 batch loss 2.51903725\n",
      "Validated batch 172 batch loss 2.67906904\n",
      "Validated batch 173 batch loss 2.58163428\n",
      "Validated batch 174 batch loss 2.82185507\n",
      "Validated batch 175 batch loss 2.68603826\n",
      "Validated batch 176 batch loss 2.82056022\n",
      "Validated batch 177 batch loss 2.78807402\n",
      "Validated batch 178 batch loss 3.18768644\n",
      "Validated batch 179 batch loss 3.28780651\n",
      "Validated batch 180 batch loss 2.94957209\n",
      "Validated batch 181 batch loss 2.54074216\n",
      "Validated batch 182 batch loss 2.60228777\n",
      "Validated batch 183 batch loss 2.63100743\n",
      "Validated batch 184 batch loss 2.36186433\n",
      "Validated batch 185 batch loss 2.43830299\n",
      "Validated batch 186 batch loss 2.50085068\n",
      "Validated batch 187 batch loss 2.40574884\n",
      "Validated batch 188 batch loss 2.51065898\n",
      "Validated batch 189 batch loss 2.46052289\n",
      "Validated batch 190 batch loss 2.40820575\n",
      "Validated batch 191 batch loss 2.74538112\n",
      "Validated batch 192 batch loss 2.3948288\n",
      "Validated batch 193 batch loss 2.31759357\n",
      "Validated batch 194 batch loss 2.62605762\n",
      "Validated batch 195 batch loss 2.42130041\n",
      "Validated batch 196 batch loss 2.58684373\n",
      "Validated batch 197 batch loss 2.68875647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 198 batch loss 2.53481579\n",
      "Validated batch 199 batch loss 2.76313424\n",
      "Validated batch 200 batch loss 2.57352352\n",
      "Validated batch 201 batch loss 3.08416152\n",
      "Validated batch 202 batch loss 2.79147458\n",
      "Validated batch 203 batch loss 2.66361713\n",
      "Validated batch 204 batch loss 2.72889614\n",
      "Validated batch 205 batch loss 2.49806237\n",
      "Validated batch 206 batch loss 2.69334078\n",
      "Validated batch 207 batch loss 2.4944706\n",
      "Validated batch 208 batch loss 2.84606862\n",
      "Validated batch 209 batch loss 2.83479285\n",
      "Validated batch 210 batch loss 2.83620405\n",
      "Validated batch 211 batch loss 2.64006186\n",
      "Validated batch 212 batch loss 2.80038977\n",
      "Validated batch 213 batch loss 2.70072412\n",
      "Validated batch 214 batch loss 2.8780973\n",
      "Validated batch 215 batch loss 2.70804763\n",
      "Validated batch 216 batch loss 2.96351361\n",
      "Validated batch 217 batch loss 2.55645418\n",
      "Validated batch 218 batch loss 2.95180297\n",
      "Validated batch 219 batch loss 2.64404821\n",
      "Validated batch 220 batch loss 2.49672914\n",
      "Validated batch 221 batch loss 2.19544697\n",
      "Validated batch 222 batch loss 2.53454185\n",
      "Validated batch 223 batch loss 2.61276817\n",
      "Validated batch 224 batch loss 2.79574871\n",
      "Validated batch 225 batch loss 2.77757835\n",
      "Validated batch 226 batch loss 2.79455662\n",
      "Validated batch 227 batch loss 2.58536339\n",
      "Validated batch 228 batch loss 2.58615732\n",
      "Validated batch 229 batch loss 2.72234559\n",
      "Validated batch 230 batch loss 2.7464397\n",
      "Validated batch 231 batch loss 2.37207365\n",
      "Validated batch 232 batch loss 2.53740287\n",
      "Validated batch 233 batch loss 2.83374977\n",
      "Validated batch 234 batch loss 2.51505613\n",
      "Validated batch 235 batch loss 2.63585472\n",
      "Validated batch 236 batch loss 2.7219162\n",
      "Validated batch 237 batch loss 2.57172871\n",
      "Validated batch 238 batch loss 2.38682699\n",
      "Validated batch 239 batch loss 2.50269723\n",
      "Validated batch 240 batch loss 2.76883078\n",
      "Validated batch 241 batch loss 2.53145814\n",
      "Validated batch 242 batch loss 3.28877711\n",
      "Validated batch 243 batch loss 2.82517\n",
      "Validated batch 244 batch loss 2.49289322\n",
      "Validated batch 245 batch loss 2.49561572\n",
      "Validated batch 246 batch loss 2.6937685\n",
      "Validated batch 247 batch loss 2.93938899\n",
      "Validated batch 248 batch loss 2.62058425\n",
      "Validated batch 249 batch loss 2.6840744\n",
      "Validated batch 250 batch loss 2.71922874\n",
      "Validated batch 251 batch loss 2.67445087\n",
      "Validated batch 252 batch loss 2.59772301\n",
      "Validated batch 253 batch loss 2.64913058\n",
      "Validated batch 254 batch loss 2.59573913\n",
      "Validated batch 255 batch loss 2.19316459\n",
      "Validated batch 256 batch loss 2.62448359\n",
      "Validated batch 257 batch loss 3.11256933\n",
      "Validated batch 258 batch loss 2.89505529\n",
      "Validated batch 259 batch loss 2.42633891\n",
      "Validated batch 260 batch loss 2.655653\n",
      "Validated batch 261 batch loss 2.89805937\n",
      "Validated batch 262 batch loss 2.43458652\n",
      "Validated batch 263 batch loss 2.79417324\n",
      "Validated batch 264 batch loss 2.66270947\n",
      "Validated batch 265 batch loss 2.75527215\n",
      "Validated batch 266 batch loss 2.81422377\n",
      "Validated batch 267 batch loss 2.04002476\n",
      "Validated batch 268 batch loss 2.51541328\n",
      "Validated batch 269 batch loss 2.56979823\n",
      "Validated batch 270 batch loss 2.62834859\n",
      "Validated batch 271 batch loss 2.48502684\n",
      "Validated batch 272 batch loss 2.56518531\n",
      "Validated batch 273 batch loss 2.45335364\n",
      "Validated batch 274 batch loss 2.73249292\n",
      "Validated batch 275 batch loss 2.60930037\n",
      "Validated batch 276 batch loss 2.53617191\n",
      "Validated batch 277 batch loss 2.60918093\n",
      "Validated batch 278 batch loss 2.79108691\n",
      "Validated batch 279 batch loss 2.79860783\n",
      "Validated batch 280 batch loss 2.42021942\n",
      "Validated batch 281 batch loss 2.20450211\n",
      "Validated batch 282 batch loss 2.63451\n",
      "Validated batch 283 batch loss 2.46811962\n",
      "Validated batch 284 batch loss 2.56606627\n",
      "Validated batch 285 batch loss 2.30930805\n",
      "Validated batch 286 batch loss 2.5222795\n",
      "Validated batch 287 batch loss 2.44910526\n",
      "Validated batch 288 batch loss 2.67714071\n",
      "Validated batch 289 batch loss 2.48430681\n",
      "Validated batch 290 batch loss 2.66266465\n",
      "Validated batch 291 batch loss 2.59143591\n",
      "Validated batch 292 batch loss 2.69910574\n",
      "Validated batch 293 batch loss 2.54766321\n",
      "Validated batch 294 batch loss 2.7434392\n",
      "Validated batch 295 batch loss 2.57902813\n",
      "Validated batch 296 batch loss 2.52016163\n",
      "Validated batch 297 batch loss 2.53693056\n",
      "Validated batch 298 batch loss 3.05928\n",
      "Validated batch 299 batch loss 2.45724821\n",
      "Validated batch 300 batch loss 2.87618375\n",
      "Validated batch 301 batch loss 2.27956414\n",
      "Validated batch 302 batch loss 2.25171494\n",
      "Validated batch 303 batch loss 2.61252332\n",
      "Validated batch 304 batch loss 2.63452244\n",
      "Validated batch 305 batch loss 2.4571023\n",
      "Validated batch 306 batch loss 2.80044055\n",
      "Validated batch 307 batch loss 2.56521988\n",
      "Validated batch 308 batch loss 2.43581891\n",
      "Validated batch 309 batch loss 2.5552392\n",
      "Validated batch 310 batch loss 2.86544895\n",
      "Validated batch 311 batch loss 2.86358404\n",
      "Validated batch 312 batch loss 2.33758759\n",
      "Validated batch 313 batch loss 2.91959667\n",
      "Validated batch 314 batch loss 2.82276177\n",
      "Validated batch 315 batch loss 2.26959038\n",
      "Validated batch 316 batch loss 2.48037577\n",
      "Validated batch 317 batch loss 2.76326394\n",
      "Validated batch 318 batch loss 2.39802647\n",
      "Validated batch 319 batch loss 2.61275792\n",
      "Validated batch 320 batch loss 2.74079943\n",
      "Validated batch 321 batch loss 2.31047773\n",
      "Validated batch 322 batch loss 2.34457421\n",
      "Validated batch 323 batch loss 2.65803742\n",
      "Validated batch 324 batch loss 2.5957098\n",
      "Validated batch 325 batch loss 2.63688231\n",
      "Validated batch 326 batch loss 2.86347389\n",
      "Validated batch 327 batch loss 2.57809043\n",
      "Validated batch 328 batch loss 2.41233349\n",
      "Validated batch 329 batch loss 2.4744184\n",
      "Validated batch 330 batch loss 2.3687067\n",
      "Validated batch 331 batch loss 2.87389135\n",
      "Validated batch 332 batch loss 2.71987557\n",
      "Validated batch 333 batch loss 2.5877161\n",
      "Validated batch 334 batch loss 2.78201175\n",
      "Validated batch 335 batch loss 2.60479331\n",
      "Validated batch 336 batch loss 2.82369137\n",
      "Validated batch 337 batch loss 2.75492811\n",
      "Validated batch 338 batch loss 2.74073219\n",
      "Validated batch 339 batch loss 2.79957294\n",
      "Validated batch 340 batch loss 2.63657284\n",
      "Validated batch 341 batch loss 2.77074957\n",
      "Validated batch 342 batch loss 2.42502046\n",
      "Validated batch 343 batch loss 2.50191832\n",
      "Validated batch 344 batch loss 2.63279915\n",
      "Validated batch 345 batch loss 2.76935983\n",
      "Validated batch 346 batch loss 2.75430441\n",
      "Validated batch 347 batch loss 2.56727505\n",
      "Validated batch 348 batch loss 2.30048418\n",
      "Validated batch 349 batch loss 2.73556566\n",
      "Validated batch 350 batch loss 2.94152212\n",
      "Validated batch 351 batch loss 2.74169612\n",
      "Validated batch 352 batch loss 2.91630888\n",
      "Validated batch 353 batch loss 2.53052139\n",
      "Validated batch 354 batch loss 2.7005825\n",
      "Validated batch 355 batch loss 2.74685955\n",
      "Validated batch 356 batch loss 2.51572275\n",
      "Validated batch 357 batch loss 3.1262722\n",
      "Validated batch 358 batch loss 2.49774027\n",
      "Validated batch 359 batch loss 2.36624861\n",
      "Validated batch 360 batch loss 2.50787473\n",
      "Validated batch 361 batch loss 2.43909335\n",
      "Validated batch 362 batch loss 2.70418119\n",
      "Validated batch 363 batch loss 2.27527094\n",
      "Validated batch 364 batch loss 2.22106552\n",
      "Validated batch 365 batch loss 2.4852736\n",
      "Validated batch 366 batch loss 3.02876115\n",
      "Validated batch 367 batch loss 2.6627636\n",
      "Validated batch 368 batch loss 2.45915508\n",
      "Validated batch 369 batch loss 2.31376648\n",
      "Validated batch 370 batch loss 2.74941587\n",
      "Epoch 2 val loss 2.6067774295806885\n",
      "Model ./models/model-v0.0.1-epoch-2-loss-2.6068.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "tfrecords_dir = os.getenv('HOME')+'/aiffel/mpii/tfrecords_mpii/'\n",
    "train_tfrecords = os.path.join(tfrecords_dir, 'train*')\n",
    "val_tfrecords = os.path.join(tfrecords_dir, 'val*')\n",
    "epochs = 2\n",
    "batch_size = 8\n",
    "num_heatmap = 16\n",
    "tensorboard_dir = './logs/'\n",
    "learning_rate = 0.0007\n",
    "start_epoch = 1\n",
    "\n",
    "automatic_gpu_usage()\n",
    "\n",
    "pretrained_path = None # './models_old/model-v0.0.2-epoch-15-loss-1.1013.h5'\n",
    "\n",
    "history = train(epochs, start_epoch, learning_rate, tensorboard_dir, pretrained_path,\n",
    "      num_heatmap, batch_size, train_tfrecords, val_tfrecords, '0.0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplebaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')\n",
    "\n",
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = tf.keras.models.Sequential()\n",
    "\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same'))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization())\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model\n",
    "\n",
    "\n",
    "upconv = _make_deconv_layer(3)\n",
    "final_layer = tf.keras.layers.Conv2D(16, kernel_size=(1,1), padding='same')\n",
    "\n",
    "\n",
    "def Simplebaseline(input_shape=(256, 256, 3)):\n",
    "    resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    x = resnet(inputs)\n",
    "    x = upconv(x)\n",
    "    out = final_layer(x)\n",
    "    model = tf.keras.Model(inputs, out, name='simple_baseline')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Simplebaseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, start_epoch, learning_rate, tensorboard_dir, checkpoint,\n",
    "          num_heatmap, batch_size, train_tfrecords, val_tfrecords, version):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(os.path.join('./models')):\n",
    "        os.makedirs(os.path.join('./models/'))\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model2 = Simplebaseline(IMAGE_SHAPE)\n",
    "        if checkpoint and os.path.exists(checkpoint):\n",
    "            model.load_weights(checkpoint)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model2,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate,\n",
    "            start_epoch=start_epoch,\n",
    "            version=version,\n",
    "            tensorboard_dir=tensorboard_dir)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords_dir = os.getenv('HOME')+'/aiffel/mpii/tfrecords_mpii/'\n",
    "train_tfrecords = os.path.join(tfrecords_dir, 'train*')\n",
    "val_tfrecords = os.path.join(tfrecords_dir, 'val*')\n",
    "epochs = 2\n",
    "batch_size = 8\n",
    "num_heatmap = 16\n",
    "tensorboard_dir = './logs/'\n",
    "learning_rate = 0.0007\n",
    "start_epoch = 1\n",
    "\n",
    "automatic_gpu_usage()\n",
    "\n",
    "pretrained_path = None # './models_old/model-v0.0.2-epoch-15-loss-1.1013.h5'\n",
    "\n",
    "history2 = train(epochs, start_epoch, learning_rate, tensorboard_dir, pretrained_path,\n",
    "      num_heatmap, batch_size, train_tfrecords, val_tfrecords, '0.0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
